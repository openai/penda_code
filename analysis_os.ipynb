{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Literal\n",
    "from typing_extensions import TypeIs\n",
    "import textwrap\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas._libs.missing import NAType\n",
    "from scipy.stats import mannwhitneyu, fisher_exact, binomtest\n",
    "from scipy.stats.contingency import relative_risk\n",
    "from IPython.display import Image\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as mtick\n",
    "from graphviz import Digraph\n",
    "import seaborn as sns\n",
    "\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "import blobfile as bf\n",
    "\n",
    "import types_os as types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_fp = 'REMOVED'\n",
    "clinical_study_results_fp = 'REMOVED'\n",
    "gpt41_results_fp = 'REMOVED'\n",
    "o3_results_fp = 'REMOVED'\n",
    "plots_folder = 'ENTER_FOLDER_NAME_HERE'\n",
    "shutil.rmtree(plots_folder)\n",
    "os.makedirs(plots_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bf.exists(plots_folder):\n",
    "    print(f\"Removing existing plots folder: {plots_folder}\")\n",
    "    bf.rmtree(plots_folder)\n",
    "\n",
    "print(f\"Creating plots folder: {plots_folder}\")\n",
    "bf.makedirs(plots_folder)\n",
    "\n",
    "def save_fig(plot_name: str, fig: plt.Figure):\n",
    "    fp = bf.join(plots_folder, f\"{plot_name}.png\")\n",
    "    with bf.BlobFile(fp, \"wb\") as f:\n",
    "        fig.savefig(f, format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "def save_latex(name: str, df: pd.DataFrame):\n",
    "    fp = bf.join(plots_folder, f\"{name}.tex\")\n",
    "    s = df.to_latex().replace('%', '\\\\%')\n",
    "    with bf.BlobFile(fp, \"w\") as f:\n",
    "        f.write(s)\n",
    "\n",
    "def save_csv(name: str, df: pd.DataFrame):\n",
    "    fp = bf.join(plots_folder, \"data_for_web/\", f\"{name}.csv\")\n",
    "    print(f\"Saving {name} to {fp}\")\n",
    "    with bf.BlobFile(fp, \"w\") as f:\n",
    "        df.to_csv(f, index=False)\n",
    "\n",
    "latexvars = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(\n",
    "    style=\"dark\",\n",
    "    palette=\"muted\",\n",
    "    font=\"serif\",\n",
    "    rc={\n",
    "        \"figure.dpi\": 120, # TODO: modify for paper figures\n",
    "        \"axes.titleweight\":\"normal\",\n",
    "        \"axes.labelweight\":\"normal\",\n",
    "        \"axes.spines.top\":False,\n",
    "        \"axes.spines.right\": False,\n",
    "        \"legend.frameon\": False,\n",
    "        \"figure.autolayout\": True,\n",
    "        \"legend.fontsize\": \"small\",\n",
    "        \"legend.title_fontsize\": \"medium\",\n",
    "        \"xtick.labelsize\": \"small\",\n",
    "        \"ytick.labelsize\": \"small\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_dir = os.path.expanduser(parquet_fp)\n",
    "\n",
    "dfs = {}\n",
    "for file in bf.listdir(parquet_dir):\n",
    "    if not file.endswith('.parquet'):\n",
    "        continue\n",
    "\n",
    "    file_path = bf.join(parquet_dir, file)\n",
    "    with bf.BlobFile(file_path, \"rb\") as f:\n",
    "        df = pd.read_parquet(f)\n",
    "    filename = os.path.basename(file_path).replace('.parquet', '')\n",
    "    dfs[filename] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_old_new_dfs(old_df: pd.DataFrame, new_df: pd.DataFrame, assert_no_dupes: bool = True, index_col: str = 'VisitCode', columns_should_be_subset: tuple[str, ...] | None = ('UserId', 'CallDate')) -> pd.DataFrame:\n",
    "    assert (old_df.columns == new_df.columns).all()\n",
    "    for col in old_df.columns:\n",
    "        assert old_df[col].dtype == new_df[col].dtype, f\"Column {col} has different dtypes in the old and new dfs\"\n",
    "\n",
    "    concat_df = pd.concat([old_df, new_df], ignore_index=True)\n",
    "\n",
    "    for subset in [None, index_col]:\n",
    "        n_dupes_old = len(old_df) - len(old_df.drop_duplicates(subset=subset))\n",
    "        n_dupes_new = len(new_df) - len(new_df.drop_duplicates(subset=subset))\n",
    "        if assert_no_dupes:\n",
    "            assert n_dupes_old == 0, f\"Old df has {n_dupes_old} duplicates\" + (\"looking at all columns\" if subset is None else f\"looking at {subset} column\")\n",
    "            assert n_dupes_new == 0, f\"New df has {n_dupes_new} duplicates\" + (\"looking at all columns\" if subset is None else f\"looking at {subset} column\")\n",
    "\n",
    "        assert len(concat_df) == len(concat_df.drop_duplicates(subset=subset)) + n_dupes_old + n_dupes_new, f\"Duplicates exist across the old and new dfs\" + (\"looking at all columns\" if subset is None else f\"looking at {subset} column\")\n",
    "\n",
    "    if columns_should_be_subset is not None:\n",
    "        for col in columns_should_be_subset:\n",
    "            diff = set(new_df[col].dropna()) - set(old_df[col].dropna())\n",
    "            assert len(diff) == 0, f\"New df has {col} that are not in the old df: {diff}\"\n",
    "\n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interrogate outcomes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = dfs['outcomes']\n",
    "outcomes_new = dfs['outcomes_new']\n",
    "\n",
    "outcomes = concat_old_new_dfs(outcomes, outcomes_new, assert_no_dupes=False, columns_should_be_subset=('Q1',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of outcomes: {len(outcomes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure no entirely duplicated rows\n",
    "assert not any(outcomes.duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the outcomes results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes.Q1.value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcomes_q1_remapper(s: str | NAType) -> int | NAType:\n",
    "    if pd.isna(s):\n",
    "        return pd.NA\n",
    "\n",
    "    # many of these have | in them, indicating multiple answers; drop\n",
    "    if '|' in s:\n",
    "        return pd.NA\n",
    "\n",
    "    return int(s[0])\n",
    "\n",
    "outcomes.Q1 = outcomes.Q1.map(outcomes_q1_remapper)\n",
    "outcomes.Q1.value_counts(dropna=False, normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes.Q2.value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_map = {\n",
    "    'I got all my treatment and medicines at Penda': 'all_at_penda',\n",
    "    'I went myself to another hospital or specialist': 'self_referred',\n",
    "    'Penda referred me to another hospital or specialist': 'penda_referred',\n",
    "    'I visited another chemist': 'another_chemist',\n",
    "}\n",
    "def map_outcomes(s: str) -> str:\n",
    "    mapped_outcomes = []\n",
    "    for k, v in outcomes_map.items():\n",
    "        if k in s:\n",
    "            mapped_outcomes.append(v)\n",
    "    assert len(mapped_outcomes) == 1, f\"Multiple outcomes in {s} ({mapped_outcomes})\"\n",
    "    return mapped_outcomes[0]\n",
    "\n",
    "def outcomes_q2_remapper(s: str | None) -> str | None:\n",
    "    if s is None:\n",
    "        return s\n",
    "\n",
    "    outcomes_present = s.split('|')\n",
    "    outcomes_mapped = {map_outcomes(o) for o in outcomes_present}\n",
    "\n",
    "    match len(outcomes_mapped):\n",
    "        case 0:\n",
    "            raise ValueError(f\"No outcomes were found in non-None {s}\")\n",
    "        case 1:\n",
    "            return outcomes_mapped.pop()\n",
    "        case 2:\n",
    "            if 'all_at_penda' in outcomes_mapped:\n",
    "                outcomes_mapped.remove('all_at_penda')\n",
    "                return outcomes_mapped.pop()\n",
    "            else:\n",
    "                return None\n",
    "        case _:\n",
    "            raise ValueError(f\"Unexpected: three outcomes were returned for {s}\")\n",
    "\n",
    "outcomes.Q2 = outcomes.Q2.map(outcomes_q2_remapper)\n",
    "outcomes.Q2.value_counts(dropna=False, normalize=True) * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VisitCodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visit codes and call dates must be non-null\n",
    "assert not any(outcomes.VisitCode.isna())\n",
    "assert not any(outcomes.CallDate.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_visit_codes = outcomes.VisitCode.value_counts()\n",
    "repeat_visit_code_counts = counts_visit_codes[counts_visit_codes > 1]\n",
    "repeat_visit_codes = repeat_visit_code_counts.index\n",
    "print(f'Number of visit codes that occur more than once: {len(repeat_visit_code_counts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example of a visit code that occurs more than once:\")\n",
    "outcomes[outcomes.VisitCode == repeat_visit_codes[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_most_recent_call_date(group):\n",
    "    # first, keep only the rows with the most recent call date\n",
    "    max_call_date = group.CallDate.max()\n",
    "    max_call_date_rows = group[group.CallDate == max_call_date]\n",
    "\n",
    "    if len(max_call_date_rows) == 1:\n",
    "        return max_call_date_rows\n",
    "\n",
    "    # Combine rows with the same max call date for a visit code\n",
    "    # For Q1 and Q2, if there's a unique non-NA value, use it; if multiple, set to NA\n",
    "    row = max_call_date_rows.iloc[0].copy()\n",
    "    q1_values = max_call_date_rows.Q1.dropna().unique()\n",
    "    row['Q1'] = q1_values[0] if len(q1_values) == 1 else None\n",
    "    q2_values = max_call_date_rows.Q2.dropna().unique()\n",
    "    row['Q2'] = q2_values[0] if len(q2_values) == 1 else None\n",
    "    return row.to_frame().T\n",
    "\n",
    "outcomes = outcomes.groupby('VisitCode', group_keys=False).apply(keep_most_recent_call_date)\n",
    "assert outcomes.VisitCode.is_unique, \"VisitCode is not unique in outcomes_deduplicated\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allocations is subsumed by visits_with_users and is not necessary to use. Not all of the allocated providers saw patients - `visits_with_users` contains the canonical record of those who actually saw patients and should be used as the source of truth for what providers saw patients within the scope of the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allocations = dfs['allocations']\n",
    "allocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert allocations['UserId'].is_unique, \"UserId is not unique in allocations\"\n",
    "assert allocations['UserId'].notna().all(), \"UserId has nulls in allocations\"\n",
    "assert allocations['StaffId'].is_unique, \"StaffId is not unique in allocations\"\n",
    "assert allocations['StaffId'].notna().all(), \"StaffId has nulls in allocations\"\n",
    "assert allocations['Group'].notna().all(), \"Group has nulls in allocations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that the 'Group' in visits_with_users matches the 'Group' from allocations\n",
    "# Left join visits_with_users with allocations on 'UserId'\n",
    "visits_with_users = dfs['visits_with_users']\n",
    "vw_alloc = visits_with_users.merge(\n",
    "    allocations[['UserId', 'Group']],\n",
    "    on = 'UserId',\n",
    "    how='left',\n",
    "    suffixes=('', '_alloc')\n",
    ")\n",
    "\n",
    "# Confirm that the 'Group' in visits_with_users matches the 'Group' from allocations\n",
    "# (i.e., 'Group' == 'Group_alloc' for all rows where 'Group_alloc' is not null)\n",
    "mismatch = vw_alloc[~vw_alloc['Group'].eq(vw_alloc['Group_alloc']) & vw_alloc['Group_alloc'].notna()]\n",
    "assert len(mismatch) == 0, \"There are mismatches between visits_with_users.Group and allocations.Group\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out how many are doctors and nurses\n",
    "unique_users_with_visits = visits_with_users[['UserId', 'RoleName', 'Group']].drop_duplicates()\n",
    "assert unique_users_with_visits['UserId'].is_unique, \"UserId is not unique in unique_users_with_visits\"\n",
    "len(unique_users_with_visits), len(allocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuv_alloc = unique_users_with_visits.merge(\n",
    "    allocations[['UserId', 'Group']],\n",
    "    on='UserId',\n",
    "    how='left',\n",
    "    suffixes=('', '_alloc')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuv_alloc_providers = uuv_alloc[uuv_alloc['RoleName'] == 'Provider']\n",
    "uuv_alloc_providers['Group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(uuv_alloc_providers['Group'] == uuv_alloc_providers['Group_alloc']), \"Group does not match Group_alloc in uuv_alloc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = dfs['visits']\n",
    "visits_new = dfs['visits_new']\n",
    "\n",
    "visits = concat_old_new_dfs(visits, visits_new, columns_should_be_subset = ('VisitCategory', 'Gender', 'LocationName', 'VisitType', 'VisitDate', 'HadUnplannedVisit'))\n",
    "\n",
    "visits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert visits.VisitCode.is_unique, \"VisitCode is not unique in visits\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visits with users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits_with_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits_with_users = dfs['visits_with_users']\n",
    "visits_with_users_new = dfs['visits_with_users_new']\n",
    "\n",
    "visits_with_users = concat_old_new_dfs(visits_with_users, visits_with_users_new, assert_no_dupes=False, columns_should_be_subset=('UserId', 'RoleName', 'Group'))\n",
    "\n",
    "visits_with_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nurse_provider_mapping_df = visits_with_users[['UserId', 'RoleName', 'Group']].drop_duplicates()\n",
    "nurse_provider_mapping_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not any(visits_with_users.duplicated())\n",
    "assert visits_with_users.VisitCode.notna().all(), \"VisitCode has nulls in visits_with_users\"\n",
    "assert visits_with_users.UserId.notna().all(), \"UserId has nulls in visits_with_users\"\n",
    "assert visits_with_users.RoleName.notna().all(), \"RoleName has nulls in visits_with_users\"\n",
    "assert visits_with_users.Group.notna().all(), \"Group has nulls in visits_with_users\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits_with_users.RoleName.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits_with_users.Group.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_duplicated_visits_with_users(visits_with_users_curr):\n",
    "    # Find duplicated VisitCodes\n",
    "    dup_mask = visits_with_users_curr.VisitCode.duplicated(keep=False)\n",
    "    dups = visits_with_users_curr[dup_mask]\n",
    "\n",
    "    # What % of the overall dataset are these rows?\n",
    "    percent_dups = len(dups) / len(visits_with_users_curr) * 100\n",
    "    print(f\"Rows with duplicated VisitCode: {len(dups)} ({percent_dups:.2f}% of all rows)\")\n",
    "\n",
    "    # number of unique visit codes with duplicates\n",
    "    print(f'Number of unique visit codes with duplicates: {len(dups.VisitCode.unique())} ({len(dups.VisitCode.unique()) / len(visits_with_users_curr.VisitCode.unique()) * 100:.2f}% of all unique visit codes)')\n",
    "\n",
    "    # For each duplicated VisitCode, check the unique groups present\n",
    "    group_counts = dups.groupby('VisitCode')['Group'].unique().reset_index()\n",
    "\n",
    "    all_silent = group_counts['Group'].apply(lambda x: set(x) == {'Silent AI'}).sum()\n",
    "    all_active = group_counts['Group'].apply(lambda x: set(x) == {'Active AI'}).sum()\n",
    "    mixed = group_counts['Group'].apply(lambda x: set(x) == {'Silent AI', 'Active AI'} or set(x) == {'Active AI', 'Silent AI'}).sum()\n",
    "\n",
    "    print(f\"Number of duplicated VisitCodes where all groups are Silent AI: {all_silent} ({all_silent / len(group_counts) * 100:.2f}% of all duplicated visit codes)\")\n",
    "    print(f\"Number of duplicated VisitCodes where all groups are Active AI: {all_active} ({all_active / len(group_counts) * 100:.2f}% of all duplicated visit codes)\")\n",
    "    print(f\"Number of duplicated VisitCodes where there is a mix of Silent AI and Active AI: {mixed} ({mixed / len(group_counts) * 100:.2f}% of all duplicated visit codes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicated visits with users (all clinicians)\")\n",
    "summarize_duplicated_visits_with_users(visits_with_users)\n",
    "print(\"\\nDuplicated visits with users (providers only)\")\n",
    "summarize_duplicated_visits_with_users(visits_with_users[visits_with_users.RoleName == \"Provider\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_visits_with_users = len(visits_with_users)\n",
    "visits_with_users_providers_only = visits_with_users[visits_with_users.RoleName == \"Provider\"]\n",
    "n_provider_visits_with_users = len(visits_with_users_providers_only)\n",
    "print(f\"Percentage of visits with users that are providers: {n_provider_visits_with_users / n_total_visits_with_users * 100:.2f}% - remaining visits were dropped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_visits_with_users(group: pd.DataFrame) -> pd.Series:\n",
    "    # AI group composition for all users\n",
    "    group_set_all = set(group[\"Group\"].unique())\n",
    "    if group_set_all == {\"Active AI\"}:\n",
    "        all_user_role = \"AI\"\n",
    "    elif group_set_all == {\"Silent AI\"}:\n",
    "        all_user_role = \"Non-AI\"\n",
    "    elif group_set_all == {\"Active AI\", \"Silent AI\"}:\n",
    "        all_user_role = \"Crossover\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected group composition: {group_set_all}\")\n",
    "\n",
    "    user_ids_all = set(group[\"UserId\"].unique())\n",
    "\n",
    "    return pd.Series({\n",
    "        \"ClinicianGroup\": all_user_role,\n",
    "        \"UserIDs\": group[\"UserId\"].unique().tolist(),\n",
    "    })\n",
    "\n",
    "summarized_visits_with_users = visits_with_users_providers_only.groupby(\"VisitCode\", group_keys=False).apply(summarize_visits_with_users).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_visits_with_users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ai_interaction_scrubbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_interaction_scrubbed = dfs['ai_interaction_scrubbed']\n",
    "ai_interaction_scrubbed_new = dfs['scrubbed_ai_interaction_new']\n",
    "ai_interaction_scrubbed = concat_old_new_dfs(ai_interaction_scrubbed, ai_interaction_scrubbed_new, assert_no_dupes=False, index_col='VisitCode', columns_should_be_subset=('Version', 'SystemRolePrompt', 'AiLike', 'ClinicalDecisionRule', 'FocusOutEmrComponent', 'Silent', 'Acknowledged'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_interaction_scrubbed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ai_interaction_scrubbed.AiResponse.str.startswith(\"```\")).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_interaction_scrubbed.CreatedOn = pd.to_datetime(ai_interaction_scrubbed.CreatedOn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_ACKNOWLEDGEMENT_STARTED = ai_interaction_scrubbed[ai_interaction_scrubbed.Acknowledged == 1.0].CreatedOn.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace thumbs up data with 'Up', 'Down', 'None'\n",
    "ai_interaction_scrubbed.AiLike = ai_interaction_scrubbed.AiLike.map({1.0: 'Up', 0.0: 'Down'}, na_action='ignore')\n",
    "ai_interaction_scrubbed.AiLike = ai_interaction_scrubbed.AiLike.fillna(value = 'None')\n",
    "\n",
    "# replace acknowledgement data with True/False (and NA if before acknowledgement data started to be collected)\n",
    "ai_interaction_scrubbed.Acknowledged = ai_interaction_scrubbed.Acknowledged.map({1.0: True}, na_action='ignore')\n",
    "ai_interaction_scrubbed.Acknowledged = ai_interaction_scrubbed.apply(\n",
    "    lambda row: False if pd.isna(row.Acknowledged) and row.CreatedOn > DATE_ACKNOWLEDGEMENT_STARTED else row.Acknowledged,\n",
    "    axis=1\n",
    ")\n",
    "ai_interaction_scrubbed.Acknowledged = ai_interaction_scrubbed.Acknowledged.where(pd.notna(ai_interaction_scrubbed.Acknowledged), None)\n",
    "\n",
    "# replace silent data with 'Silent'/'Active'\n",
    "ai_interaction_scrubbed['Silent'] = ai_interaction_scrubbed['Silent'].map({True: 'Silent', False: 'Active'}, na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(\n",
    "    ai_interaction_scrubbed.FocusOutEmrComponent,\n",
    "    ai_interaction_scrubbed.ClinicalDecisionRule,\n",
    "    dropna=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_parses = []\n",
    "def parse_ai_response(ai_response):\n",
    "    \"\"\"\n",
    "    Parse an AiResponse string into a dict using parse_json.\n",
    "    Handles:\n",
    "      - raw JSON\n",
    "      - JSON wrapped in ```\n",
    "      - JSON wrapped in ```json ... ```\n",
    "    Returns None if parsing fails or input is null/empty.\n",
    "    \"\"\"\n",
    "    if not isinstance(ai_response, str) or not ai_response.strip():\n",
    "        return None\n",
    "\n",
    "    s = ai_response.strip()\n",
    "    if s.startswith(\"```\"): # handle markdown-wrapped JSON\n",
    "        s = s[3:]\n",
    "        s = s.lstrip()\n",
    "        if s.lower().startswith(\"json\"):\n",
    "            s = s[4:]\n",
    "            s = s.lstrip()\n",
    "        if s.endswith(\"```\"):\n",
    "            s = s[:-3]\n",
    "        s = s.strip()\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        failed_parses.append(s)\n",
    "        return None\n",
    "\n",
    "AIResponseParsed = ai_interaction_scrubbed.AiResponse.map(parse_ai_response)\n",
    "sum(AIResponseParsed.isna()), sum(AIResponseParsed.isna()) / len(ai_interaction_scrubbed), sum(ai_interaction_scrubbed.AiResponse.isna()) / len(ai_interaction_scrubbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Percentage of rows that failed to parse: {len(failed_parses) / len(ai_interaction_scrubbed) * 100:.2f}%')\n",
    "n_failed_parse_curly_brace = sum(['{' in s or '}' in s for s in failed_parses])\n",
    "print(f'Percentage of failed parses that contain a curly brace: {n_failed_parse_curly_brace / len(failed_parses) * 100:.2f}%')\n",
    "n_failed_parse_error = sum(['error' in s.lower() and 'http' in s.lower() for s in failed_parses])\n",
    "print(f'Percentage of failed parses that contain an error and http: {n_failed_parse_error / len(failed_parses) * 100:.2f}%')\n",
    "n_failed_parse_unknown = len(failed_parses) - n_failed_parse_curly_brace - n_failed_parse_error\n",
    "print(f'Percentage of failed parses that are of other types (typically model not comforming to JSON format): {n_failed_parse_unknown / len(failed_parses) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_interaction_scrubbed['AIResponseParsed'] = AIResponseParsed\n",
    "pre_drop_n = len(ai_interaction_scrubbed)\n",
    "ai_interaction_scrubbed.dropna(subset=['AIResponseParsed'], inplace=True)\n",
    "post_drop_n = len(ai_interaction_scrubbed)\n",
    "print(f'Percentage of rows that were dropped: {(pre_drop_n - post_drop_n) / pre_drop_n * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_interaction_scrubbed.ClinicalDecisionRule.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ai_response(ai_response: dict):\n",
    "    try:\n",
    "        return types.AIResponse.model_validate(ai_response)\n",
    "    except ValidationError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_interaction_scrubbed['AIResponseValidated'] = ai_interaction_scrubbed.AIResponseParsed.apply(parse_ai_response)\n",
    "n_none = sum(ai_interaction_scrubbed.AIResponseValidated.isna())\n",
    "print(f'Percentage of rows that failed to validate: {n_none / len(ai_interaction_scrubbed) * 100:.4f}% ({n_none} rows / {len(ai_interaction_scrubbed)} total)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_length = len(ai_interaction_scrubbed)\n",
    "ai_interaction_scrubbed.dropna(subset=['AIResponseValidated'], inplace=True)\n",
    "post_drop_length = len(ai_interaction_scrubbed)\n",
    "print(f'Percentage of rows that were dropped: {(original_length - post_drop_length) / original_length * 100:.4f}% ({original_length - post_drop_length} rows / {original_length} total)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_interaction_scrubbed = ai_interaction_scrubbed.merge(nurse_provider_mapping_df, on = 'UserId', how = 'left')\n",
    "ai_interaction_scrubbed = ai_interaction_scrubbed[ai_interaction_scrubbed.RoleName == 'Provider']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to convert a single dataframe row into an AICall object\n",
    "def _row_to_aicall(row) -> types.AICall:\n",
    "    return types.AICall(\n",
    "        rule=row.ClinicalDecisionRule,\n",
    "        response=row.AIResponseValidated,\n",
    "        user_id=row.UserId,\n",
    "        time=row.CreatedOn,\n",
    "        thumbs_up_down=row.AiLike,\n",
    "        silent=row.Silent,\n",
    "        acknowledged=row.Acknowledged,\n",
    "        user_role_prompt=row.UserRolePrompt,\n",
    "    )\n",
    "\n",
    "def _rows_to_aicalls(rows: pd.DataFrame) -> types.AICalls:\n",
    "    return types.AICalls(calls=[_row_to_aicall(row) for _, row in rows.iterrows()])\n",
    "\n",
    "visit_aicalls_df = (\n",
    "    ai_interaction_scrubbed\n",
    "    .groupby(\"VisitCode\", sort=False)\n",
    "    .apply(_rows_to_aicalls)\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"AICalls\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_aicalls = visit_aicalls_df.AICalls.apply(lambda x: x.calls).explode()\n",
    "active_aicalls = all_aicalls[all_aicalls.map(lambda x: x.silent == 'Active')]\n",
    "active_red_aicalls = active_aicalls[active_aicalls.map(lambda x: x.color == types.Color.Red)]\n",
    "active_red_aicalls.map(lambda x: x.acknowledged).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clinical_documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_documentation = dfs['clinical_documentation']\n",
    "clinical_documentation = clinical_documentation.drop(columns = ['VisitId_1'])\n",
    "clinical_documentation_new = dfs['scrubbed_clinical_documentation_cleaned']\n",
    "clinical_documentation = concat_old_new_dfs(clinical_documentation, clinical_documentation_new, columns_should_be_subset = ('LocationName', 'VisitCategory', 'VisitType', 'VisitDate'))\n",
    "\n",
    "clinical_documentation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert clinical_documentation.VisitCode.is_unique, \"VisitCode is not unique in clinical_documentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_documentation.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_not_na = ['VisitCode', 'LocationName', 'VisitCategory', 'VisitType', 'VisitDate', 'Age']\n",
    "categories_na = [c for c in clinical_documentation.columns if c not in categories_not_na]\n",
    "for c in categories_not_na:\n",
    "    assert not clinical_documentation[c].isna().any(), f\"{c} has nulls\"\n",
    "for c in categories_na:\n",
    "    assert clinical_documentation[c].isna().any(), f\"{c} has no nulls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clinical_documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate NA rate in 'CC' per VisitDate\n",
    "cc_na_rate_by_date = (\n",
    "    clinical_documentation\n",
    "    .groupby('VisitDate')['CC']\n",
    "    .apply(lambda x: x.isna().mean())\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "cc_na_rate_by_date.plot(marker='o')\n",
    "plt.title('Rate of NAs in clinical_documentation.CC vs VisitDate')\n",
    "plt.ylabel('NA Rate in CC')\n",
    "plt.xlabel('VisitDate')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_visitdate_with_cc = clinical_documentation.loc[clinical_documentation['CC'].notna(), 'VisitDate'].max()\n",
    "last_visitdate_with_cc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one_day_outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day_outcomes = dfs['one_day_outcomes']\n",
    "one_day_outcomes_new = dfs['one_day_outcomes_new']\n",
    "\n",
    "one_day_outcomes = concat_old_new_dfs(one_day_outcomes, one_day_outcomes_new, assert_no_dupes=False, columns_should_be_subset=('Q1',))\n",
    "\n",
    "one_day_outcomes.head()\n",
    "\n",
    "one_day_outcomes['one_day_call_outcome'] = one_day_outcomes['Q1']\n",
    "one_day_outcomes.drop(columns=['Q1'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by VisitCode and aggregate one_day_call_outcome according to the rules:\n",
    "# - If both values are the same (including both None), keep that value.\n",
    "# - If one is 'No' and the other is None, keep 'No'.\n",
    "# - If one is 'Yes' and the other is None, keep 'Yes'.\n",
    "# - If one is 'No' and the other is 'Yes', drop the row.\n",
    "\n",
    "def resolve_outcome(s: pd.Series):\n",
    "    values = s.tolist()           # ← the Series is already the column we need\n",
    "    if len(values) > 2:\n",
    "        raise ValueError(f\"Expected at most 2 values, got {len(values)}\")\n",
    "\n",
    "    # treat NaNs as “None” for the logic below\n",
    "    both_same = len({v for v in values if pd.notnull(v)}) == 1 and \\\n",
    "                (all(pd.notnull(v) for v in values) or all(pd.isnull(v) for v in values))\n",
    "    if both_same:\n",
    "        return values[0]\n",
    "\n",
    "    if 'No' in values and any(pd.isnull(v) for v in values):\n",
    "        return 'No'\n",
    "    if 'Yes' in values and any(pd.isnull(v) for v in values):\n",
    "        return 'Yes'\n",
    "    if 'No' in values and 'Yes' in values:\n",
    "        return None\n",
    "    # all NaN or other unexpected pattern\n",
    "    return None\n",
    "\n",
    "\n",
    "one_day_outcomes_grouped = (\n",
    "    one_day_outcomes\n",
    "      .groupby('VisitCode', as_index=False)\n",
    "      .agg({'one_day_call_outcome': resolve_outcome})   # no inner lambda needed\n",
    ")\n",
    "\n",
    "one_day_outcomes = one_day_outcomes_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visit durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = dfs['visit_provider_durations']\n",
    "durations['duration_minutes'] = (durations['EndDate'] - durations['StartDate']).dt.total_seconds() / 60\n",
    "assert durations.duration_minutes.notna().all()\n",
    "assert durations.duration_minutes.min() > 0\n",
    "\n",
    "total_duration_by_visit = durations.groupby('VisitCode').duration_minutes.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process to merge the data:\n",
    "- Use \"visits\" as the core data frame \n",
    "- Join on \"visits with users\" (should be a completely 1:1 join) to get what users saw that patient and what group those users were in (i.e., the allocation)\n",
    "- Join on \"outcomes\" (left join) to add in the outcomes nicely where those are present for a given patient\n",
    "- Join on \"clinical documentation\" (left join)\n",
    "- Join on \"ai_interaction_scrubbed\" left join to get the final red rate\n",
    "- Join on \"one_day_outcomes\" (left join) to get the deeper outcome data here \n",
    "\n",
    "skip:\n",
    "- Allocations - redundant with visits with users \n",
    "\n",
    "So in sum:\n",
    "- Every visit has basic information about the patient\n",
    "- Every visit has information about the provider, including what group they were in\n",
    "- The vast majority of visits, but not every single visit, has clinical documentation\n",
    "- A fraction of visits have outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_columns_modulo_nans(df, allowed_exceptions: list[tuple[str, str, str]] = []):\n",
    "    for x_col in df.columns:\n",
    "        if x_col.endswith('_x'):\n",
    "            y_col = x_col.replace('_x', '_y')\n",
    "            # Check equality only where y_col is notna\n",
    "            mask = df[y_col].notna()\n",
    "            unequal = df.loc[mask, [x_col, y_col]][df.loc[mask, x_col] != df.loc[mask, y_col]]\n",
    "            if not unequal.empty:\n",
    "                for colname, xval, yval in allowed_exceptions:\n",
    "                    if colname in x_col and colname in y_col:\n",
    "                        unequal = unequal[~((unequal[x_col] == xval) & (unequal[y_col] == yval))]\n",
    "\n",
    "                if not unequal.empty:\n",
    "                    print(f\"Cells where {x_col} and {y_col} differ (showing index and values):\")\n",
    "                    print(unequal)\n",
    "                    raise AssertionError(f\"{x_col} and {y_col} are not the same modulo NaNs in {y_col}\")\n",
    "            df = df.drop(columns=[y_col])\n",
    "            df.rename(columns={x_col: x_col.replace('_x', '')}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = visits\n",
    "\n",
    "assert set(df.VisitCode) >= set(clinical_documentation.VisitCode), \"visits.VisitCode and clinical_documentation.VisitCode are not the same\"\n",
    "df_clinical_documentation = df.merge(clinical_documentation, on='VisitCode', how='left')\n",
    "df_clinical_documentation = deduplicate_columns_modulo_nans(df_clinical_documentation, allowed_exceptions=[('Age', '57y 9m', '56y 2m')]) # one case of a mismatch, allowing it as spurious / a one-off change\n",
    "df_clinical_documentation = df_clinical_documentation.replace(np.nan, None)\n",
    "clinical_documentation_objects = {row['VisitCode']: types.ClinicalDocumentation.model_validate(row.to_dict()) for _, row in df_clinical_documentation.iterrows()}\n",
    "df['ClinicalDocumentation'] = df.VisitCode.map(clinical_documentation_objects)\n",
    "\n",
    "assert set(df.VisitCode) >= set(outcomes.VisitCode), \"visits.VisitCode is not a superset of outcomes.VisitCode\"\n",
    "df = df.merge(outcomes, on='VisitCode', how='left')\n",
    "\n",
    "assert set(df.VisitCode) >= set(visit_aicalls_df.VisitCode), \"visits.VisitCode is not a superset of visit_aicalls_df.VisitCode\"\n",
    "df = df.merge(visit_aicalls_df, on='VisitCode', how='left')\n",
    "\n",
    "assert set(df.VisitCode) >= set(summarized_visits_with_users.VisitCode), \"Expected visits.VisitCode to be a superset of summarized_visits_with_users.VisitCode\"\n",
    "df = df.merge(summarized_visits_with_users, on='VisitCode', how='right')\n",
    "assert len(df) == len(summarized_visits_with_users)\n",
    "\n",
    "assert set(df.VisitCode) <= set(total_duration_by_visit.index), \"Expected visits.VisitCode to be a subset of visit_provider_durations.VisitCode\"\n",
    "df = df.merge(total_duration_by_visit, on='VisitCode', how='left')\n",
    "\n",
    "len_pre = len(df)\n",
    "df = df.merge(one_day_outcomes, on='VisitCode', how='left')\n",
    "len_post = len(df)\n",
    "assert len_pre == len_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop g45 visits\n",
    "df.LocationName.value_counts()\n",
    "g45_dropped_length = len(df[df.LocationName == 'Githurai 45'])\n",
    "print(f'Dropping {g45_dropped_length} visits from Githurai 45')\n",
    "\n",
    "g45_initial_len = len(df)\n",
    "df = df[df.LocationName != 'Githurai 45']\n",
    "g45_final_len = len(df)\n",
    "assert g45_initial_len - g45_dropped_length == g45_final_len\n",
    "all_included_visits_len = g45_final_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop crossover visits\n",
    "crossover_dropped_len = len(df[df.ClinicianGroup == 'Crossover'])\n",
    "print(f\"Dropping {crossover_dropped_len} visits with mixed clinician groups\")\n",
    "crossover_initial_len = len(df)\n",
    "df = df[df.ClinicianGroup.isin(['Non-AI', 'AI'])]\n",
    "crossover_final_len = len(df)\n",
    "assert crossover_final_len == crossover_initial_len - crossover_dropped_len, \"Expected final length to be initial length minus dropped length\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add additional computed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latexvars['rolloutStartDate'] = df.VisitDate.min().strftime(\"%B %-d %Y\")\n",
    "latexvars['rolloutEndDate'] = df.VisitDate.max().strftime(\"%B %-d %Y\")\n",
    "cut_point = datetime.strptime('2025-03-01', '%Y-%m-%d')\n",
    "cut_point_str = cut_point.strftime(\"%B %-d %Y\")\n",
    "latexvars['inductionEndDate'] = (cut_point - timedelta(days=1)).strftime(\"%B %-d %Y\")\n",
    "latexvars['restStudyStartDate'] = cut_point.strftime(\"%B %-d %Y\")\n",
    "\n",
    "df['during_main_study'] = df['VisitDate'] >= cut_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treatment_group(row: pd.Series) -> str:\n",
    "    if row.ClinicianGroup == 'Non-AI':\n",
    "        return 'Non-AI'\n",
    "    elif row.ClinicianGroup == 'AI':\n",
    "        if row.VisitDate >= cut_point:\n",
    "            return 'AI - main study'\n",
    "        else:\n",
    "            return 'AI - induction period'\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid ClinicianGroup: {row.ClinicianGroup}\")\n",
    "\n",
    "df['treatment_group'] = df.apply(treatment_group, axis=1)\n",
    "df.treatment_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_na(x: object) -> TypeIs[NAType]:\n",
    "    return pd.isna(x)\n",
    "\n",
    "def n_relevant_calls(\n",
    "    call: types.AICalls | NAType,\n",
    "    rules: list[types.ClinicalDecisionRule] | None = None,\n",
    "    colors: list[types.Color] | None = None,\n",
    ") -> int | NAType:\n",
    "    if is_na(call):\n",
    "        return pd.NA\n",
    "\n",
    "    if rules is None:\n",
    "        rules = list(types.ClinicalDecisionRule)\n",
    "\n",
    "    if colors is None:\n",
    "        colors = list(types.Color)\n",
    "\n",
    "    relevant_calls = [\n",
    "        x for x in call.calls\n",
    "        if x.rule in rules\n",
    "        and x.color in colors\n",
    "    ]\n",
    "    return len(relevant_calls)\n",
    "\n",
    "df['n_aicalls'] = df.AICalls.map(lambda x: n_relevant_calls(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"week\"] = df[\"VisitDate\"].dt.to_period(\"W\").apply(\n",
    "    lambda r: r.start_time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def risky_cases(\n",
    "    call: types.AICalls | NAType,\n",
    "    rules: list[types.ClinicalDecisionRule],\n",
    "    colors: list[types.Color] | None = None,\n",
    ") -> bool | NAType:\n",
    "    \"\"\"Determine if a visit is a risky case, defined as having at least one red call for the rules provided.\"\"\"\n",
    "    if is_na(call):\n",
    "        return False\n",
    "\n",
    "    if colors is None:\n",
    "        colors = [types.Color.Red]\n",
    "\n",
    "    relevant_calls = [\n",
    "        x for x in call.calls\n",
    "        if x.rule in rules\n",
    "    ]\n",
    "    return any(x.color in colors for x in relevant_calls)\n",
    "\n",
    "df['risky_cases_for_history'] = df.AICalls.map(lambda x: risky_cases(x, rules=[types.ClinicalDecisionRule.VitalsChiefComplaintEvaluation, types.ClinicalDecisionRule.ClinicalNotes]))\n",
    "df['risky_cases_for_investigations'] = df.AICalls.map(lambda x: risky_cases(x, rules=[types.ClinicalDecisionRule.InvestigationRecommendations]))\n",
    "df['risky_cases_for_diagnosis'] = df.AICalls.map(lambda x: risky_cases(x, rules=[types.ClinicalDecisionRule.DiagnosisEvaluation]))\n",
    "df['risky_cases_for_treatment'] = df.AICalls.map(lambda x: risky_cases(x, rules=[types.ClinicalDecisionRule.TreatmentRecommendation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations_with_min_start_date = durations.loc[durations.groupby('VisitCode')['StartDate'].idxmin()]\n",
    "assert durations_with_min_start_date.VisitCode.is_unique\n",
    "earliest_user_id = durations_with_min_start_date[['VisitCode', 'UserId']]\n",
    "earliest_user_id_dict = dict(zip(earliest_user_id.VisitCode, earliest_user_id.UserId))\n",
    "\n",
    "df['user_id'] = df.VisitCode.map(earliest_user_id_dict)\n",
    "\n",
    "df.loc[:, 'weeks_since_start'] = ((df.VisitDate - df.VisitDate.min()).dt.days / 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_start   = min(df.VisitDate)\n",
    "induction_start = study_start          # induction starts same day\n",
    "main_start      = cut_point\n",
    "main_end        = max(df.VisitDate)\n",
    "\n",
    "d2n = mdates.date2num\n",
    "\n",
    "col_induction = '#c7e9c0'  # light green tint\n",
    "col_main      = '#74c476'  # medium green tint\n",
    "col_silent    = '#b3cde3'  # pastel blue\n",
    "\n",
    "# Font sizes\n",
    "label_fs  = 12   # bar labels\n",
    "marker_fs = 12   # milestone labels\n",
    "axis_fs   = 10    # axis tick / group label font\n",
    "\n",
    "# Figure\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "\n",
    "y_active, y_silent, h = 1, 0.5, 0.3\n",
    "\n",
    "# Active bars\n",
    "ax.barh(\n",
    "    y_active,\n",
    "    float(d2n(main_start)) - float(d2n(induction_start)),\n",
    "    left=float(d2n(induction_start)),\n",
    "    height=h,\n",
    "    color=col_induction, edgecolor='none'\n",
    ")\n",
    "ax.barh(\n",
    "    y_active,\n",
    "    float(d2n(main_end)) - float(d2n(main_start)),\n",
    "    left=float(d2n(main_start)),\n",
    "    height=h,\n",
    "    color=col_main, edgecolor='none'\n",
    ")\n",
    "\n",
    "# Silent bar\n",
    "ax.barh(\n",
    "    y_silent,\n",
    "    float(d2n(main_end)) - float(d2n(study_start)),\n",
    "    left=float(d2n(study_start)),\n",
    "    height=h,\n",
    "    color=col_silent, edgecolor='none'\n",
    ")\n",
    "\n",
    "# Bar labels (just phase names)\n",
    "ax.text(\n",
    "    float(d2n(induction_start)) + (float(d2n(main_start)) - float(d2n(induction_start))) / 2,\n",
    "    y_active,\n",
    "    \"Induction\",\n",
    "    ha='center', va='center', fontsize=label_fs, color='black'\n",
    ")\n",
    "ax.text(\n",
    "    float(d2n(main_start)) + (float(d2n(main_end)) - float(d2n(main_start))) / 2,\n",
    "    y_active,\n",
    "    \"Main study\",\n",
    "    ha='center', va='center', fontsize=label_fs, color='black'\n",
    ")\n",
    "ax.text(\n",
    "    float(d2n(study_start)) + (float(d2n(main_end)) - float(d2n(study_start))) / 2 + 1,\n",
    "    y_silent,\n",
    "    \"No CDSS\",\n",
    "    ha='center', va='center', fontsize=label_fs, color='black'\n",
    ")\n",
    "\n",
    "# ---------------- Time Point Markers ----------------\n",
    "marker_y = y_active + 0.28  # closer but slightly up\n",
    "\n",
    "# Horizontal timeline bar\n",
    "ax.hlines(marker_y - 0.03, float(d2n(study_start)), float(d2n(main_end)), color='black', linewidth=1)\n",
    "\n",
    "# Milestones\n",
    "offset = 0.07  # vertical offset for milestone labels\n",
    "ax.axvline(float(d2n(study_start)), color='black', linewidth=1.2, linestyle=':')\n",
    "ax.text(\n",
    "    float(d2n(study_start)),\n",
    "    marker_y + offset,\n",
    "    f\"Start ({study_start.strftime('%b %-d')})\",\n",
    "    ha='center', va='bottom', fontsize=marker_fs\n",
    ")\n",
    "\n",
    "ax.axvline(float(d2n(main_start)), color='black', linewidth=1.2, linestyle=':')\n",
    "ax.text(\n",
    "    float(d2n(main_start)),\n",
    "    marker_y + offset,\n",
    "    f\"Main study begins ({main_start.strftime('%b %-d')})\",\n",
    "    ha='center', va='bottom', fontsize=marker_fs, weight='bold'\n",
    ")\n",
    "\n",
    "ax.axvline(float(d2n(main_end)), color='black', linewidth=1.2, linestyle=':')\n",
    "ax.text(\n",
    "    float(d2n(main_end)),\n",
    "    marker_y + offset,\n",
    "    f\"End ({main_end.strftime('%b %-d')})\",\n",
    "    ha='center', va='bottom', fontsize=marker_fs\n",
    ")\n",
    "\n",
    "# ---------------- Axis & Styling ----------------\n",
    "ax.set_yticks([y_active, y_silent])\n",
    "ax.set_yticklabels([\"AI group\", \"Non-AI group\"], fontsize=axis_fs)\n",
    "\n",
    "# Add xticks every two weeks and set xlabels\n",
    "tick_dates = []\n",
    "tick_labels = []\n",
    "current = study_start\n",
    "while current <= main_end:\n",
    "    tick_dates.append(float(d2n(current)))\n",
    "    tick_labels.append(current.strftime('%b %-d'))\n",
    "    current += timedelta(days=14)\n",
    "\n",
    "ax.set_xticks(tick_dates)\n",
    "ax.set_xticklabels(tick_labels, fontsize=axis_fs)\n",
    "\n",
    "# Add little vertical lines at each xtick\n",
    "for x in tick_dates:\n",
    "    ax.axvline(x, color='gray', linestyle='-', linewidth=0.7, ymin=0, ymax=1, alpha=0.2)\n",
    "\n",
    "ax.set_xlim(float(d2n(study_start)) - 2, float(d2n(main_end)) + 2)\n",
    "\n",
    "# Remove all grid & spines\n",
    "ax.grid(False)\n",
    "for spine in ['top', 'right', 'bottom', 'left']:\n",
    "    ax.spines[spine].set_visible(False)\n",
    "ax.xaxis.set_ticks_position('none')\n",
    "ax.yaxis.set_ticks_position('none')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig('study_timeline', fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latexvars['nTotalVisits'] = 87931\n",
    "latexvars['nEligibleVisits'] = 52409\n",
    "latexvars['nIneligibleVisits'] = 35522\n",
    "assert latexvars['nTotalVisits'] == latexvars['nEligibleVisits'] + latexvars['nIneligibleVisits']\n",
    "\n",
    "latexvars['nOutsideNairobiCounty'] = 3878\n",
    "latexvars['nIneligibleVisitType'] = 29210\n",
    "latexvars['nNotSeenByProvider'] = 2434\n",
    "assert latexvars['nIneligibleVisits'] == latexvars['nOutsideNairobiCounty'] + latexvars['nIneligibleVisitType'] + latexvars['nNotSeenByProvider']\n",
    "\n",
    "latexvars['nConsentedVisits'] = 40745\n",
    "assert latexvars['nConsentedVisits'] == all_included_visits_len\n",
    "latexvars['nSeenByProviderInBothGroups'] = crossover_dropped_len\n",
    "latexvars['nNotConsentedVisits'] = latexvars['nEligibleVisits'] - latexvars['nConsentedVisits']\n",
    "\n",
    "latexvars['nSilentVisits'] = df[df.ClinicianGroup == 'Non-AI'].shape[0]\n",
    "latexvars['nActiveVisits'] = df[df.ClinicianGroup == 'AI'].shape[0]\n",
    "assert len(df) == latexvars['nSilentVisits'] + latexvars['nActiveVisits']\n",
    "\n",
    "def has_outcome_data(s: pd.Series) -> bool:\n",
    "    if pd.notna(s.Q1) or pd.notna(s.Q2):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "latexvars['nDocsSilentVisits'] = df[df.ClinicianGroup == 'Non-AI'].ClinicalDocumentation.notna().sum()\n",
    "latexvars['nOutcomesSilentVisits'] = df[df.ClinicianGroup == 'Non-AI'].apply(has_outcome_data, axis=1).sum()\n",
    "\n",
    "latexvars['nDocsActiveVisits'] = df[df.ClinicianGroup == 'AI'].ClinicalDocumentation.notna().sum()\n",
    "latexvars['nOutcomesActiveVisits'] = df[df.ClinicianGroup == 'AI'].apply(has_outcome_data, axis=1).sum()\n",
    "\n",
    "dot = Digraph(comment=\"AI CDSS Study Flow Diagram (Visits) – v2\", format=\"png\")\n",
    "dot.attr(rankdir=\"TB\", splines=\"ortho\")\n",
    "dot.attr(\"node\",\n",
    "            shape=\"box\",\n",
    "            style=\"filled\",\n",
    "            fontname=\"Times\",\n",
    "            fontsize=\"10\",\n",
    "            color=\"black\",\n",
    "            width=\"2.4\",\n",
    "            fixedsize=\"width\")  # lock width; height auto\n",
    "\n",
    "dot.attr(\"edge\", arrowhead=\"normal\", arrowsize=\"0.7\")\n",
    "\n",
    "# --- Backbone column (col 1) ------------------------------------------\n",
    "dot.node(\"total\", f\"Total visits during study period\\n(n = {latexvars['nTotalVisits']})\", fillcolor=\"white\")\n",
    "latexvars['pctEligible'] = f'{latexvars[\"nEligibleVisits\"] / latexvars[\"nTotalVisits\"] * 100:.1f}%'\n",
    "latexvars['pctConsented'] = f'{latexvars[\"nConsentedVisits\"] / latexvars[\"nEligibleVisits\"] * 100:.1f}%'\n",
    "dot.node(\"eligible\", f\"Eligible visits\\n(n = {latexvars['nEligibleVisits']}, {latexvars['pctEligible']})\", fillcolor=\"white\")\n",
    "dot.node(\"consent\", f\"Visits where patients consented to\\nPenda's general consent form\\n(n = {latexvars['nConsentedVisits']}, {latexvars['pctConsented']})\", fillcolor=\"white\")\n",
    "\n",
    "# Invisible point to fan out edges\n",
    "dot.node(\"split\", \"\", shape=\"point\", width=\"0.01\", height=\"0.01\", color=\"black\")\n",
    "\n",
    "dot.edge(\"total\", \"eligible\")\n",
    "dot.edge(\"eligible\", \"consent\")\n",
    "dot.edge(\"consent\", \"split\", arrowhead=\"none\")\n",
    "\n",
    "# --- Column 4: Excluded / ineligible ----------------------------------\n",
    "latexvars['pctIneligible'] = f'{latexvars[\"nIneligibleVisits\"] / latexvars[\"nTotalVisits\"] * 100:.1f}%'\n",
    "latexvars['pctOutsideNairobiCounty'] = f'{latexvars[\"nOutsideNairobiCounty\"] / latexvars[\"nTotalVisits\"] * 100:.1f}%'\n",
    "latexvars['pctIneligibleVisitType'] = f'{latexvars[\"nIneligibleVisitType\"] / latexvars[\"nTotalVisits\"] * 100:.1f}%'\n",
    "latexvars['pctNotSeenByProvider'] = f'{latexvars[\"nNotSeenByProvider\"] / latexvars[\"nTotalVisits\"] * 100:.1f}%'\n",
    "dot.node(\"ineligible\",\n",
    "            f\"Ineligible visits (n = {latexvars['nIneligibleVisits']}, {latexvars['pctIneligible']})\\n• Outside Nairobi County (n = {latexvars['nOutsideNairobiCounty']}, {latexvars['pctOutsideNairobiCounty']})\\n• Ineligible visit type (n = {latexvars['nIneligibleVisitType']}, {latexvars['pctIneligibleVisitType']})\\n• Not seen by a provider (n = {latexvars['nNotSeenByProvider']}, {latexvars['pctNotSeenByProvider']})\",\n",
    "            fillcolor=\"#eeeeee\")\n",
    "latexvars['pctNotConsented'] = f'{latexvars[\"nNotConsentedVisits\"] / latexvars[\"nEligibleVisits\"] * 100:.1f}%'\n",
    "dot.node(\"nonconsent\",\n",
    "            f\"Did not consent\\n(n = {latexvars['nNotConsentedVisits']}, {latexvars['pctNotConsented']})\",\n",
    "            fillcolor=\"#eeeeee\")\n",
    "latexvars['pctSeenByProviderInBothGroups'] = f'{latexvars[\"nSeenByProviderInBothGroups\"] / latexvars[\"nEligibleVisits\"] * 100:.1f}%'\n",
    "dot.node(\"both\",\n",
    "            f\"Seen by providers in\\nboth groups (excluded)\\n(n = {latexvars['nSeenByProviderInBothGroups']}, {latexvars['pctSeenByProviderInBothGroups']})\",\n",
    "            fillcolor=\"#eeeeee\")\n",
    "\n",
    "# Horizontal arrows from backbone to exclusions\n",
    "with dot.subgraph() as s:\n",
    "    s.attr(rank=\"same\")\n",
    "    s.node(\"eligible\")\n",
    "    s.node(\"ineligible\")\n",
    "dot.edge(\"eligible:e\", \"ineligible:w\", constraint=\"false\")\n",
    "\n",
    "with dot.subgraph() as s:\n",
    "    s.attr(rank=\"same\")\n",
    "    s.node(\"consent\")\n",
    "    s.node(\"nonconsent\")\n",
    "dot.edge(\"consent:e\", \"nonconsent:w\", constraint=\"false\")\n",
    "\n",
    "# Edge from split to 'both' (excluded third branch) - make it go to the top of the 'both' box\n",
    "dot.edge(\"split\", \"both\")\n",
    "\n",
    "# --- Column 2: Silent‑AI branch ---------------------------------------\n",
    "latexvars['pctSilentVisits'] = f'{latexvars[\"nSilentVisits\"] / latexvars[\"nConsentedVisits\"] * 100:.1f}%'\n",
    "latexvars['pctDocsSilentVisits'] = f'{latexvars[\"nDocsSilentVisits\"] / latexvars[\"nSilentVisits\"] * 100:.1f}%'\n",
    "latexvars['pctOutcomesSilentVisits'] = f'{latexvars[\"nOutcomesSilentVisits\"] / latexvars[\"nSilentVisits\"] * 100:.1f}%'\n",
    "dot.node(\"silent\",\n",
    "            f\"Non-AI group visits\\n(n = {latexvars['nSilentVisits']}, {latexvars['pctSilentVisits']})\", fillcolor=\"#d0e7ff\")\n",
    "dot.node(\"docs_silent\",\n",
    "            f\"Documentation available – non-AI group\\n(n = {latexvars['nDocsSilentVisits']}, {latexvars['pctDocsSilentVisits']})\", fillcolor=\"#d0e7ff\")\n",
    "dot.node(\"out_silent\",\n",
    "            f\"Outcome data available – non-AI group\\n(n = {latexvars['nOutcomesSilentVisits']}, {latexvars['pctOutcomesSilentVisits']})\", fillcolor=\"#d0e7ff\")\n",
    "\n",
    "dot.edge(\"split\", \"silent\")\n",
    "dot.edge(\"silent\", \"docs_silent\")\n",
    "dot.edge(\"docs_silent\", \"out_silent\")\n",
    "\n",
    "# --- Column 3: Active‑AI branch ---------------------------------------\n",
    "latexvars['pctActiveVisits'] = f'{latexvars[\"nActiveVisits\"] / latexvars[\"nConsentedVisits\"] * 100:.1f}%'\n",
    "latexvars['pctDocsActiveVisits'] = f'{latexvars[\"nDocsActiveVisits\"] / latexvars[\"nActiveVisits\"] * 100:.1f}%'\n",
    "latexvars['pctOutcomesActiveVisits'] = f'{latexvars[\"nOutcomesActiveVisits\"] / latexvars[\"nActiveVisits\"] * 100:.1f}%'\n",
    "dot.node(\"active\",\n",
    "            f\"AI group visits\\n(n = {latexvars['nActiveVisits']}, {latexvars['pctActiveVisits']})\", fillcolor=\"#ffd6d6\")\n",
    "dot.node(\"docs_active\",\n",
    "            f\"Documentation available – AI group\\n(n = {latexvars['nDocsActiveVisits']}, {latexvars['pctDocsActiveVisits']})\", fillcolor=\"#ffd6d6\")\n",
    "dot.node(\"out_active\",\n",
    "            f\"Outcome data available – AI group\\n(n = {latexvars['nOutcomesActiveVisits']}, {latexvars['pctOutcomesActiveVisits']})\", fillcolor=\"#ffd6d6\")\n",
    "\n",
    "dot.edge(\"split\", \"active\")\n",
    "dot.edge(\"active\", \"docs_active\")\n",
    "dot.edge(\"docs_active\", \"out_active\")\n",
    "\n",
    "# --- Align columns with invisible edges (to keep grid) ----------------\n",
    "# Ensure columns line vertical by connecting invisible edges downwards in ineligible column\n",
    "dot.node(\"excl_dummy1\", \"\", shape=\"plaintext\", width=\"0\", height=\"0\")\n",
    "dot.node(\"excl_dummy2\", \"\", shape=\"plaintext\", width=\"0\", height=\"0\")\n",
    "dot.edge(\"ineligible\", \"nonconsent\", style=\"invis\")\n",
    "dot.edge(\"nonconsent\", \"both\", style=\"invis\")\n",
    "\n",
    "# Render PNG to bytes\n",
    "png_bytes = dot.pipe(format='png')\n",
    "\n",
    "# Display from in-memory bytes\n",
    "display(Image(data=png_bytes))\n",
    "\n",
    "bf.write_bytes(plots_folder + \"flow_diagram.png\", png_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.LocationName.unique()\n",
    "\n",
    "location_name_map = {\n",
    "    'Tassia': 'Eastlands',\n",
    "    'Umoja 1': 'Eastlands',\n",
    "    'Umoja 2': 'Eastlands',\n",
    "    'Embakasi': 'Eastlands',\n",
    "    'Pipeline': 'Eastlands',\n",
    "    'Kangemi': 'Southwest',\n",
    "    'Kawangware': 'Southwest',\n",
    "    'Kimathi Street': 'Southwest',\n",
    "    \"Lang'ata\": 'Southwest',\n",
    "    'Mathare North': 'Thika Road Corridor',\n",
    "    'Kasarani': 'Thika Road Corridor',\n",
    "    'Sunton': 'Thika Road Corridor',\n",
    "    'Lucky Summer': 'Thika Road Corridor',\n",
    "    'Zimmerman': 'Thika Road Corridor',\n",
    "    'Kahawa West': 'Thika Road Corridor',\n",
    "}\n",
    "\n",
    "df['LocationGroup'] = df['LocationName'].map(location_name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table 1: demographics\n",
    "\n",
    "def _age_to_years(age_str: str | float | int) -> float:\n",
    "    \"\"\"\n",
    "    Convert strings like “29y 10m” / “3y” / “8m” into decimal years.\n",
    "    If already numeric (float / int) or NaN, return as-is.\n",
    "    \"\"\"\n",
    "    if pd.isna(age_str):\n",
    "        return np.nan\n",
    "    if isinstance(age_str, (int, float)):\n",
    "        return float(age_str)\n",
    "\n",
    "    s = str(age_str).strip().lower()\n",
    "    yrs, mths = 0, 0\n",
    "    m = re.match(r'(?:(\\d+)\\s*y)?\\s*(?:(\\d+)\\s*m)?', s)\n",
    "    if m:\n",
    "        if m.group(1):\n",
    "            yrs = int(m.group(1))\n",
    "        if m.group(2):\n",
    "            mths = int(m.group(2))\n",
    "    return yrs + mths / 12\n",
    "\n",
    "\n",
    "def _fmt_n_pct(n: int, denom: int) -> str:\n",
    "    pct = 100 * n / denom if denom else 0\n",
    "    return f\"{n:,} ({pct:0.1f}%)\"\n",
    "\n",
    "\n",
    "def _fmt_mean_sd(series: pd.Series) -> str:\n",
    "    return f\"{series.mean():0.1f} ± {series.std(ddof=0):0.1f}\"\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Derive / clean variables\n",
    "# --------------------------------------------------------------------------- #\n",
    "df['VisitDate'] = pd.to_datetime(df['VisitDate'])\n",
    "df['AgeYears'] = df['Age'].apply(_age_to_years)\n",
    "\n",
    "df[\"Period\"] = np.where(\n",
    "    df[\"VisitDate\"] < cut_point,\n",
    "    f\"Visits before {cut_point_str}\",\n",
    "    f\"Visits {cut_point_str} or later\",\n",
    ")\n",
    "\n",
    "df[\"Payor\"] = df[\"VisitType\"].replace({\"Insurance\": \"Insurance\", \"Cash\": \"Cash\"}).fillna(\n",
    "    \"Other\"\n",
    ")\n",
    "\n",
    "df[\"RespondedTo8DayCall\"] = df[\"CallDate\"].notna().map({True: \"Yes\", False: \"No\"})\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Sub-cohort to display and helper containers\n",
    "# --------------------------------------------------------------------------- #\n",
    "cohort_df = df[df[\"ClinicianGroup\"].isin([\"AI\", \"Non-AI\"])].copy()\n",
    "groups = [\"AI\", \"Non-AI\"]\n",
    "group_counts = (\n",
    "    cohort_df[\"ClinicianGroup\"]\n",
    "    .value_counts()\n",
    "    .reindex(groups)\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "rows: list[dict] = []\n",
    "section_breaks: list[int] = []  # index after which to insert a blank line\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Helper functions for building the table\n",
    "# --------------------------------------------------------------------------- #\n",
    "def _add_total_n_row() -> None:\n",
    "    row = {\"Variable\": \"n\"}\n",
    "    for grp in groups:\n",
    "        row[grp] = f\"{group_counts[grp]:,}\"\n",
    "    rows.append(row)\n",
    "\n",
    "\n",
    "def _add_categorical(\n",
    "    var: str,\n",
    "    *,\n",
    "    levels: list[str] | None = None,\n",
    "    custom_labels: dict[str, str] | None = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Add one row per level of the categorical variable `var`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    var : str\n",
    "        Column name.\n",
    "    levels : list[str] | None\n",
    "        Display order.  If None, inferred from value_counts().\n",
    "    custom_labels : dict[str, str] | None\n",
    "        Mapping ``raw_value -> label`` to override default “<var>: <value>” text.\n",
    "    \"\"\"\n",
    "    if levels is None:\n",
    "        levels = cohort_df[var].dropna().value_counts().index.tolist()\n",
    "\n",
    "    for lvl in levels:\n",
    "        label = custom_labels.get(lvl) if custom_labels else None\n",
    "        if label is None:\n",
    "            label = f\"{var}: {lvl}\"\n",
    "\n",
    "        row = {\"Variable\": label}\n",
    "        for grp in groups:\n",
    "            n = cohort_df[\n",
    "                (cohort_df[\"ClinicianGroup\"] == grp) & (cohort_df[var] == lvl)\n",
    "            ].shape[0]\n",
    "            row[grp] = _fmt_n_pct(n, group_counts[grp])\n",
    "        rows.append(row)\n",
    "\n",
    "\n",
    "def _fmt_median_iqr(series: pd.Series) -> str:\n",
    "    median = series.median()\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    return f\"{median:0.1f} [{q1:0.1f}, {q3:0.1f}]\"\n",
    "\n",
    "\n",
    "def _add_continuous(series_name: str, display_name: str) -> None:\n",
    "    row = {\"Variable\": display_name}\n",
    "    for grp in groups:\n",
    "        series = cohort_df[cohort_df[\"ClinicianGroup\"] == grp][series_name]\n",
    "        row[grp] = _fmt_median_iqr(series)\n",
    "    rows.append(row)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Build the descriptive table\n",
    "# --------------------------------------------------------------------------- #\n",
    "# 1. Study period\n",
    "_add_total_n_row()\n",
    "section_breaks.append(len(rows))\n",
    "\n",
    "_add_categorical(\n",
    "    \"Period\",\n",
    "    levels=[\n",
    "        f\"Visits before {cut_point_str}\",\n",
    "        f\"Visits {cut_point_str} or later\",\n",
    "    ],\n",
    "    custom_labels={\n",
    "        f\"Visits before {cut_point_str}\": f\"Induction period (before {cut_point_str})\",\n",
    "        f\"Visits {cut_point_str} or later\": f\"Main study period ({cut_point_str} or later)\",\n",
    "    },\n",
    ")\n",
    "section_breaks.append(len(rows))\n",
    "\n",
    "# 2. Location\n",
    "_add_categorical(\n",
    "    \"LocationGroup\",\n",
    "    levels=[\"Eastlands\", \"Southwest\", \"Thika Road Corridor\"],\n",
    "    custom_labels={\"Eastlands\": \"Visit location: Eastlands clinics\", \"Southwest\": \"Visit location: Southwest clinics\", \"Thika Road Corridor\": \"Visit location: Thika Road Corridor clinics\"},\n",
    ")\n",
    "section_breaks.append(len(rows))\n",
    "\n",
    "# 2. Age\n",
    "_add_continuous(\"AgeYears\", \"Age (years), median [q25, q75]\")\n",
    "section_breaks.append(len(rows))\n",
    "\n",
    "# 3. Gender\n",
    "_add_categorical(\n",
    "    \"Gender\",\n",
    "    levels=[\"Female\", \"Male\"],\n",
    "    custom_labels={\"Female\": \"Female\", \"Male\": \"Male\"},\n",
    ")\n",
    "section_breaks.append(len(rows))\n",
    "\n",
    "# 4. Payor\n",
    "_add_categorical(\n",
    "    \"Payor\",\n",
    "    levels=[\"Insurance\", \"Cash\"],\n",
    "    custom_labels={\n",
    "        \"Insurance\": \"Insurance visit\",\n",
    "        \"Cash\": \"Cash visit\",\n",
    "    },\n",
    ")\n",
    "section_breaks.append(len(rows))\n",
    "\n",
    "# 5. Responded to 8-day follow-up call\n",
    "_add_categorical(\n",
    "    \"RespondedTo8DayCall\",\n",
    "    levels=[\"Yes\", \"No\"],\n",
    "    custom_labels={\n",
    "        \"Yes\": \"Did respond to 8-day follow-up call\",\n",
    "        \"No\": \"Did not respond to 8-day follow-up call\",\n",
    "    },\n",
    ")\n",
    "section_breaks.append(len(rows))\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Insert blank rows between thematic sections\n",
    "# --------------------------------------------------------------------------- #\n",
    "rows_with_blanks: list[dict] = []\n",
    "last_break = 0\n",
    "for i, break_idx in enumerate(section_breaks):\n",
    "    rows_with_blanks.extend(rows[last_break:break_idx])\n",
    "    if i < len(section_breaks) - 1:\n",
    "        rows_with_blanks.append({\"Variable\": \"\", \"AI\": \"\", \"Non-AI\": \"\"})\n",
    "    last_break = break_idx\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Final DataFrame\n",
    "# --------------------------------------------------------------------------- #\n",
    "table1_df = (\n",
    "    pd.DataFrame(rows_with_blanks)\n",
    "    .set_index(\"Variable\")[\n",
    "        [\"Non-AI\", \"AI\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(table1_df)\n",
    "save_latex('table1', table1_df)\n",
    "\n",
    "# Age: median and IQR for each group\n",
    "age_row = table1_df.loc[\"Age (years), median [q25, q75]\"]\n",
    "def _parse_median_iqr(val):\n",
    "    m = re.match(r\"([0-9.]+) \\[([0-9.]+), ([0-9.]+)\\]\", val)\n",
    "    if m:\n",
    "        return m.group(1), m.group(2), m.group(3)\n",
    "    return None, None, None\n",
    "\n",
    "medianAgeSilent, ageQFirstSilent, ageQThirdSilent = _parse_median_iqr(age_row[\"Non-AI\"])\n",
    "medianAgeActive, ageQFirstActive, ageQThirdActive = _parse_median_iqr(age_row[\"AI\"])\n",
    "latexvars[\"medianAgeSilent\"] = medianAgeSilent\n",
    "latexvars[\"ageQFirstSilent\"] = ageQFirstSilent\n",
    "latexvars[\"ageQThirdSilent\"] = ageQThirdSilent\n",
    "latexvars[\"medianAgeActive\"] = medianAgeActive\n",
    "latexvars[\"ageQFirstActive\"] = ageQFirstActive\n",
    "latexvars[\"ageQThirdActive\"] = ageQThirdActive\n",
    "\n",
    "# Gender: percent female for each group\n",
    "female_row = table1_df.loc[\"Female\"]\n",
    "def _extract_pct(val):\n",
    "    m = re.search(r\"\\(([\\d.]+)%\\)\", val)\n",
    "    return f\"{float(m.group(1)):0.1f}\\\\%\" if m else \"\"\n",
    "latexvars[\"pctFemaleSilent\"] = _extract_pct(female_row[\"Non-AI\"])\n",
    "latexvars[\"pctFemaleActive\"] = _extract_pct(female_row[\"AI\"])\n",
    "\n",
    "# Payor: percent insurance for each group\n",
    "insurance_row = table1_df.loc[\"Insurance visit\"]\n",
    "latexvars[\"pctInsuranceSilent\"] = _extract_pct(insurance_row[\"Non-AI\"])\n",
    "latexvars[\"pctInsuranceActive\"] = _extract_pct(insurance_row[\"AI\"])\n",
    "\n",
    "# 8-day follow-up: percent responded for each group\n",
    "followup_row = table1_df.loc[\"Did respond to 8-day follow-up call\"]\n",
    "latexvars[\"pctFollowupSilent\"] = _extract_pct(followup_row[\"Non-AI\"])\n",
    "latexvars[\"pctFollowupActive\"] = _extract_pct(followup_row[\"AI\"])\n",
    "\n",
    "# Location: percent for each region and group\n",
    "eastlands_row = table1_df.loc[\"Visit location: Eastlands clinics\"]\n",
    "southwest_row = table1_df.loc[\"Visit location: Southwest clinics\"]\n",
    "thika_row = table1_df.loc[\"Visit location: Thika Road Corridor clinics\"]\n",
    "latexvars[\"pctEastlandsSilent\"] = _extract_pct(eastlands_row[\"Non-AI\"])\n",
    "latexvars[\"pctEastlandsActive\"] = _extract_pct(eastlands_row[\"AI\"])\n",
    "latexvars[\"pctSouthwestSilent\"] = _extract_pct(southwest_row[\"Non-AI\"])\n",
    "latexvars[\"pctSouthwestActive\"] = _extract_pct(southwest_row[\"AI\"])\n",
    "latexvars[\"pctThikaSilent\"] = _extract_pct(thika_row[\"Non-AI\"])\n",
    "latexvars[\"pctThikaActive\"] = _extract_pct(thika_row[\"AI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median and IQR (q25, q75) number of rows per UserID, and compare between the two groups\n",
    "user_group_counts = (\n",
    "    df[['UserIDs', 'ClinicianGroup']]\n",
    "    .explode('UserIDs')\n",
    "    .dropna(subset=['UserIDs', 'ClinicianGroup'])\n",
    "    .groupby(['ClinicianGroup', 'UserIDs'])\n",
    "    .size()\n",
    "    .rename('n_rows')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Median and IQR (q25, q75) number of rows per UserID in each ClinicianGroup, plus number of unique UserIDs\n",
    "agg_rows_per_userid = (\n",
    "    user_group_counts.groupby('ClinicianGroup')['n_rows']\n",
    "    .agg(\n",
    "        median='median',\n",
    "        q25=lambda x: x.quantile(0.25),\n",
    "        q75=lambda x: x.quantile(0.75)\n",
    "    )\n",
    ")\n",
    "\n",
    "median_iqr_series = agg_rows_per_userid.apply(\n",
    "    lambda row: f\"{int(row['median'])} [{int(row['q25'])}-{int(row['q75'])}]\", axis=1\n",
    ")\n",
    "\n",
    "# Number of unique UserIDs in each ClinicianGroup\n",
    "unique_userids_per_group = (\n",
    "    user_group_counts.groupby('ClinicianGroup')['UserIDs'].nunique()\n",
    ")\n",
    "\n",
    "# Concatenate unique_userids_per_group and median_iqr_series into a single DataFrame for summary\n",
    "userids_and_median_iqr = pd.concat(\n",
    "    [unique_userids_per_group.rename(\"n providers\"), median_iqr_series.rename(\"median [q25-q75]\")],\n",
    "    axis=1\n",
    ")\n",
    "userids_and_median_iqr = userids_and_median_iqr.rename_axis(\"Group\")\n",
    "\n",
    "latexvars['nSilentAI'] = unique_userids_per_group['Non-AI']\n",
    "nsilentai = unique_userids_per_group['Non-AI']\n",
    "latexvars['nActiveAI'] = unique_userids_per_group['AI']\n",
    "nactiveai = unique_userids_per_group['AI']\n",
    "latexvars['nProviders'] = unique_userids_per_group['Non-AI'] + unique_userids_per_group['AI']\n",
    "\n",
    "latexvars[\"visitsPerProviderMedSilent\"] = f\"{agg_rows_per_userid['median']['Non-AI']:.0f}\"\n",
    "latexvars[\"visitsPerProviderQFirstSilent\"] = f\"{agg_rows_per_userid['q25']['Non-AI']:.0f}\"\n",
    "latexvars[\"visitsPerProviderQThirdSilent\"] = f\"{agg_rows_per_userid['q75']['Non-AI']:.0f}\"\n",
    "latexvars[\"visitsPerProviderMedActive\"] = f\"{agg_rows_per_userid['median']['AI']:.0f}\"\n",
    "latexvars[\"visitsPerProviderQFirstActive\"] = f\"{agg_rows_per_userid['q25']['AI']:.0f}\"\n",
    "latexvars[\"visitsPerProviderQThirdActive\"] = f\"{agg_rows_per_userid['q75']['AI']:.0f}\"\n",
    "\n",
    "\n",
    "save_latex('table2', userids_and_median_iqr)\n",
    "userids_and_median_iqr.to_latex()\n",
    "userids_and_median_iqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clinician rater study analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonl_load(file_path: str) -> list[dict]:\n",
    "    records: list[dict] = []\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            try:\n",
    "                records.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                raise ValueError(f\"Invalid JSON on line {len(records)+1} of {file_path!s}\") from e\n",
    "\n",
    "    return records\n",
    "\n",
    "results_clinical_study = jsonl_load(clinical_study_results_fp)\n",
    "results_clinical_study_df = pd.DataFrame(results_clinical_study)\n",
    "results_clinical_study_df.rename(columns={'form_a_likert': 'history_likert', 'form_b_likert': 'investigations_likert', 'form_c_likert': 'diagnosis_likert', 'form_d_likert': 'treatment_likert'}, inplace=True)\n",
    "\n",
    "results_clinical_study_df = results_clinical_study_df.merge(df, left_on='visit_code', right_on='VisitCode', how='left')\n",
    "results_clinical_study_df.dropna(subset = ['ClinicianGroup'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trainer_emails = results_clinical_study_df.trainer_email.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stats for the paper\n",
    "n_total_ratings = len(results_clinical_study)\n",
    "n_unique_visits_rated = len({state['visit_code'] for state in results_clinical_study})\n",
    "visit_code_counts = Counter(state['visit_code'] for state in results_clinical_study)\n",
    "visit_code_count_counts = Counter(visit_code_counts.values())\n",
    "n_visits_single_rated = visit_code_count_counts[1]\n",
    "n_visits_double_rated = visit_code_count_counts[2]\n",
    "assert n_unique_visits_rated == n_visits_single_rated + n_visits_double_rated\n",
    "\n",
    "latexvars['nTotalHumanRatings'] = n_total_ratings\n",
    "latexvars['nUniqueVisitsHumanRated'] = n_unique_visits_rated\n",
    "latexvars['nVisitsSingleRated'] = n_visits_single_rated\n",
    "latexvars['nVisitsDoubleRated'] = n_visits_double_rated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intify_likert(likert: str | int | float | None) -> int:\n",
    "    \"\"\"\n",
    "    Convert a Likert value stored either as an int (1–5) or as a string whose\n",
    "    first character is the integer code.  Raises ValueError otherwise.\n",
    "\n",
    "    NOTE: `None` values are interpreted as NaN and will raise.\n",
    "    \"\"\"\n",
    "    if isinstance(likert, str):\n",
    "        try:\n",
    "            return_val = int(likert[0])\n",
    "        except (TypeError, ValueError):\n",
    "            raise ValueError(\n",
    "                f\"Str value for Likert that could not be converted to int by taking first character: {likert=}\"\n",
    "            )\n",
    "    elif isinstance(likert, int):\n",
    "        return_val = likert\n",
    "    elif isinstance(likert, float):\n",
    "        assert likert.is_integer(), f\"Invalid Likert value: {likert}\"\n",
    "        return_val = int(likert)\n",
    "    else:  # includes `None`\n",
    "        raise ValueError(f\"Invalid value type: {type(likert)}, {likert=}\")\n",
    "\n",
    "    assert 1 <= return_val <= 5, f\"Invalid Likert value: {return_val}\"\n",
    "    return return_val\n",
    "\n",
    "for col in ['history_likert', 'investigations_likert', 'diagnosis_likert', 'treatment_likert']:\n",
    "    results_clinical_study_df[col] = results_clinical_study_df[col].apply(intify_likert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero out history Likert and MCQ for VisitDate > last_visitdate_with_cc\n",
    "results_clinical_study_df.loc[results_clinical_study_df.VisitDate > last_visitdate_with_cc, ['history_likert']] = pd.NA\n",
    "results_clinical_study_df.loc[results_clinical_study_df.VisitDate > last_visitdate_with_cc, ['history_mcq']] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_low_likert(likert: int | NAType) -> int | NAType:\n",
    "    if is_na(likert):\n",
    "        return pd.NA\n",
    "\n",
    "    if isinstance(likert, float):\n",
    "        if not likert.is_integer():\n",
    "            raise ValueError(f\"likert must be an int, got {type(likert)}. Likert: {likert}\")\n",
    "        likert = int(likert)\n",
    "\n",
    "    if not isinstance(likert, int):\n",
    "        raise ValueError(f\"likert must be an int, got {type(likert)}. Likert: {likert}\")\n",
    "\n",
    "    return 1 if likert <= 2 else 0\n",
    "\n",
    "results_clinical_study_df['history_likert_is_low'] = results_clinical_study_df['history_likert'].apply(is_low_likert)\n",
    "results_clinical_study_df['investigations_likert_is_low'] = results_clinical_study_df['investigations_likert'].apply(is_low_likert)\n",
    "results_clinical_study_df['diagnosis_likert_is_low'] = results_clinical_study_df['diagnosis_likert'].apply(is_low_likert)\n",
    "results_clinical_study_df['treatment_likert_is_low'] = results_clinical_study_df['treatment_likert'].apply(is_low_likert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get acuity nice\n",
    "def get_acuity(mcq_str: str) -> str:\n",
    "    if mcq_str.startswith('Medium'):\n",
    "        return 'Medium'\n",
    "    elif mcq_str.startswith('High'):\n",
    "        return 'High'\n",
    "    elif mcq_str.startswith('Low'):\n",
    "        return 'Low'\n",
    "    else:\n",
    "        raise ValueError(f'Unknown acuity: {mcq_str}')\n",
    "\n",
    "results_clinical_study_df['acuity'] = results_clinical_study_df.form_e_mcq.apply(get_acuity)\n",
    "results_clinical_study_df.drop(columns=['form_e_mcq'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert results_clinical_study_df['VisitCode'].value_counts().isin([1, 2]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add row weights for duplicated visits\n",
    "results_clinical_study_df['row_weight'] = np.where(results_clinical_study_df.duplicated(\"VisitCode\", keep=False), 0.5, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoByTwoTable(BaseModel):\n",
    "    group_one_true_count: int\n",
    "    group_one_false_count: int\n",
    "    group_two_true_count: int\n",
    "    group_two_false_count: int\n",
    "\n",
    "    @property\n",
    "    def group_one_n(self) -> int:\n",
    "        return self.group_one_true_count + self.group_one_false_count\n",
    "\n",
    "    @property\n",
    "    def group_two_n(self) -> int:\n",
    "        return self.group_two_true_count + self.group_two_false_count\n",
    "\n",
    "def get_2x2_table(\n",
    "    df: pd.DataFrame,\n",
    "    allocation_col: str,\n",
    "    allocation_group_one_values: list,\n",
    "    allocation_group_two_values: list,\n",
    "    outcome_col: str,\n",
    "    outcome_true_values: list,\n",
    "    outcome_false_values: list,\n",
    "    unique_column: str | None = None\n",
    ") -> TwoByTwoTable:\n",
    "    \"\"\"\n",
    "    Gets a 2x2 table of counts for a given allocation and outcome.\n",
    "\n",
    "    Ignores rows for which the allocation column is not in the allocation_group_one_values or allocation_group_two_values, and rows for which the outcome column is not in the outcome_true_values or outcome_false_values.\n",
    "\n",
    "    If unique_column is provided, then if there are multiple observations for a given unique_column, then these observations are averaged, and the resulting counts rounded to the nearest integer.\n",
    "    \"\"\"\n",
    "    relevant_df = df[\n",
    "        (df[allocation_col].isin(allocation_group_one_values) | df[allocation_col].isin(allocation_group_two_values))\n",
    "        & (df[outcome_col].isin(outcome_true_values) | df[outcome_col].isin(outcome_false_values))\n",
    "    ]\n",
    "\n",
    "    if unique_column is not None:\n",
    "        unique_column_counts = Counter(relevant_df[unique_column])\n",
    "    else:\n",
    "        unique_column_counts = {}\n",
    "\n",
    "    group_one_true_count = 0\n",
    "    group_one_false_count = 0\n",
    "    group_two_true_count = 0\n",
    "    group_two_false_count = 0\n",
    "    for _, row in relevant_df.iterrows():\n",
    "        allocation_value = row[allocation_col]\n",
    "        outcome_value = row[outcome_col]\n",
    "\n",
    "        if unique_column is not None:\n",
    "            unique_column_value = row[unique_column]\n",
    "            weight = 1.0 / unique_column_counts[unique_column_value]\n",
    "        else:\n",
    "            weight = 1.0\n",
    "\n",
    "        if allocation_value in allocation_group_one_values:\n",
    "            if outcome_value in outcome_true_values:\n",
    "                group_one_true_count += weight\n",
    "            elif outcome_value in outcome_false_values:\n",
    "                group_one_false_count += weight\n",
    "            else:\n",
    "                raise ValueError(f\"Outcome value {outcome_value} not in outcome_true_values or outcome_false_values\")\n",
    "        elif allocation_value in allocation_group_two_values:\n",
    "            if outcome_value in outcome_true_values:\n",
    "                group_two_true_count += weight\n",
    "            elif outcome_value in outcome_false_values:\n",
    "                group_two_false_count += weight\n",
    "            else:\n",
    "                raise ValueError(f\"Outcome value {outcome_value} not in outcome_true_values or outcome_false_values\")\n",
    "        else:\n",
    "            raise ValueError(f\"Allocation value {allocation_value} not in allocation_group_one_values or allocation_group_two_values\")\n",
    "\n",
    "    group_one_true_count = round(group_one_true_count)\n",
    "    group_one_false_count = round(group_one_false_count)\n",
    "    group_two_true_count = round(group_two_true_count)\n",
    "    group_two_false_count = round(group_two_false_count)\n",
    "\n",
    "    return TwoByTwoTable(\n",
    "        group_one_true_count=group_one_true_count,\n",
    "        group_one_false_count=group_one_false_count,\n",
    "        group_two_true_count=group_two_true_count,\n",
    "        group_two_false_count=group_two_false_count,\n",
    "    )\n",
    "\n",
    "def get_2x2_stats(\n",
    "    table: TwoByTwoTable,\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Get 2x2 stats for a given allocation and outcome.\n",
    "    \"\"\"\n",
    "\n",
    "    group_one_binomial_test = binomtest(table.group_one_true_count, table.group_one_n)\n",
    "    group_one_rate = group_one_binomial_test.statistic\n",
    "    group_one_ci = group_one_binomial_test.proportion_ci(confidence_level=0.95, method='wilson')\n",
    "    group_one_ci_lower = group_one_ci.low\n",
    "    group_one_ci_upper = group_one_ci.high\n",
    "\n",
    "    group_two_binomial_test = binomtest(table.group_two_true_count, table.group_two_n)\n",
    "    group_two_rate = group_two_binomial_test.statistic\n",
    "    group_two_ci = group_two_binomial_test.proportion_ci(confidence_level=0.95, method='wilson')\n",
    "    group_two_ci_lower = group_two_ci.low\n",
    "    group_two_ci_upper = group_two_ci.high\n",
    "\n",
    "    fisher_exact_test = fisher_exact(np.array([[table.group_one_true_count, table.group_one_false_count], [table.group_two_true_count, table.group_two_false_count]]))\n",
    "    fisher_exact_p_value = fisher_exact_test.pvalue\n",
    "\n",
    "    relative_risk_test = relative_risk(\n",
    "        exposed_cases=table.group_two_true_count,\n",
    "        exposed_total=table.group_two_n,\n",
    "        control_cases=table.group_one_true_count,\n",
    "        control_total=table.group_one_n,\n",
    "    )\n",
    "    rr = relative_risk_test.relative_risk\n",
    "    relative_risk_ci = relative_risk_test.confidence_interval(confidence_level=0.95)\n",
    "    relative_risk_ci_lower = relative_risk_ci.low\n",
    "    relative_risk_ci_upper = relative_risk_ci.high\n",
    "\n",
    "    relative_risk_reduction = 1 - rr\n",
    "    relative_risk_reduction_ci_lower = 1 - relative_risk_ci.high\n",
    "    relative_risk_reduction_ci_upper = 1 - relative_risk_ci.low\n",
    "\n",
    "    symmetric_difference = (group_one_rate - group_two_rate) / ((group_one_rate + group_two_rate) / 2) if (group_one_rate + group_two_rate) > 0 else pd.NA\n",
    "\n",
    "    absolute_risk_reduction = group_one_rate - group_two_rate\n",
    "    if absolute_risk_reduction > 0:\n",
    "        nnt = 1 / absolute_risk_reduction\n",
    "    else:\n",
    "        nnt = np.inf\n",
    "\n",
    "    return {\n",
    "        'first_group_y': table.group_one_true_count,\n",
    "        'first_group_n': table.group_one_n,\n",
    "        'first_group_rate': group_one_rate,\n",
    "        'first_group_lower_CI': group_one_ci_lower,\n",
    "        'first_group_upper_CI': group_one_ci_upper,\n",
    "        'second_group_y': table.group_two_true_count,\n",
    "        'second_group_n': table.group_two_n,\n",
    "        'second_group_rate': group_two_rate,\n",
    "        'second_group_lower_CI': group_two_ci_lower,\n",
    "        'second_group_upper_CI': group_two_ci_upper,\n",
    "        'PVal': fisher_exact_p_value,\n",
    "        'relative_risk': rr,\n",
    "        'relative_risk_lower_CI': relative_risk_ci_lower,\n",
    "        'relative_risk_upper_CI': relative_risk_ci_upper,\n",
    "        'RRR': relative_risk_reduction,\n",
    "        'RRR_low_CI': relative_risk_reduction_ci_lower,\n",
    "        'RRR_high_CI': relative_risk_reduction_ci_upper,\n",
    "        'ARR': absolute_risk_reduction,\n",
    "        'NNT': nnt,\n",
    "        'symmetric_difference': symmetric_difference,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = get_2x2_table(\n",
    "    df = results_clinical_study_df[results_clinical_study_df.VisitDate >= cut_point],\n",
    "    allocation_col = 'ClinicianGroup',\n",
    "    allocation_group_one_values = ['Non-AI'],\n",
    "    allocation_group_two_values = ['AI'],\n",
    "    outcome_col = 'treatment_likert',\n",
    "    outcome_true_values = [1, 2],\n",
    "    outcome_false_values = [3, 4, 5],\n",
    "    unique_column = 'VisitCode'\n",
    ")\n",
    "stats = get_2x2_stats(table)\n",
    "\n",
    "def get_likert_bar_plot_data(\n",
    "    df: pd.DataFrame | list[pd.DataFrame],\n",
    "    allocation_col: str | list[str] = 'ClinicianGroup',\n",
    "    allocation_group_one_values = ['Non-AI'],\n",
    "    allocation_group_two_values = ['AI'],\n",
    "    outcome_cols = ['history_likert', 'investigations_likert', 'diagnosis_likert', 'treatment_likert'],\n",
    "    outcome_true_values = [1, 2],\n",
    "    outcome_false_values = [3, 4, 5],\n",
    "    unique_column: str | None = 'VisitCode'\n",
    ") -> dict[str, dict[str, float]]:\n",
    "    if isinstance(allocation_col, str):\n",
    "        allocation_col = [allocation_col] * len(outcome_cols)\n",
    "\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        df = [df] * len(outcome_cols)\n",
    "\n",
    "    return {\n",
    "        oc.replace('_likert', '').capitalize():\n",
    "        get_2x2_stats(get_2x2_table(\n",
    "            df = d,\n",
    "            allocation_col = ac,\n",
    "            allocation_group_one_values = allocation_group_one_values,\n",
    "            allocation_group_two_values = allocation_group_two_values,\n",
    "            outcome_col = oc,\n",
    "            outcome_true_values = outcome_true_values,\n",
    "            outcome_false_values = outcome_false_values,\n",
    "            unique_column = unique_column\n",
    "        ))\n",
    "        for d, ac, oc in zip(df, allocation_col, outcome_cols, strict=True)\n",
    "    }\n",
    "\n",
    "def get_rrr_bar_plot_data(\n",
    "    dfs: list[pd.DataFrame] | None = None,\n",
    "    likert_bar_plot_data_sets: list[dict[str, dict[str, float]]] | None = None,\n",
    "    allocation_col = 'ClinicianGroup',\n",
    "    allocation_group_one_values = ['Non-AI'],\n",
    "    allocation_group_two_values = ['AI'],\n",
    "    outcome_cols = ['history_likert', 'investigations_likert', 'diagnosis_likert', 'treatment_likert'],\n",
    "    outcome_true_values = [1, 2],\n",
    "    outcome_false_values = [3, 4, 5],\n",
    "    unique_column = 'VisitCode'\n",
    ") -> dict[str, dict[str, float]]:\n",
    "    assert dfs is not None or likert_bar_plot_data_sets is not None, \"Either dfs or likert_bar_plot_data_sets must be provided\"\n",
    "    assert dfs is None or likert_bar_plot_data_sets is None, \"Only one of dfs or likert_bar_plot_data_sets can be provided\"\n",
    "\n",
    "    if dfs is not None:\n",
    "        assert len(dfs) == 2, \"Only two dataframes are supported\"\n",
    "        likert_bar_plot_data_sets = [\n",
    "            get_likert_bar_plot_data(\n",
    "                df = df,\n",
    "                allocation_col = allocation_col,\n",
    "                allocation_group_one_values = allocation_group_one_values,\n",
    "                allocation_group_two_values = allocation_group_two_values,\n",
    "                outcome_cols = outcome_cols,\n",
    "                outcome_true_values = outcome_true_values,\n",
    "                outcome_false_values = outcome_false_values,\n",
    "                unique_column = unique_column\n",
    "            )\n",
    "            for df in dfs\n",
    "        ]\n",
    "\n",
    "    assert likert_bar_plot_data_sets is not None, \"likert_bar_plot_data_sets is provided\"\n",
    "\n",
    "    categories = list(likert_bar_plot_data_sets[0].keys())\n",
    "\n",
    "    return {\n",
    "        category: {\n",
    "            'first_group_rate': likert_bar_plot_data_sets[0][category]['RRR'],\n",
    "            'first_group_lower_CI': likert_bar_plot_data_sets[0][category]['RRR_low_CI'],\n",
    "            'first_group_upper_CI': likert_bar_plot_data_sets[0][category]['RRR_high_CI'],\n",
    "            'second_group_rate': likert_bar_plot_data_sets[1][category]['RRR'],\n",
    "            'second_group_lower_CI': likert_bar_plot_data_sets[1][category]['RRR_low_CI'],\n",
    "            'second_group_upper_CI': likert_bar_plot_data_sets[1][category]['RRR_high_CI'],\n",
    "        }\n",
    "        for category in categories\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sig_star(p: float | NAType | None) -> str:\n",
    "    if pd.isna(p):\n",
    "        return \"\"\n",
    "    if p < 0.001:\n",
    "        return \"***\"\n",
    "    if p < 0.01:\n",
    "        return \"**\"\n",
    "    if p < 0.05:\n",
    "        return \"*\"\n",
    "    return \"\"\n",
    "\n",
    "def clustered_bar_plot(\n",
    "    data: dict[str, dict[str, float]],\n",
    "    group_one_name: str = \"Group 1\",\n",
    "    group_two_name: str = \"Group 2\",\n",
    "    x_axis_title: str = \"\",\n",
    "    y_axis_title: str = \"\",\n",
    "    fig_title: str = \"\",\n",
    "    figsize: tuple[int, int] = (10, 6),\n",
    "    bar_width: float = 0.35,\n",
    "    colors: tuple[str, str] = (\"C0\", \"C1\"),\n",
    "    y_axis_percent: bool = True,\n",
    "    show_significance: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a clustered (side-by-side) bar chart with error bars and\n",
    "    return the underlying data in tabular form.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (matplotlib.figure.Figure, pandas.DataFrame)\n",
    "        Figure object and a DataFrame with columns:\n",
    "        ['cluster', 'group name', 'value', 'lower CI', 'upper CI']\n",
    "    \"\"\"\n",
    "    # Pull the ordered categories and their statistics\n",
    "    categories = list(data.keys())\n",
    "    n = len(categories)\n",
    "    idx = np.arange(n)\n",
    "\n",
    "    g1_vals = [data[c][\"first_group_rate\"] for c in categories]\n",
    "    g2_vals = [data[c][\"second_group_rate\"] for c in categories]\n",
    "\n",
    "    # Asymmetric errors: shape (2, N) → [lower, upper]\n",
    "    g1_err_lower = [data[c][\"first_group_rate\"] - data[c][\"first_group_lower_CI\"] for c in categories]\n",
    "    g1_err_upper = [data[c][\"first_group_upper_CI\"] - data[c][\"first_group_rate\"] for c in categories]\n",
    "    g2_err_lower = [data[c][\"second_group_rate\"] - data[c][\"second_group_lower_CI\"] for c in categories]\n",
    "    g2_err_upper = [data[c][\"second_group_upper_CI\"] - data[c][\"second_group_rate\"] for c in categories]\n",
    "\n",
    "    g1_yerr = np.array([g1_err_lower, g1_err_upper])\n",
    "    g2_yerr = np.array([g2_err_lower, g2_err_upper])\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.bar(\n",
    "        idx - bar_width / 2,\n",
    "        g1_vals,\n",
    "        width=bar_width,\n",
    "        label=group_one_name,\n",
    "        color=colors[0],\n",
    "        yerr=g1_yerr,\n",
    "        capsize=5,\n",
    "        align=\"center\",\n",
    "    )\n",
    "    ax.bar(\n",
    "        idx + bar_width / 2,\n",
    "        g2_vals,\n",
    "        width=bar_width,\n",
    "        label=group_two_name,\n",
    "        color=colors[1],\n",
    "        yerr=g2_yerr,\n",
    "        capsize=5,\n",
    "        align=\"center\",\n",
    "    )\n",
    "\n",
    "    # Significance stars\n",
    "    if show_significance:\n",
    "        stars: list[str] = []\n",
    "        for c in categories:\n",
    "            p = data[c].get(\"PVal\")\n",
    "            stars.append(_sig_star(p))\n",
    "\n",
    "        # Place the stars just above the taller bar (including its CI)\n",
    "        for i, star in enumerate(stars):\n",
    "            if not star:\n",
    "                continue\n",
    "            top_height = max(\n",
    "                g1_vals[i] + g1_err_upper[i],\n",
    "                g2_vals[i] + g2_err_upper[i],\n",
    "            )\n",
    "            ax.text(\n",
    "                x=i,\n",
    "                y=top_height,\n",
    "                s=star,\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=14,\n",
    "                fontweight=\"bold\",\n",
    "                color=\"black\",\n",
    "            )\n",
    "\n",
    "    # Aesthetics\n",
    "    ax.set_xticks(idx)\n",
    "    ax.set_xticklabels(categories, rotation=0)\n",
    "    if x_axis_title:\n",
    "        ax.set_xlabel(x_axis_title)\n",
    "    if y_axis_title:\n",
    "        ax.set_ylabel(y_axis_title)\n",
    "    if fig_title:\n",
    "        ax.set_title(fig_title)\n",
    "    ax.legend()\n",
    "    ax.margins(x=0.05)\n",
    "\n",
    "    if y_axis_percent:\n",
    "        ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "    # Ensure y-axis starts at 0\n",
    "    _, ymax = ax.get_ylim()\n",
    "    ax.set_ylim(bottom=0, top=max(ymax, 0))\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Build the summary table\n",
    "    table_rows = []\n",
    "    for i, cat in enumerate(categories):\n",
    "        # Group 1\n",
    "        table_rows.append(\n",
    "            {\n",
    "                \"cluster\": cat,\n",
    "                \"group name\": group_one_name,\n",
    "                \"value\": g1_vals[i],\n",
    "                \"lower CI\": data[cat][\"first_group_lower_CI\"],\n",
    "                \"upper CI\": data[cat][\"first_group_upper_CI\"],\n",
    "                \"p_value\": data[cat].get(\"PVal\") if show_significance else None,\n",
    "            }\n",
    "        )\n",
    "        # Group 2\n",
    "        table_rows.append(\n",
    "            {\n",
    "                \"cluster\": cat,\n",
    "                \"group name\": group_two_name,\n",
    "                \"value\": g2_vals[i],\n",
    "                \"lower CI\": data[cat][\"second_group_lower_CI\"],\n",
    "                \"upper CI\": data[cat][\"second_group_upper_CI\"],\n",
    "                \"p_value\": data[cat].get(\"PVal\") if show_significance else None,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    table_df = pd.DataFrame(table_rows)\n",
    "\n",
    "    return fig, table_df\n",
    "\n",
    "def capitalize_first_letter(s: str) -> str:\n",
    "    return s[0].upper() + s[1:]\n",
    "\n",
    "def likert_bar_plot_data_to_latex_vars(\n",
    "    data: dict[str, dict[str, float]],\n",
    "    prefix: str,\n",
    "    percent_variable: bool = True,\n",
    ") -> dict[str, str]:\n",
    "    # this function is this way for backwards compatibility with the old latex vars\n",
    "\n",
    "    latex_vars = {}\n",
    "    for metric_label, metric_data in data.items():\n",
    "        for key, value in metric_data.items():\n",
    "            latexified_key = ''.join([capitalize_first_letter(x) for x in key.split(\"_\")])\n",
    "\n",
    "            metric_label_camel = capitalize_first_letter(metric_label)\n",
    "\n",
    "            # Determine where (or whether) to insert the metric label\n",
    "            insertion_points = [\"FirstGroup\", \"SecondGroup\", \"RRR\", \"NNT\"]\n",
    "            updated_key = None\n",
    "            for substr in insertion_points:\n",
    "                if substr in latexified_key:\n",
    "                    # Insert the metric label immediately after the recognised substring\n",
    "                    updated_key = latexified_key.replace(substr, f\"{substr}{metric_label_camel}\")\n",
    "                    break\n",
    "\n",
    "            # If none of the substrings were found, prepend the metric label\n",
    "            if updated_key is None:\n",
    "                updated_key = f\"{metric_label_camel}{latexified_key}\"\n",
    "\n",
    "            # add on the prefix\n",
    "            updated_key = f\"{prefix}{updated_key}\"\n",
    "\n",
    "            if updated_key.endswith(\"N\") or updated_key.endswith(\"Y\"):\n",
    "                value = f\"{value:.0f}\"\n",
    "\n",
    "            elif \"NNT\" in updated_key:\n",
    "                value = f\"{value:.1f}\"\n",
    "\n",
    "            elif \"PVal\" in updated_key:\n",
    "                value = f\"{value:.3f}\"\n",
    "\n",
    "            else:\n",
    "                if percent_variable:\n",
    "                    value = f\"{value * 100:.1f}%\"\n",
    "                else:\n",
    "                    value = f\"{value:.1f}\"\n",
    "\n",
    "            latex_vars[updated_key] = value\n",
    "\n",
    "    return latex_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lbp_data_to_group_one_series(lbp_data):\n",
    "    return pd.Series({\n",
    "        key: f\"{value['first_group_rate'] * 100:.1f}% ({value['first_group_lower_CI'] * 100:.1f}%-{value['first_group_upper_CI'] * 100:.1f}%)\"\n",
    "        for key, value in lbp_data.items()\n",
    "    })\n",
    "\n",
    "def lbp_data_to_group_two_series(lbp_data):\n",
    "    return pd.Series({\n",
    "        key: f\"{value['second_group_rate'] * 100:.1f}% ({value['second_group_lower_CI'] * 100:.1f}%-{value['second_group_upper_CI'] * 100:.1f}%)\"\n",
    "        for key, value in lbp_data.items()\n",
    "    })\n",
    "\n",
    "def lbp_data_to_pval_series(lbp_data):\n",
    "    return pd.Series({\n",
    "        key: f\"{value['PVal']:.3f}\"\n",
    "        for key, value in lbp_data.items()\n",
    "    })\n",
    "\n",
    "def lbp_data_to_rrr_series(lbp_data):\n",
    "    return pd.Series({\n",
    "        key: f\"{value['RRR'] * 100:.1f}% ({value['RRR_low_CI'] * 100:.1f}%-{value['RRR_high_CI'] * 100:.1f}%)\"\n",
    "        for key, value in lbp_data.items()\n",
    "    })\n",
    "\n",
    "def lbp_data_to_NNT_series(lbp_data):\n",
    "    return pd.Series({\n",
    "        key: f\"{value['NNT']:.1f}\" if value['PVal'] < 0.05 else \"-\"\n",
    "        for key, value in lbp_data.items()\n",
    "    })\n",
    "\n",
    "PENDA_ANNUAL_PATIENT_VOLUME = 400000\n",
    "def lbp_data_to_absolute_benefit_series(lbp_data):\n",
    "    return pd.Series({\n",
    "        key: f\"{value['ARR'] * PENDA_ANNUAL_PATIENT_VOLUME:.0f}\" if value['PVal'] < 0.05 else \"-\"\n",
    "        for key, value in lbp_data.items()\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main study analysis and main period vs induction period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "induction_period_df = results_clinical_study_df[results_clinical_study_df.VisitDate < cut_point]\n",
    "main_study_df = results_clinical_study_df[results_clinical_study_df.VisitDate >= cut_point]\n",
    "\n",
    "induction_period_lbp_data = get_likert_bar_plot_data(induction_period_df)\n",
    "lv = likert_bar_plot_data_to_latex_vars(induction_period_lbp_data, \"induction\")\n",
    "latexvars.update(lv)\n",
    "\n",
    "main_study_lbp_data = get_likert_bar_plot_data(main_study_df)\n",
    "lv = likert_bar_plot_data_to_latex_vars(main_study_lbp_data, \"mainPeriodLikertErrorRates\")\n",
    "latexvars.update(lv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, fig_df = clustered_bar_plot(\n",
    "    main_study_lbp_data,\n",
    "    group_one_name = 'Non-AI',\n",
    "    group_two_name = 'AI',\n",
    "    fig_title = 'Error rates in history-taking, investigations, diagnosis & treatment questions\\nNon-AI vs AI in main study period',\n",
    "    y_axis_title = '% of visits with clinical errors for category',\n",
    ")\n",
    "save_fig('main_period_likert_error_rates', fig)\n",
    "save_csv('main_period_likert_error_rates', fig_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: f'{v[\"RRR\"]:0.3f} ({v[\"PVal\"]:0.3f}) - NNT {v[\"NNT\"]:0.1f} (for 400k yearly visits, {400000 * v[\"ARR\"]:.0f} errors reduced)' for k, v in main_study_lbp_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PENDA_ANNUAL_PATIENT_VOLUME = 400000\n",
    "lv = {\n",
    "    f\"totalErrorReduction{k}\": f\"{PENDA_ANNUAL_PATIENT_VOLUME * v['ARR']:.0f}\"\n",
    "    for k, v in main_study_lbp_data.items()\n",
    "}\n",
    "latexvars.update(lv)\n",
    "lv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, fig_df = clustered_bar_plot(\n",
    "    induction_period_lbp_data,\n",
    "    group_one_name = 'Non-AI',\n",
    "    group_two_name = 'AI',\n",
    "    fig_title = 'Error rates in history-taking, investigation, diagnosis & treatment questions\\nNon-AI vs AI in induction period',\n",
    "    y_axis_title = '% of visits with clinical errors for category',\n",
    ")\n",
    "save_fig('induction_likert_error_rates', fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = clustered_bar_plot(\n",
    "    get_rrr_bar_plot_data([induction_period_df, main_study_df]),\n",
    "    group_one_name = 'Non-AI',\n",
    "    group_two_name = 'AI',\n",
    "    y_axis_title = '% of visits with clinical errors for category',\n",
    "    show_significance = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cases with at least one red response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risky_cases_for_history = main_study_df[main_study_df.risky_cases_for_history == True]\n",
    "risky_cases_for_investigations = main_study_df[main_study_df.risky_cases_for_investigations == True]\n",
    "risky_cases_for_diagnosis = main_study_df[main_study_df.risky_cases_for_diagnosis == True]\n",
    "risky_cases_for_treatment = main_study_df[main_study_df.risky_cases_for_treatment == True]\n",
    "\n",
    "risky_case_lbp_data = get_likert_bar_plot_data(\n",
    "    [risky_cases_for_history, risky_cases_for_investigations, risky_cases_for_diagnosis, risky_cases_for_treatment]\n",
    ")\n",
    "lv = likert_bar_plot_data_to_latex_vars(risky_case_lbp_data, \"mainPeriodRedOnly\")\n",
    "latexvars.update(lv)\n",
    "lv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, fig_df = clustered_bar_plot(\n",
    "    risky_case_lbp_data,\n",
    "    group_one_name = 'Non-AI - risky cases for that category',\n",
    "    group_two_name = 'AI - risky cases for that category',\n",
    "    fig_title = 'Error rates in history-taking, investigation, diagnosis & treatment questions\\nVisits with risky cases for that category',\n",
    "    y_axis_title = '% of visits with clinical errors for category',\n",
    ")\n",
    "save_fig('main_period_red_only_likert_error_rates', fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: f'{v[\"RRR\"]:0.3f} ({v[\"PVal\"]:0.3f})' for k, v in risky_case_lbp_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrr_bar_plot_data_risky_vs_all = get_rrr_bar_plot_data(\n",
    "    likert_bar_plot_data_sets = [main_study_lbp_data, risky_case_lbp_data]\n",
    ")\n",
    "_ = clustered_bar_plot(\n",
    "    rrr_bar_plot_data_risky_vs_all,\n",
    "    group_one_name = 'All visits',\n",
    "    group_two_name = 'Only risky visits (i.e., one or more reds for the category in question)',\n",
    "    fig_title = 'Relative risk reduction in history-taking, investigation, diagnosis & treatment errors from AI consult',\n",
    "    y_axis_title = '% of visits with clinical errors for category',\n",
    "    show_significance = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_risky_nnt_df = pd.DataFrame({\n",
    "    'RRR: all visits': lbp_data_to_rrr_series(main_study_lbp_data),\n",
    "    'NNT': lbp_data_to_NNT_series(main_study_lbp_data),\n",
    "    'Yearly errors averted at Penda': lbp_data_to_absolute_benefit_series(main_study_lbp_data),\n",
    "})\n",
    "save_latex('error_rate_reduction_main', main_risky_nnt_df)\n",
    "main_risky_nnt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "induction_risky_main_df = pd.DataFrame({\n",
    "    'Main period, all visits': lbp_data_to_rrr_series(main_study_lbp_data),\n",
    "    'Induction period': lbp_data_to_rrr_series(induction_period_lbp_data),\n",
    "    'Main period, only visits with reds': lbp_data_to_rrr_series(risky_case_lbp_data),\n",
    "})\n",
    "save_latex(\"induction_vs_risky_vs_main\", induction_risky_main_df)\n",
    "induction_risky_main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final red/yellow vs final green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def any_final_red(x: types.AICalls | NAType) -> bool:\n",
    "    if is_na(x):\n",
    "        return False\n",
    "\n",
    "    final_reds = [x.for_rule(rule).final_red for rule in types.ClinicalDecisionRule]\n",
    "    return any(final_reds)\n",
    "\n",
    "\n",
    "def final_color_for_rule(x: types.AICalls | NAType, rule: types.ClinicalDecisionRule, color: Literal['red', 'red_yellow', 'yellow', 'green']) -> bool:\n",
    "    # returns True if the final color is the one we're looking for - red/yellow; red otherwise\n",
    "    if is_na(x):\n",
    "        return False\n",
    "\n",
    "    calls_for_rule = x.for_rule(rule)\n",
    "\n",
    "    if calls_for_rule._is_empty():\n",
    "        return False\n",
    "\n",
    "    if color == 'red':\n",
    "        is_final_color = calls_for_rule.final_is_color(types.Color.Red)\n",
    "    elif color == 'red_yellow':\n",
    "        is_final_color = calls_for_rule.final_is_color(types.Color.Red) or calls_for_rule.final_is_color(types.Color.Yellow)\n",
    "    elif color == 'yellow':\n",
    "        is_final_color = calls_for_rule.final_is_color(types.Color.Yellow)\n",
    "    elif color == 'green':\n",
    "        is_final_color = calls_for_rule.final_is_color(types.Color.Green)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid color: {color}\")\n",
    "\n",
    "    if is_final_color is None:\n",
    "        raise ValueError(f\"Final {color} is None for rule {rule}\") # should only happen if the object is empty\n",
    "\n",
    "    return is_final_color\n",
    "\n",
    "def final_color_results_for_category(x: types.AICalls | NAType, category: str, color: Literal['red', 'red_yellow', 'yellow', 'green']) -> bool | NAType:\n",
    "    if is_na(x):\n",
    "        return pd.NA\n",
    "\n",
    "    match category:\n",
    "        case 'history':\n",
    "            history_final_color = final_color_for_rule(x, types.ClinicalDecisionRule.VitalsChiefComplaintEvaluation, color)\n",
    "            clinical_notes_final_color = final_color_for_rule(x, types.ClinicalDecisionRule.ClinicalNotes, color)\n",
    "            return history_final_color | clinical_notes_final_color\n",
    "        case 'investigations':\n",
    "            return final_color_for_rule(x, types.ClinicalDecisionRule.InvestigationRecommendations, color)\n",
    "        case 'diagnosis':\n",
    "            return final_color_for_rule(x, types.ClinicalDecisionRule.DiagnosisEvaluation, color)\n",
    "        case 'treatment':\n",
    "            return final_color_for_rule(x, types.ClinicalDecisionRule.TreatmentRecommendation, color)\n",
    "        case _:\n",
    "            raise ValueError(f\"Invalid category: {category}\")\n",
    "\n",
    "def color_x_vs_color_y(x: types.AICalls | NAType, category: str, true_color: Literal['red', 'red_yellow', 'yellow', 'green'], false_color: Literal['red', 'red_yellow', 'yellow', 'green']) -> bool | NAType:\n",
    "    if is_na(x):\n",
    "        return pd.NA\n",
    "\n",
    "    final_color_for_rule_is_true_color = final_color_results_for_category(x, category, true_color)\n",
    "    final_color_for_rule_is_false_color = final_color_results_for_category(x, category, false_color)\n",
    "\n",
    "    if final_color_for_rule_is_true_color is True:\n",
    "        return True\n",
    "    elif final_color_for_rule_is_false_color is True:\n",
    "        return False\n",
    "    else:\n",
    "        return pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_clinical_study_df['final_red_vs_yellow_for_history'] = main_study_df.AICalls.map(lambda x: color_x_vs_color_y(x, 'history', 'red', 'yellow'))\n",
    "results_clinical_study_df['final_red_vs_yellow_for_investigations'] = main_study_df.AICalls.map(lambda x: color_x_vs_color_y(x, 'investigations', 'red', 'yellow'))\n",
    "results_clinical_study_df['final_red_vs_yellow_for_diagnosis'] = main_study_df.AICalls.map(lambda x: color_x_vs_color_y(x, 'diagnosis', 'red', 'yellow'))\n",
    "results_clinical_study_df['final_red_vs_yellow_for_treatment'] = main_study_df.AICalls.map(lambda x: color_x_vs_color_y(x, 'treatment', 'red', 'yellow'))\n",
    "\n",
    "final_red_yellow_lbp_data = get_likert_bar_plot_data(\n",
    "    results_clinical_study_df,\n",
    "    allocation_col = ['final_red_vs_yellow_for_history', 'final_red_vs_yellow_for_investigations', 'final_red_vs_yellow_for_diagnosis', 'final_red_vs_yellow_for_treatment'],\n",
    "    allocation_group_one_values = [True],\n",
    "    allocation_group_two_values = [False]\n",
    ")\n",
    "lv = likert_bar_plot_data_to_latex_vars(final_red_yellow_lbp_data, \"redVYellow\")\n",
    "latexvars.update(lv)\n",
    "lv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, fig_df = clustered_bar_plot(\n",
    "    final_red_yellow_lbp_data,\n",
    "    group_one_name = 'Final red for the category',\n",
    "    group_two_name = 'Final yellow for the category',\n",
    "    fig_title = 'Error rates in history-taking, investigation, diagnosis & treatment questions\\nVisits with final red vs visits with final yellow',\n",
    "    y_axis_title = '% of visits with clinical errors for category',\n",
    ")\n",
    "save_fig('red_v_yellow_likert_error_rates', fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_clinical_study_df['final_yellow_vs_green_for_history'] = main_study_df.AICalls.map(lambda x: color_x_vs_color_y(x, 'history', 'yellow', 'green'))\n",
    "results_clinical_study_df['final_yellow_vs_green_for_investigations'] = main_study_df.AICalls.map(lambda x: color_x_vs_color_y(x, 'investigations', 'yellow', 'green'))\n",
    "results_clinical_study_df['final_yellow_vs_green_for_diagnosis'] = main_study_df.AICalls.map(lambda x: color_x_vs_color_y(x, 'diagnosis', 'yellow', 'green'))\n",
    "results_clinical_study_df['final_yellow_vs_green_for_treatment'] = main_study_df.AICalls.map(lambda x: color_x_vs_color_y(x, 'treatment', 'yellow', 'green'))\n",
    "\n",
    "final_yellow_green_lbp_data = get_likert_bar_plot_data(\n",
    "    results_clinical_study_df,\n",
    "    allocation_col = ['final_yellow_vs_green_for_history', 'final_yellow_vs_green_for_investigations', 'final_yellow_vs_green_for_diagnosis', 'final_yellow_vs_green_for_treatment'],\n",
    "    allocation_group_one_values = [True],\n",
    "    allocation_group_two_values = [False]\n",
    ")\n",
    "lv = likert_bar_plot_data_to_latex_vars(final_yellow_green_lbp_data, \"yellowVGreen\")\n",
    "latexvars.update(lv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, fig_df = clustered_bar_plot(\n",
    "    final_yellow_green_lbp_data,\n",
    "    group_one_name = 'Final yellow for the category',\n",
    "    group_two_name = 'Final green for the category',\n",
    "    fig_title = 'Error rates in history-taking, investigation, diagnosis & treatment questions\\nVisits with final yellow vs visits with final green',\n",
    "    y_axis_title = '% of visits with clinical errors for category',\n",
    ")\n",
    "save_fig('yellow_v_green_likert_error_rates', fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ryg_corr_df = pd.DataFrame({\n",
    "    'Left in red': lbp_data_to_group_one_series(final_red_yellow_lbp_data),\n",
    "    'Left inyellow': lbp_data_to_group_two_series(final_red_yellow_lbp_data),\n",
    "    'Left in green': lbp_data_to_group_two_series(final_yellow_green_lbp_data),\n",
    "    'p: R vs Y': lbp_data_to_pval_series(final_red_yellow_lbp_data),\n",
    "    'p: Y vs G': lbp_data_to_pval_series(final_yellow_green_lbp_data),\n",
    "})\n",
    "save_latex(\"ryg_corr\", ryg_corr_df)\n",
    "ryg_corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acuity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_acuity_df = results_clinical_study_df[(results_clinical_study_df.VisitDate >= cut_point) & (results_clinical_study_df.acuity == 'Low')]\n",
    "medium_acuity_df = results_clinical_study_df[(results_clinical_study_df.VisitDate >= cut_point) & (results_clinical_study_df.acuity == 'Medium')]\n",
    "high_acuity_df = results_clinical_study_df[(results_clinical_study_df.VisitDate >= cut_point) & (results_clinical_study_df.acuity == 'High')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = clustered_bar_plot(\n",
    "    get_rrr_bar_plot_data([low_acuity_df, medium_acuity_df]),\n",
    "    group_one_name = 'Low acuity',\n",
    "    group_two_name = 'Medium acuity',\n",
    "    y_axis_title = '% of visits with clinical errors for category',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = clustered_bar_plot(\n",
    "    get_rrr_bar_plot_data([low_acuity_df, high_acuity_df]),\n",
    "    group_one_name = 'Low acuity',\n",
    "    group_two_name = 'High acuity',\n",
    "    y_axis_title = '% of visits with clinical errors for category',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_acuity_lbp_data = get_likert_bar_plot_data(low_acuity_df)\n",
    "medium_acuity_lbp_data = get_likert_bar_plot_data(medium_acuity_df)\n",
    "high_acuity_lbp_data = get_likert_bar_plot_data(high_acuity_df)\n",
    "\n",
    "low_acuity_error_rates = lbp_data_to_rrr_series(low_acuity_lbp_data)\n",
    "medium_acuity_error_rates = lbp_data_to_rrr_series(medium_acuity_lbp_data)\n",
    "high_acuity_error_rates = lbp_data_to_rrr_series(high_acuity_lbp_data)\n",
    "\n",
    "acuity_rrr_df = pd.DataFrame({\n",
    "    \"Low-acuity cases\": low_acuity_error_rates,\n",
    "    \"Medium-acuity cases\": medium_acuity_error_rates,\n",
    "    \"High-acuity cases\": high_acuity_error_rates,\n",
    "})\n",
    "save_latex(\"acuity_rrr\", acuity_rrr_df)\n",
    "acuity_rrr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-rater agreement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likert_cols = ['history_likert', 'investigations_likert', 'diagnosis_likert', 'treatment_likert']\n",
    "\n",
    "agreement_statistics = {}\n",
    "\n",
    "for likert_col in likert_cols:\n",
    "    error_col = f'{likert_col}_is_low'\n",
    "\n",
    "    relevant_df = results_clinical_study_df.dropna(subset=[likert_col])\n",
    "    grouped = relevant_df[[\"VisitCode\", likert_col, error_col]].groupby(\"VisitCode\")\n",
    "    fleiss_kappa_results = []\n",
    "\n",
    "    for _, group in grouped:\n",
    "        if len(group) == 1:\n",
    "            continue\n",
    "        elif len(group) > 2:\n",
    "            raise ValueError(f\"Expected exactly one or two ratings per visit, got {len(group)}\")\n",
    "\n",
    "        is_error_1, is_error_2 = group[error_col]\n",
    "\n",
    "        fleiss_kappa_results.append({\n",
    "            0: (is_error_1 == 0) + (is_error_2 == 0),\n",
    "            1: (is_error_1 == 1) + (is_error_2 == 1),\n",
    "        })\n",
    "\n",
    "    fleiss_kappa_df = pd.DataFrame(fleiss_kappa_results)\n",
    "    fk = fleiss_kappa(fleiss_kappa_df.values, method=\"fleiss\")\n",
    "\n",
    "    agreement_statistics[likert_col] = {\n",
    "        'fleiss_kappa': fk,\n",
    "    }\n",
    "\n",
    "agreement_statistics_df = pd.DataFrame(agreement_statistics)\n",
    "latexvars['fleissKappaHistory'] = f\"{agreement_statistics_df['history_likert']['fleiss_kappa']:.3f}\"\n",
    "latexvars['fleissKappaInvestigations'] = f\"{agreement_statistics_df['investigations_likert']['fleiss_kappa']:.3f}\"\n",
    "latexvars['fleissKappaDiagnosis'] = f\"{agreement_statistics_df['diagnosis_likert']['fleiss_kappa']:.3f}\"\n",
    "latexvars['fleissKappaTreatment'] = f\"{agreement_statistics_df['treatment_likert']['fleiss_kappa']:.3f}\"\n",
    "agreement_statistics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_confusion_matrix(\n",
    "    df: pd.DataFrame,\n",
    "    likert_col: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a standard confusion matrix (row: rater 1, col: rater 2) as percentages of total pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    matrix = pd.DataFrame(0, index=range(1, 6), columns=range(1, 6), dtype=int)\n",
    "    grouped = df[[\"VisitCode\", likert_col]].dropna().groupby(\"VisitCode\")\n",
    "    total_pairs = 0\n",
    "\n",
    "    for _, group in grouped:\n",
    "        if len(group) == 1:\n",
    "            continue\n",
    "        elif len(group) > 2:\n",
    "            raise ValueError(f\"Expected exactly one or two ratings per visit, got {len(group)}\")\n",
    "\n",
    "        likert_1, likert_2 = group[likert_col].tolist()\n",
    "        if is_na(likert_1) or is_na(likert_2):\n",
    "            continue\n",
    "\n",
    "        # Add both (rater1, rater2) and (rater2, rater1) to make symmetric\n",
    "        matrix.at[likert_1, likert_2] += 1\n",
    "        matrix.at[likert_2, likert_1] += 1\n",
    "        total_pairs += 2  # Each visit adds two pairs\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def agreement_summary_table(\n",
    "    df: pd.DataFrame,\n",
    "    likert_cols: list[str] | None = None,\n",
    "    diffs: tuple[int, ...] = (0, 1, 2),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a summary table of inter-rater agreement (proportion ± 95 % CI).\n",
    "    \"\"\"\n",
    "    if likert_cols is None:\n",
    "        likert_cols = [\n",
    "            \"history_likert\",\n",
    "            \"investigations_likert\",\n",
    "            \"diagnosis_likert\",\n",
    "            \"treatment_likert\",\n",
    "        ]\n",
    "\n",
    "    summary_rows: list[dict[str, float | str]] = []\n",
    "\n",
    "    for question in likert_cols:\n",
    "        confusion_matrix = standard_confusion_matrix(df, question)\n",
    "        total_pairs = confusion_matrix.values.sum()\n",
    "\n",
    "        row_vals = confusion_matrix.index.to_numpy()\n",
    "        col_vals = confusion_matrix.columns.to_numpy()\n",
    "        abs_diff_grid = np.abs(row_vals[:, None] - col_vals[None, :])\n",
    "        confusion_matrix_values = confusion_matrix.to_numpy()\n",
    "\n",
    "        for diff_threshold in diffs:\n",
    "            matched_pairs = confusion_matrix_values[abs_diff_grid <= diff_threshold].sum()\n",
    "\n",
    "            test_result = binomtest(matched_pairs, total_pairs)\n",
    "            prop = test_result.statistic\n",
    "            ci_low, ci_high = test_result.proportion_ci(method=\"wilson\")\n",
    "\n",
    "            summary_rows.append(\n",
    "                {\n",
    "                    \"likert\": question,\n",
    "                    \"diff\": f\"≤{diff_threshold}\",\n",
    "                    \"prop\": prop,\n",
    "                    \"low\": ci_low,\n",
    "                    \"high\": ci_high,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Convert the tidy records list into the desired wide table\n",
    "    summary_table = (\n",
    "        pd.DataFrame(summary_rows)\n",
    "        .set_index([\"likert\", \"diff\"])\n",
    "        .unstack(\"diff\")            # columns -> (metric, diff)\n",
    "        .swaplevel(0, 1, axis=1)    # order -> (diff, metric)\n",
    "        .sort_index(axis=1, level=0)\n",
    "        .reindex(likert_cols)       # preserve original order of questions\n",
    "    )\n",
    "\n",
    "    summary_table.columns.names = [\"diff\", \"metric\"]\n",
    "    return summary_table\n",
    "\n",
    "def plot_standard_confusion_matrix(\n",
    "    conf_mat: pd.DataFrame,\n",
    "    title: str = \"Standard confusion matrix\",\n",
    "    cmap: str = \"Oranges\",\n",
    "    ax: plt.Axes | None = None,\n",
    "):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "    data = conf_mat.fillna(0).astype(float).values\n",
    "    data = data / data.sum() * 100\n",
    "\n",
    "    annot = np.empty_like(data, dtype=object)\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            annot[i, j] = f\"{data[i, j]:.1f}%\"\n",
    "\n",
    "    sns.heatmap(\n",
    "        data,\n",
    "        annot=annot,\n",
    "        fmt=\"\",\n",
    "        cmap=cmap,\n",
    "        cbar=False,\n",
    "        ax=ax,\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"white\",\n",
    "        xticklabels=list(conf_mat.columns),\n",
    "        yticklabels=list(conf_mat.index),\n",
    "    )\n",
    "    ax.set_xlabel(\"Rater 2 rating\")\n",
    "    ax.set_ylabel(\"Rater 1 rating\")\n",
    "    ax.set_title(title)\n",
    "    ax.invert_yaxis()  # So higher ratings are at the top\n",
    "\n",
    "    # Rotate y tick labels to be horizontal\n",
    "    for label in ax.get_yticklabels():\n",
    "        label.set_rotation(0)\n",
    "    plt.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "# display summary statistics and save as latex variables\n",
    "inter_rater_agreement_table = agreement_summary_table(results_clinical_study_df)\n",
    "print(\"Inter-rater agreement (proportion and 95 % CI):\")\n",
    "display(inter_rater_agreement_table)\n",
    "\n",
    "for likert in inter_rater_agreement_table.index:\n",
    "    likert_latexable = likert.replace('_likert', 'Likert')\n",
    "    for diff in inter_rater_agreement_table.columns.get_level_values('diff'):\n",
    "        diff_latexable = {\n",
    "            \"≤0\": \"Exact\",\n",
    "            \"≤1\": \"OnePoint\",\n",
    "            \"≤2\": \"TwoPoint\",\n",
    "        }[diff]\n",
    "        for metric in inter_rater_agreement_table.columns.get_level_values('metric'):\n",
    "            varstring = f'{likert_latexable}{diff_latexable}{metric.capitalize()}'\n",
    "            val = f'{inter_rater_agreement_table.loc[likert, (diff, metric)] * 100:.1f}%'\n",
    "            print(f'{varstring}: {val}')\n",
    "            latexvars[varstring] = val\n",
    "\n",
    "# Four-panel (2x2) plot of standard_confusion_matrix for each Likert type\n",
    "likert_titles = [\n",
    "    (\"history_likert\", \"History Likert – trainer agreement\"),\n",
    "    (\"investigations_likert\", \"Investigations Likert – trainer agreement\"),\n",
    "    (\"diagnosis_likert\", \"Diagnosis Likert – trainer agreement\"),\n",
    "    (\"treatment_likert\", \"Treatment Likert – trainer agreement\"),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "axes = axes.flatten()\n",
    "for ax, (likert, title) in zip(axes, likert_titles):\n",
    "    conf_mat_std = standard_confusion_matrix(results_clinical_study_df, likert)\n",
    "    plot_standard_confusion_matrix(\n",
    "        conf_mat_std,\n",
    "        title=title,\n",
    "        ax=ax,\n",
    "    )\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "save_fig('human_rater_study_agreement', fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_risk_ratios(result):\n",
    "    \"\"\"\n",
    "    Convert a statsmodels fit object into a clean\n",
    "    DataFrame of odds ratios with 95 % CIs and p-values.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        pd.DataFrame({\n",
    "            \"RR\":       np.exp(result.params),\n",
    "            \"CI_low\":   np.exp(result.conf_int()[0]),\n",
    "            \"CI_high\":  np.exp(result.conf_int()[1]),\n",
    "            \"p\":        result.pvalues\n",
    "        })\n",
    "        .round(3)\n",
    "    )\n",
    "\n",
    "def clean_index_name(name):\n",
    "    if name == \"AgeYears\":\n",
    "        return \"Age (years)\"\n",
    "    if name == \"Intercept\":\n",
    "        return \"Intercept\"\n",
    "\n",
    "    m = re.match(r\"C\\(([^,]+),\\s*Treatment\\(reference=['\\\"]([^'\\\"]+)['\\\"]\\)\\)\\[T\\.([^\\]]+)\\]\", name)\n",
    "    if m:\n",
    "        var, ref, level = m.groups()\n",
    "\n",
    "        match var:\n",
    "            case \"ClinicianGroup\":\n",
    "                var = \"Group\"\n",
    "            case \"VisitType\":\n",
    "                var = \"Visit type\"\n",
    "            case \"LocationName\":\n",
    "                var = \"Clinic\"\n",
    "            case _:\n",
    "                pass\n",
    "\n",
    "        return f\"{var}: {level} vs {ref}\"\n",
    "\n",
    "    m = re.match(r\"C\\(([^,]+),\\s*Sum\\)\\[S\\.([^\\]]+)\\]\", name)\n",
    "    if m:\n",
    "        var, level = m.groups()\n",
    "\n",
    "        assert var == \"LocationName\"\n",
    "        return f\"Clinic: {level} vs mean clinic\"\n",
    "\n",
    "    raise ValueError(f\"Column name not recognized: {name}\")\n",
    "\n",
    "\n",
    "def clean_regression_result(result):\n",
    "    tidied_df = tidy_risk_ratios(result)\n",
    "    tidied_df.index = tidied_df.index.map(clean_index_name)\n",
    "    tidied_df.columns = ['Relative risk', '95% CI lower', '95% CI upper', 'p']\n",
    "\n",
    "    main_effect = tidied_df.loc['Group: AI vs Non-AI', :]\n",
    "\n",
    "    return main_effect, tidied_df\n",
    "\n",
    "def main_effect_to_rrr(main_effect, latexname):\n",
    "    rrr_point = 1 - main_effect['Relative risk']\n",
    "    rrr_ci_lower = 1 - main_effect['95% CI upper']\n",
    "    rrr_ci_upper = 1 - main_effect['95% CI lower']\n",
    "    return {\n",
    "        f'{latexname}RRRPoint': f'{rrr_point * 100:.1f}%',\n",
    "        f'{latexname}RRRLower': f'{rrr_ci_lower * 100:.1f}%',\n",
    "        f'{latexname}RRRUpper': f'{rrr_ci_upper * 100:.1f}%',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain in [\"history\", \"investigations\", \"diagnosis\", \"treatment\"]:\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"FOR DOMAIN: {domain.upper()}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # DATA PREP\n",
    "    results_clinical_study_main_period_df = results_clinical_study_df[results_clinical_study_df.VisitDate >= cut_point]\n",
    "    results_clinical_study_main_period_df = results_clinical_study_main_period_df.dropna(subset=[f'{domain}_likert_is_low'])\n",
    "    results_clinical_study_main_period_df[f'{domain}_likert_is_low'] = results_clinical_study_main_period_df[f'{domain}_likert_is_low'].astype(int)\n",
    "\n",
    "    # BASELINE MODEL\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{domain.upper()}: BASELINE MODEL\")\n",
    "    print(\"-\" * 100)\n",
    "    result = smf.glm(\n",
    "        f\"{domain}_likert_is_low ~ C(ClinicianGroup, Treatment(reference = 'Non-AI'))\",\n",
    "        data=results_clinical_study_main_period_df,\n",
    "        family=sm.families.Binomial(link=sm.families.links.Log()),\n",
    "        freq_weights=results_clinical_study_main_period_df[\"row_weight\"]\n",
    "    ).fit()\n",
    "\n",
    "    display(result.summary().tables[0])\n",
    "    display(tidy_risk_ratios(result))\n",
    "\n",
    "    # BINOMIAL GEE MODEL\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{domain.upper()}: BINOMIAL GEE MODEL\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    formula = \"\"\"{domain}_likert_is_low ~\n",
    "    C(ClinicianGroup, Treatment(reference = 'Non-AI')) +\n",
    "    AgeYears +\n",
    "    C(Gender, Treatment(reference = 'Male')) +\n",
    "    C(VisitType, Treatment(reference = 'Cash')) +\n",
    "    C(LocationName, Sum)\"\"\"\n",
    "\n",
    "    result = smf.gee(\n",
    "        formula.format(domain=domain),\n",
    "        groups = results_clinical_study_main_period_df.user_id,\n",
    "        data=results_clinical_study_main_period_df,\n",
    "        family=sm.families.Binomial(link=sm.families.links.Log()),\n",
    "        weights=results_clinical_study_main_period_df[\"row_weight\"],\n",
    "        cov_struct = sm.genmod.cov_struct.Exchangeable(),\n",
    "    ).fit()\n",
    "\n",
    "    display(result.summary().tables[0])\n",
    "    display(tidy_risk_ratios(result))\n",
    "\n",
    "    main_effect, tidied_df = clean_regression_result(result)\n",
    "\n",
    "    save_latex(f'main_study_gee_{domain}', tidied_df)\n",
    "    lv = main_effect_to_rrr(main_effect, f'mainStudyGEE{domain}')\n",
    "    latexvars.update(lv)\n",
    "\n",
    "    # LOG-POISSON MODEL WITH ZOU-DONNER CLUSTERING FOR COVARIANCE\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{domain.upper()}: LOG-POISSON MODEL WITH ZOU-DONNER CLUSTERING FOR COVARIANCE\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    model = smf.glm(\n",
    "        formula.format(domain=domain),\n",
    "        data=results_clinical_study_main_period_df,\n",
    "        family=sm.families.Poisson(link=sm.families.links.Log()),\n",
    "        freq_weights=results_clinical_study_main_period_df[\"row_weight\"]\n",
    "    )\n",
    "\n",
    "    result = model.fit(\n",
    "        cov_type='cluster',\n",
    "        cov_kwds={'groups': results_clinical_study_main_period_df['user_id']}\n",
    "    )\n",
    "\n",
    "    display(result.summary().tables[0])\n",
    "    display(tidy_risk_ratios(result))\n",
    "\n",
    "    main_effect, tidied_df = clean_regression_result(result)\n",
    "    save_latex(f'main_study_poisson_{domain}', tidied_df)\n",
    "    lv = main_effect_to_rrr(main_effect, f'mainStudyPoisson{domain}')\n",
    "    latexvars.update(lv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple-choice question analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mcq_option(s: str) -> str:\n",
    "    if '(' in s:\n",
    "        return s.split('(')[0].strip()\n",
    "    return s\n",
    "\n",
    "def process_mcq_options(s: set[str]) -> set[str]:\n",
    "    return {process_mcq_option(opt) for opt in s}\n",
    "\n",
    "mcq_cols = ['form_a_mcq', 'form_b_mcq', 'form_c_mcq', 'form_d_mcq']\n",
    "mcq_col_short_name_map = {\n",
    "    'form_a_mcq': 'History',\n",
    "    'form_b_mcq': 'Investigations',\n",
    "    'form_c_mcq': 'Diagnosis',\n",
    "    'form_d_mcq': 'Treatment'\n",
    "}\n",
    "\n",
    "for mcq_col in mcq_cols:\n",
    "    results_clinical_study_df[mcq_col] = results_clinical_study_df[mcq_col].map(process_mcq_options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcq_options_by_question = defaultdict(set)\n",
    "\n",
    "for col in mcq_cols:\n",
    "    for row in results_clinical_study_df[col]:\n",
    "        for option in row:\n",
    "            if option == 'None of the above':\n",
    "                continue\n",
    "\n",
    "            mcq_options_by_question[col].add(option)\n",
    "\n",
    "all_options = set.union(*mcq_options_by_question.values())\n",
    "assert len(all_options) == sum(len(opts) for opts in mcq_options_by_question.values())\n",
    "\n",
    "for col in mcq_cols:\n",
    "    options = sorted(mcq_options_by_question[col])\n",
    "    for opt in options:\n",
    "        results_clinical_study_df[f\"{opt}\"] = results_clinical_study_df[col].apply(lambda opts: opt in opts if not is_na(opts) else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_study_df = results_clinical_study_df[results_clinical_study_df['VisitDate'] >= cut_point]\n",
    "\n",
    "all_bar_plot_data = {}\n",
    "for mcq_col in mcq_cols:\n",
    "    mcq_col_short_name = mcq_col_short_name_map[mcq_col]\n",
    "\n",
    "    mcq_bar_plot_data = get_likert_bar_plot_data(\n",
    "        main_study_df,\n",
    "        allocation_col = 'ClinicianGroup',\n",
    "        allocation_group_one_values = ['Non-AI'],\n",
    "        allocation_group_two_values = ['AI'],\n",
    "        outcome_cols = [f\"{opt}\" for opt in mcq_options_by_question[mcq_col]],\n",
    "        outcome_true_values = [True],\n",
    "        outcome_false_values = [False],\n",
    "        unique_column = 'VisitCode'\n",
    "    )\n",
    "\n",
    "    for k, v in mcq_bar_plot_data.items():\n",
    "        all_bar_plot_data[f\"{mcq_col_short_name}: {k}\"] = v\n",
    "\n",
    "    wrap_length = 150 // len(mcq_bar_plot_data)\n",
    "    mcq_bar_plot_data = {\n",
    "        textwrap.fill(k, width = wrap_length): v\n",
    "        for k, v in mcq_bar_plot_data.items()\n",
    "    }\n",
    "\n",
    "    clustered_bar_plot(\n",
    "        mcq_bar_plot_data,\n",
    "        group_one_name = 'Non-AI',\n",
    "        group_two_name = 'AI',\n",
    "        fig_title = f'Specific error category rates in AI vs Non-AI group: {mcq_col_short_name}',\n",
    "        y_axis_title = \"Proportion of visits with error\",\n",
    "        figsize = (15, 6),\n",
    "        bar_width = 0.35,\n",
    "        colors = (\"C0\", \"C1\"),\n",
    "        y_axis_percent = True,\n",
    "        show_significance = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in all_bar_plot_data.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_mcq_cols_mapping = {\n",
    "    \"History: Chief complaint is absent\": None,\n",
    "    \"History: Documentation of relevant systems on physical exam are absent\": None,\n",
    "    \"History: Pertinent vital signs are absent\": None,\n",
    "    \"History: Key details in the history are missing\": \"History: Key details are missing\",\n",
    "    \"Investigations: Key investigations are missing\": \"Investigations: Key investigations are missing\",\n",
    "    \"Investigations: Unjustified investigations are ordered\": None,\n",
    "    \"Diagnosis: Primary diagnosis is missing\": None,\n",
    "    \"Diagnosis: Primary diagnosis is likely incorrect\": \"Diagnosis: Primary diagnosis is likely incorrect\",\n",
    "    \"Diagnosis: Primary diagnosis is too specific to be supported based on current documentation or investigations\": None,\n",
    "    \"Diagnosis: Additional diagnosis is likely incorrect\": None,\n",
    "    \"Diagnosis: Primary diagnosis is too broad when a more specific diagnosis is supported by the clinical notes\": None,\n",
    "    \"Diagnosis: Clinically relevant additional diagnosis is missing\": None,\n",
    "    \"Treatment: Medications are present but inappropriate\": \"Treatment: Inappropriate medications used\",\n",
    "    \"Treatment: Likely inappropriate class of antibiotics used\": None,\n",
    "    \"Treatment: Missing patient advice, education or follow up plan\": \"Treatment: Missing patient education or follow up plan\",\n",
    "    \"Treatment: Incorrect patient advice, education or follow up plan\": None,\n",
    "    \"Treatment: Needed procedures are missing\": None,\n",
    "    \"Treatment: Likely inappropriate use of antibiotics overall\": None,\n",
    "    \"Treatment: Medications are appropriate but incorrect dosages listed\": None,\n",
    "    \"Treatment: Escalations of care are present but inappropriate\": None,\n",
    "    \"Treatment: Procedures are present but inappropriate\": None,\n",
    "    \"Treatment: Needed escalations of care are missing\": None,\n",
    "    \"Treatment: Referrals are missing\": None,\n",
    "    \"Treatment: Referrals are present but inappropriate\": None,\n",
    "    \"Treatment: Medications are missing\": None,\n",
    "}\n",
    "summary_mcq_bar_plot_data = {\n",
    "    short_name: all_bar_plot_data[key_col]\n",
    "    for key_col, short_name in key_mcq_cols_mapping.items()\n",
    "    if short_name is not None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcq_df = pd.DataFrame({\n",
    "    'Non-AI': lbp_data_to_group_one_series(all_bar_plot_data),\n",
    "    'AI': lbp_data_to_group_two_series(all_bar_plot_data),\n",
    "    'RRR': lbp_data_to_rrr_series(all_bar_plot_data),\n",
    "    'p': lbp_data_to_pval_series(all_bar_plot_data),\n",
    "    'NNT': lbp_data_to_NNT_series(all_bar_plot_data),\n",
    "    'N errors reduced at Penda': lbp_data_to_absolute_benefit_series(all_bar_plot_data)\n",
    "})\n",
    "\n",
    "# Rename certain keys in the index for clarity\n",
    "mcq_df = mcq_df.rename(index={\n",
    "    \"Diagnosis: Primary diagnosis is too broad when a more specific diagnosis is supported by the clinical notes\": \"Diagnosis: Primary diagnosis broad when more specific is supported\",\n",
    "    \"Diagnosis: Primary diagnosis is too specific to be supported based on current documentation or investigations\": \"Diagnosis: Primary diagnosis too specific to be supported\",\n",
    "})\n",
    "\n",
    "save_latex('mcq', mcq_df)\n",
    "mcq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_length = 150 // len(summary_mcq_bar_plot_data)\n",
    "summary_mcq_bar_plot_data = {\n",
    "    textwrap.fill(k, width = wrap_length): v\n",
    "    for k, v in summary_mcq_bar_plot_data.items()\n",
    "}\n",
    "\n",
    "fig, fig_df = clustered_bar_plot(\n",
    "    summary_mcq_bar_plot_data,\n",
    "    group_one_name = 'Non-AI',\n",
    "    group_two_name = 'AI',\n",
    "    fig_title = f'Specific error category rates in AI vs Non-AI group\\nOnly select categories are shown',\n",
    "    y_axis_title = \"Proportion of visits with error\",\n",
    "    figsize = (15, 6),\n",
    "    bar_width = 0.35,\n",
    "    colors = (\"C0\", \"C1\"),\n",
    "    y_axis_percent = True,\n",
    "    show_significance = True\n",
    ")\n",
    "save_fig('select_mcq', fig)\n",
    "save_csv('mcq', fig_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_is_opt(x, opt):\n",
    "    if pd.isna(x):\n",
    "        return pd.NA\n",
    "    if opt in x:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "a = results_clinical_study_df.form_d_mcq.apply(lambda x: 'Medications are present but inappropriate' in x)\n",
    "a.name = \"Were inappropriate medications given?\"\n",
    "b = results_clinical_study_df.Q2.map(lambda x: not q2_is_opt(x, \"all_at_penda\") if pd.notna(x) else pd.NA)\n",
    "b.name = \"Did patients seek care outside Penda?\"\n",
    "\n",
    "crosstab = pd.crosstab(a, b)\n",
    "crosstab_normalized = pd.crosstab(a, b, normalize='index')\n",
    "display(crosstab)\n",
    "display(crosstab_normalized)\n",
    "fisher_exact(crosstab.to_numpy())\n",
    "\n",
    "seek_care_given_inappropriate = crosstab.at[True, True]\n",
    "inappropriate_meds = crosstab.at[True, True] + crosstab.at[True, False]\n",
    "bt_inappropriate = binomtest(seek_care_given_inappropriate, inappropriate_meds)\n",
    "p_seek_care_given_inappropriate = bt_inappropriate.statistic\n",
    "ci_seek_care_given_inappropriate = bt_inappropriate.proportion_ci(confidence_level=0.95, method='wilson')\n",
    "latexvars['seekCareGivenInappropriateMedsRate'] = f\"{p_seek_care_given_inappropriate:.1%}\"\n",
    "latexvars['seekCareGivenInappropriateMedsRateLowerCI'] = f\"{ci_seek_care_given_inappropriate.low:.1%}\"\n",
    "latexvars['seekCareGivenInappropriateMedsRateUpperCI'] = f\"{ci_seek_care_given_inappropriate.high:.1%}\"\n",
    "\n",
    "seek_care_given_not_inappropriate = crosstab.at[False, True]\n",
    "not_inappropriate_meds = crosstab.at[False, True] + crosstab.at[False, False]\n",
    "bt_not_inappropriate = binomtest(seek_care_given_not_inappropriate, not_inappropriate_meds)\n",
    "p_seek_care_given_not_inappropriate = bt_not_inappropriate.statistic\n",
    "ci_seek_care_given_not_inappropriate = bt_not_inappropriate.proportion_ci(confidence_level=0.95, method='wilson')\n",
    "latexvars['seekCareGivenNotInappropriateMedsRate'] = f\"{p_seek_care_given_not_inappropriate:.1%}\"\n",
    "latexvars['seekCareGivenNotInappropriateMedsRateLowerCI'] = f\"{ci_seek_care_given_not_inappropriate.low:.1%}\"\n",
    "latexvars['seekCareGivenNotInappropriateMedsRateUpperCI'] = f\"{ci_seek_care_given_not_inappropriate.high:.1%}\"\n",
    "\n",
    "latexvars['seekCareVsAppropriateTableP'] = f\"{fisher_exact(crosstab.to_numpy()).pvalue:.3f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ai_gpt41 = jsonl_load(gpt41_results_fp)\n",
    "results_ai_df_gpt41 = pd.DataFrame(results_ai_gpt41)\n",
    "results_ai_df_gpt41 = results_ai_df_gpt41.merge(df, left_on='visit_code', right_on='VisitCode', how='left', suffixes=('_physician', ''))\n",
    "\n",
    "results_ai_o3 = jsonl_load(o3_results_fp)\n",
    "results_ai_df_o3 = pd.DataFrame(results_ai_o3)\n",
    "results_ai_df_o3 = results_ai_df_o3.merge(df, left_on='visit_code', right_on='VisitCode', how='left', suffixes=('_physician', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ai_df(results_ai_df):\n",
    "    for col in ['history_likert', 'investigations_likert', 'diagnosis_likert', 'treatment_likert']:\n",
    "        results_ai_df[col] = results_ai_df[col].apply(intify_likert)\n",
    "\n",
    "    # zero out history Likert and MCQ for VisitDate > last_visitdate_with_cc\n",
    "    results_ai_df.loc[results_ai_df.VisitDate > last_visitdate_with_cc, ['history_likert']] = pd.NA\n",
    "\n",
    "    results_ai_df['history_likert_is_low'] = results_ai_df['history_likert'].apply(is_low_likert)\n",
    "    results_ai_df['investigations_likert_is_low'] = results_ai_df['investigations_likert'].apply(is_low_likert)\n",
    "    results_ai_df['diagnosis_likert_is_low'] = results_ai_df['diagnosis_likert'].apply(is_low_likert)\n",
    "    results_ai_df['treatment_likert_is_low'] = results_ai_df['treatment_likert'].apply(is_low_likert)\n",
    "\n",
    "    return results_ai_df\n",
    "\n",
    "results_ai_gpt41 = process_ai_df(results_ai_df_gpt41)\n",
    "results_ai_o3 = process_ai_df(results_ai_df_o3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbp_data_ai_analysis = {}\n",
    "\n",
    "for results_ai_df, model_name, latex_var_name in [\n",
    "    (results_ai_df_gpt41, 'GPT-4.1', 'GPTFourOne'),\n",
    "    (results_ai_df_o3, 'o3', 'OThree'),\n",
    "]:\n",
    "    print(f\"AI ANALYSIS FOR MODEL: {model_name.upper()}...\")\n",
    "    main_study_ai_df = results_ai_df[results_ai_df.VisitDate >= cut_point]\n",
    "\n",
    "    main_study_ai_lbp_data = get_likert_bar_plot_data(main_study_ai_df)\n",
    "    lbp_data_ai_analysis[model_name] = main_study_ai_lbp_data\n",
    "    lv = likert_bar_plot_data_to_latex_vars(main_study_ai_lbp_data, latex_var_name)\n",
    "    latexvars.update(lv)\n",
    "\n",
    "    fig, fig_df = clustered_bar_plot(\n",
    "        main_study_ai_lbp_data,\n",
    "        group_one_name = 'Non-AI',\n",
    "        group_two_name = 'AI',\n",
    "        fig_title = f'Error rates in history-taking, investigation, diagnosis & treatment questions\\nNon-AI vs AI in main study period\\nRatings provided by {model_name}',\n",
    "        y_axis_title = '% of visits with clinical errors for category',\n",
    "    )\n",
    "    save_fig(f'main_period_ai_likert_error_rates_{model_name}', fig)\n",
    "\n",
    "    fig = clustered_bar_plot(\n",
    "        get_rrr_bar_plot_data([main_study_df, main_study_ai_df]),\n",
    "        group_one_name = 'Ratings from human raters',\n",
    "        group_two_name = 'Ratings from LLM-based grader',\n",
    "        y_axis_title = '% of visits with clinical errors for category',\n",
    "    )\n",
    "\n",
    "    print(f\"AGREEMENT STATISTICS FOR {model_name.upper()}\")\n",
    "\n",
    "    results_ai_df.VisitCode = results_ai_df.VisitCode.astype(str)\n",
    "    results_clinical_study_df.VisitCode = results_clinical_study_df.VisitCode.astype(str)\n",
    "\n",
    "    combined_df = results_clinical_study_df.merge(results_ai_df, on='VisitCode', how='inner', suffixes=('', '_ai'))\n",
    "\n",
    "    within_one_agreement = {}\n",
    "    likert_cols = ['history_likert', 'investigations_likert', 'diagnosis_likert', 'treatment_likert']\n",
    "    for likert_col in likert_cols:\n",
    "        relevant_df = combined_df.dropna(subset=[likert_col, f'{likert_col}_ai'])\n",
    "        agreement = relevant_df.apply(lambda row: abs(row[likert_col] - row[f'{likert_col}_ai']) <= 1, axis=1)\n",
    "        within_one_agreement[likert_col] = agreement.value_counts(normalize=True)[True]\n",
    "        lv_string = f'{latex_var_name}{likert_col.replace(\"_likert\", \"\").capitalize()}WithinOneAgreement'\n",
    "        print(lv_string)\n",
    "        latexvars[lv_string] = f\"{within_one_agreement[likert_col]:.1%}\"\n",
    "        print(f\"{likert_col}: {within_one_agreement[likert_col]:.1%}\")\n",
    "\n",
    "    agreement_statistics = {}\n",
    "    for likert_col in likert_cols:\n",
    "        error_col_physician = f'{likert_col}_is_low'\n",
    "        error_col_ai = f'{likert_col}_is_low_ai'\n",
    "\n",
    "        relevant_df = combined_df.dropna(subset=[error_col_physician, error_col_ai])\n",
    "        fleiss_kappa_results = []\n",
    "\n",
    "        for _, row in relevant_df.iterrows():\n",
    "            is_error_ai = row[error_col_ai]\n",
    "            is_error_physician = row[error_col_physician]\n",
    "\n",
    "            fleiss_kappa_results.append({\n",
    "                0: (is_error_ai == 0) + (is_error_physician == 0),\n",
    "                1: (is_error_ai == 1) + (is_error_physician == 1),\n",
    "            })\n",
    "\n",
    "        fleiss_kappa_df = pd.DataFrame(fleiss_kappa_results)\n",
    "        fk = fleiss_kappa(fleiss_kappa_df.values, method=\"fleiss\")\n",
    "\n",
    "        agreement_statistics[likert_col] = {\n",
    "            'fleiss_kappa': fk,\n",
    "        }\n",
    "        lv_string = f'{latex_var_name}{likert_col.replace(\"_likert\", \"\").capitalize()}FleissKappa'\n",
    "        latexvars[lv_string] = f\"{fk:.3f}\"\n",
    "        print(lv_string)\n",
    "\n",
    "    display(pd.DataFrame(agreement_statistics))\n",
    "\n",
    "    for domain in ['history', 'investigations', 'diagnosis', 'treatment']:\n",
    "\n",
    "        main_study_ai_df_notna = main_study_ai_df.dropna(subset=[f'{domain}_likert_is_low'])\n",
    "        main_study_ai_df_notna[f'{domain}_likert_is_low'] = main_study_ai_df_notna[f'{domain}_likert_is_low'].astype(int)\n",
    "\n",
    "        # GEE LOG-BINOMIAL MODEL\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"{model_name.upper()}: {domain.upper()}: GEE LOG-BINOMIAL MODEL\")\n",
    "        print(\"-\" * 100)\n",
    "        result = smf.gee(\n",
    "            formula.format(domain=domain),\n",
    "            groups = main_study_ai_df_notna.user_id,\n",
    "            data=main_study_ai_df_notna,\n",
    "            family=sm.families.Binomial(link=sm.families.links.Log()),\n",
    "            cov_struct = sm.genmod.cov_struct.Exchangeable(),\n",
    "        ).fit()\n",
    "\n",
    "        display(result.summary().tables[0])\n",
    "        display(tidy_risk_ratios(result))\n",
    "\n",
    "        main_effect, tidied_df = clean_regression_result(result)\n",
    "        save_latex(f'{model_name}_gee_{domain}', tidied_df)\n",
    "        lv = main_effect_to_rrr(main_effect, f'{latex_var_name}GEE{domain}')\n",
    "        latexvars.update(lv)\n",
    "        print(lv)\n",
    "\n",
    "        # LOG-POISSON MODEL WITH ZOU-DONNER CLUSTERING FOR COVARIANCE\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"{model_name.upper()}: {domain.upper()}: LOG-POISSON MODEL WITH ZOU-DONNER CLUSTERING FOR COVARIANCE\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "        model = smf.glm(\n",
    "            formula.format(domain=domain),\n",
    "            data=main_study_ai_df_notna,\n",
    "            family=sm.families.Poisson(link=sm.families.links.Log()),\n",
    "        )\n",
    "\n",
    "        result = model.fit(\n",
    "            cov_type='cluster',\n",
    "            cov_kwds={'groups': main_study_ai_df_notna['user_id']}\n",
    "        )\n",
    "\n",
    "        display(result.summary().tables[0])\n",
    "        display(tidy_risk_ratios(result))\n",
    "\n",
    "        main_effect, tidied_df = clean_regression_result(result)\n",
    "        save_latex(f'{model_name}_poisson_{domain}', tidied_df)\n",
    "        lv = main_effect_to_rrr(main_effect, f'{latex_var_name}Poisson{domain}')\n",
    "        latexvars.update(lv)\n",
    "        print(lv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_one_agreement_table = {\n",
    "    'Physician-physician agreement': {\n",
    "        category.capitalize(): latexvars[f'{category}LikertOnePointProp']\n",
    "        for category in ['history', 'investigations', 'diagnosis', 'treatment']\n",
    "    },\n",
    "    'GPT-4.1-physician agreement': {\n",
    "        category.capitalize(): latexvars[f'GPTFourOne{category.capitalize()}WithinOneAgreement']\n",
    "        for category in ['history', 'investigations', 'diagnosis', 'treatment']\n",
    "    },\n",
    "    'o3-physician agreement': {\n",
    "        category.capitalize(): latexvars[f'OThree{category.capitalize()}WithinOneAgreement']\n",
    "        for category in ['history', 'investigations', 'diagnosis', 'treatment']\n",
    "    }\n",
    "}\n",
    "within_one_agreement_table_df = pd.DataFrame(within_one_agreement_table)\n",
    "save_latex('within_one_agreement_table', within_one_agreement_table_df)\n",
    "within_one_agreement_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fleiss_kappa_table = {\n",
    "    'Physician-physician $\\kappa$': {\n",
    "        category.capitalize(): latexvars[f'fleissKappa{category.capitalize()}']\n",
    "        for category in ['history', 'investigations', 'diagnosis', 'treatment']\n",
    "    },\n",
    "    'GPT-4.1-physician $\\kappa$': {\n",
    "        category.capitalize(): latexvars[f'GPTFourOne{category.capitalize()}FleissKappa']\n",
    "        for category in ['history', 'investigations', 'diagnosis', 'treatment']\n",
    "    },\n",
    "    'o3-physician $\\kappa$': {\n",
    "        category.capitalize(): latexvars[f'OThree{category.capitalize()}FleissKappa']\n",
    "        for category in ['history', 'investigations', 'diagnosis', 'treatment']\n",
    "    }\n",
    "}\n",
    "fleiss_kappa_table_df = pd.DataFrame(fleiss_kappa_table)\n",
    "save_latex('fleiss_kappa_table', fleiss_kappa_table_df)\n",
    "fleiss_kappa_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_lbp_data = lbp_data_ai_analysis['GPT-4.1']\n",
    "o3_lbp_data = lbp_data_ai_analysis['o3']\n",
    "\n",
    "humans_ai_df = pd.DataFrame({\n",
    "    'Physician raters': lbp_data_to_rrr_series(main_study_lbp_data),\n",
    "    'GPT-4.1': lbp_data_to_rrr_series(gpt_lbp_data),\n",
    "    'o3': lbp_data_to_rrr_series(o3_lbp_data),\n",
    "})\n",
    "save_latex('physicians_ai_rrr_table', humans_ai_df)\n",
    "humans_ai_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcomes analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_study_all_results_df = df[df.VisitDate >= cut_point]\n",
    "main_outcome_bar_plot_data = get_likert_bar_plot_data(\n",
    "    main_study_all_results_df,\n",
    "    allocation_col = 'ClinicianGroup',\n",
    "    allocation_group_one_values = ['Non-AI'],\n",
    "    allocation_group_two_values = ['AI'],\n",
    "    outcome_cols = ['Q1'],\n",
    "    outcome_true_values = [1, 2, 3],\n",
    "    outcome_false_values = [4, 5],\n",
    "    unique_column = None\n",
    ")\n",
    "\n",
    "lv = likert_bar_plot_data_to_latex_vars(main_outcome_bar_plot_data, prefix = 'notFeelingBetter')\n",
    "lv = {k.replace('Q1', ''): v for k, v in lv.items()}\n",
    "latexvars.update(lv)\n",
    "\n",
    "main_outcome_bar_plot_data = {\n",
    "    \"Rate of patients not feeling better\": v\n",
    "    for k, v in main_outcome_bar_plot_data.items()\n",
    "}\n",
    "clustered_bar_plot(\n",
    "    main_outcome_bar_plot_data,\n",
    "    group_one_name = 'Non-AI',\n",
    "    group_two_name = 'AI',\n",
    "    fig_title = 'Rate of patients not feeling better',\n",
    "    y_axis_title = \"Proportion of visits\",\n",
    "    figsize = (5, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_study_all_results_df.loc[:, 'Saw a pharmacist'] = main_study_all_results_df.Q2.apply(lambda x: x == \"another_chemist\" if pd.notna(x) else pd.NA)\n",
    "main_study_all_results_df.loc[:, 'Self-referred to another clinic or hospital'] = main_study_all_results_df.Q2.apply(lambda x: x == \"self_referred\" if pd.notna(x) else pd.NA)\n",
    "main_study_all_results_df.loc[:, 'Unplanned visit at Penda'] = main_study_all_results_df.HadUnplannedVisit.apply(lambda x: x == 1 if pd.notna(x) else pd.NA)\n",
    "\n",
    "other_patient_outcome_bar_plot_data = get_likert_bar_plot_data(\n",
    "    main_study_all_results_df,\n",
    "    allocation_col = 'ClinicianGroup',\n",
    "    allocation_group_one_values = ['Non-AI'],\n",
    "    allocation_group_two_values = ['AI'],\n",
    "    outcome_cols = ['Saw a pharmacist', 'Self-referred to another clinic or hospital', 'Unplanned visit at Penda'],\n",
    "    outcome_true_values = [True],\n",
    "    outcome_false_values = [False],\n",
    "    unique_column = None\n",
    ")\n",
    "\n",
    "clustered_bar_plot(\n",
    "    other_patient_outcome_bar_plot_data,\n",
    "    group_one_name = 'Non-AI',\n",
    "    group_two_name = 'AI',\n",
    "    fig_title = 'Additional patient outcomes',\n",
    "    y_axis_title = \"Proportion of visits\",\n",
    "    figsize = (10, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_study_all_results_df.loc[:, 'Feeling worse on one-day follow-up'] = main_study_all_results_df.one_day_call_outcome.apply(lambda x: x == 'Yes' if pd.notna(x) else pd.NA)\n",
    "\n",
    "one_day_outcome_bar_plot_data = get_likert_bar_plot_data(\n",
    "    main_study_all_results_df,\n",
    "    allocation_col = 'ClinicianGroup',\n",
    "    allocation_group_one_values = ['Non-AI'],\n",
    "    allocation_group_two_values = ['AI'],\n",
    "    outcome_cols = ['Feeling worse on one-day follow-up'],\n",
    "    outcome_true_values = [True],\n",
    "    outcome_false_values = [False],\n",
    "    unique_column = None\n",
    ")\n",
    "\n",
    "clustered_bar_plot(\n",
    "    one_day_outcome_bar_plot_data,\n",
    "    group_one_name = 'Non-AI',\n",
    "    group_two_name = 'AI',\n",
    "    fig_title = 'One-day follow-up outcome',\n",
    "    y_axis_title = \"Proportion of visits\",\n",
    "    figsize = (10, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_ai_series = pd.concat([\n",
    "    lbp_data_to_group_one_series(main_outcome_bar_plot_data),\n",
    "    lbp_data_to_group_one_series(other_patient_outcome_bar_plot_data),\n",
    "    lbp_data_to_group_one_series(one_day_outcome_bar_plot_data)\n",
    "])\n",
    "\n",
    "ai_series = pd.concat([\n",
    "    lbp_data_to_group_two_series(main_outcome_bar_plot_data),\n",
    "    lbp_data_to_group_two_series(other_patient_outcome_bar_plot_data),\n",
    "    lbp_data_to_group_two_series(one_day_outcome_bar_plot_data)\n",
    "])\n",
    "\n",
    "pval_series = pd.concat([\n",
    "    lbp_data_to_pval_series(main_outcome_bar_plot_data),\n",
    "    lbp_data_to_pval_series(other_patient_outcome_bar_plot_data),\n",
    "    lbp_data_to_pval_series(one_day_outcome_bar_plot_data)\n",
    "])\n",
    "\n",
    "patient_outcomes_df = pd.DataFrame({\n",
    "    \"Rate in non-AI group\": non_ai_series,\n",
    "    \"Rate in AI group\": ai_series,\n",
    "    \"p\": pval_series\n",
    "})\n",
    "\n",
    "save_latex('patient_outcomes', patient_outcomes_df)\n",
    "patient_outcomes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_study_all_results_df = df[df.VisitDate >= cut_point]\n",
    "main_outcome_bar_plot_data = get_likert_bar_plot_data(\n",
    "    main_study_all_results_df,\n",
    "    allocation_col = 'ClinicianGroup',\n",
    "    allocation_group_one_values = ['Non-AI'],\n",
    "    allocation_group_two_values = ['AI'],\n",
    "    outcome_cols = ['Q1'],\n",
    "    outcome_true_values = [1, 2, 3],\n",
    "    outcome_false_values = [4, 5],\n",
    "    unique_column = None\n",
    ")\n",
    "\n",
    "q1_non_ai_rate = main_outcome_bar_plot_data['Q1']['first_group_rate']\n",
    "q1_ai_rate = main_outcome_bar_plot_data['Q1']['second_group_rate']\n",
    "\n",
    "\n",
    "main_study_all_results_df.loc[:, 'Q1: Favorable imputation'] = main_study_all_results_df.apply(lambda row: row.Q1 if pd.notna(row.Q1) else (5 if row.ClinicianGroup == 'AI' else 1), axis=1)\n",
    "main_outcome_bar_plot_data_favorable_imputation = get_likert_bar_plot_data(\n",
    "    main_study_all_results_df,\n",
    "    allocation_col = 'ClinicianGroup',\n",
    "    allocation_group_one_values = ['Non-AI'],\n",
    "    allocation_group_two_values = ['AI'],\n",
    "    outcome_cols = ['Q1: Favorable imputation'],\n",
    "    outcome_true_values = [1, 2, 3],\n",
    "    outcome_false_values = [4, 5],\n",
    "    unique_column = None\n",
    ")\n",
    "\n",
    "main_study_all_results_df.loc[:, 'Q1: Unfavorable imputation'] = main_study_all_results_df.apply(lambda row: row.Q1 if pd.notna(row.Q1) else (1 if row.ClinicianGroup == 'AI' else 5), axis=1)\n",
    "main_outcome_bar_plot_data_unfavorable_imputation = get_likert_bar_plot_data(\n",
    "    main_study_all_results_df,\n",
    "    allocation_col = 'ClinicianGroup',\n",
    "    allocation_group_one_values = ['Non-AI'],\n",
    "    allocation_group_two_values = ['AI'],\n",
    "    outcome_cols = ['Q1: Unfavorable imputation'],\n",
    "    outcome_true_values = [1, 2, 3],\n",
    "    outcome_false_values = [4, 5],\n",
    "    unique_column = None\n",
    ")\n",
    "\n",
    "non_ai_series = pd.concat([\n",
    "    lbp_data_to_group_one_series(main_outcome_bar_plot_data),\n",
    "    lbp_data_to_group_one_series(main_outcome_bar_plot_data_favorable_imputation),\n",
    "    lbp_data_to_group_one_series(main_outcome_bar_plot_data_unfavorable_imputation)\n",
    "])\n",
    "\n",
    "ai_series = pd.concat([\n",
    "    lbp_data_to_group_two_series(main_outcome_bar_plot_data),\n",
    "    lbp_data_to_group_two_series(main_outcome_bar_plot_data_favorable_imputation),\n",
    "    lbp_data_to_group_two_series(main_outcome_bar_plot_data_unfavorable_imputation)\n",
    "])\n",
    "\n",
    "pval_series = pd.concat([\n",
    "    lbp_data_to_pval_series(main_outcome_bar_plot_data),\n",
    "    lbp_data_to_pval_series(main_outcome_bar_plot_data_favorable_imputation),\n",
    "    lbp_data_to_pval_series(main_outcome_bar_plot_data_unfavorable_imputation)\n",
    "])\n",
    "\n",
    "imputation_df = pd.DataFrame({\n",
    "    \"non_ai\": non_ai_series,\n",
    "    \"ai\": ai_series,\n",
    "    \"pval\": pval_series\n",
    "})\n",
    "save_latex(\"imputation_df\", imputation_df)\n",
    "imputation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_silent_ai_survey_responses = 23\n",
    "n_active_ai_survey_responses = 36\n",
    "pct_silent_ai_survey_responses = rf'{n_silent_ai_survey_responses / nsilentai * 100:.0f}\\%'\n",
    "pct_active_ai_survey_responses = rf'{n_active_ai_survey_responses / nactiveai * 100:.0f}\\%'\n",
    "latexvars['nSilentAISurveyResponses'] = n_silent_ai_survey_responses\n",
    "latexvars['nActiveAISurveyResponses'] = n_active_ai_survey_responses\n",
    "latexvars['pctSilentAISurveyResponses'] = pct_silent_ai_survey_responses\n",
    "latexvars['pctActiveAISurveyResponses'] = pct_active_ai_survey_responses\n",
    "pct_silent_ai_survey_responses, pct_active_ai_survey_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original counts for each group\n",
    "silent_quality_of_care = {\n",
    "    3: 4,\n",
    "    4: 6,\n",
    "    5: 13\n",
    "}\n",
    "active_quality_of_care = {\n",
    "    3: 0,\n",
    "    4: 8,\n",
    "    5: 28\n",
    "}\n",
    "\n",
    "# Expand counts into lists of scores\n",
    "silent_scores = []\n",
    "for score, count in silent_quality_of_care.items():\n",
    "    silent_scores.extend([score] * count)\n",
    "\n",
    "active_scores = []\n",
    "for score, count in active_quality_of_care.items():\n",
    "    active_scores.extend([score] * count)\n",
    "\n",
    "# Mann-Whitney U test\n",
    "u_stat, p_value = mannwhitneyu(active_scores, silent_scores, alternative='two-sided')\n",
    "print(f\"Mann-Whitney U test: U={u_stat}, p={p_value}\")\n",
    "\n",
    "latexvars['emrQualityUPVal'] = f'{p_value:.3f}'\n",
    "latexvars['emrQualityActivePctFive'] = rf\"{active_quality_of_care[5] / sum(active_quality_of_care.values()) * 100:.0f}\\%\"\n",
    "latexvars['emrQualitySilentPctFive'] = rf\"{silent_quality_of_care[5] / sum(silent_quality_of_care.values()) * 100:.0f}\\%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_ai_specific_quality_of_care = {\n",
    "    4: 9,\n",
    "    5: 27\n",
    "}\n",
    "\n",
    "active_ai_specific_satisfaction = {\n",
    "    4: 21,\n",
    "    5: 15\n",
    "}\n",
    "\n",
    "active_ai_promoter_score = {\n",
    "    8: 8,\n",
    "    9: 16,\n",
    "    10: 12\n",
    "}\n",
    "\n",
    "nps = sum(v for k, v in active_ai_promoter_score.items() if k >= 9) - sum(v for k, v in active_ai_promoter_score.items() if k <= 6)\n",
    "nps_pct = nps / sum(active_ai_promoter_score.values())\n",
    "\n",
    "latexvars['activeAISpecificNPS'] = rf'{nps_pct * 100:.0f}'\n",
    "latexvars['activeAISatisfactionPctFour'] = rf'{active_ai_specific_satisfaction[4] / sum(active_ai_specific_satisfaction.values()) * 100:.0f}\\%'\n",
    "latexvars['activeAISatisfactionPctFive'] = rf'{active_ai_specific_satisfaction[5] / sum(active_ai_specific_satisfaction.values()) * 100:.0f}\\%'\n",
    "latexvars['activeAISpecificQualityPctFour'] = rf'{active_ai_specific_quality_of_care[4] / sum(active_ai_specific_quality_of_care.values()) * 100:.0f}\\%'\n",
    "latexvars['activeAISpecificQualityPctFive'] = rf'{active_ai_specific_quality_of_care[5] / sum(active_ai_specific_quality_of_care.values()) * 100:.0f}\\%'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_of_care_long_format = []\n",
    "for k, v in silent_quality_of_care.items():\n",
    "    for _ in range(v):\n",
    "        quality_of_care_long_format.append({\n",
    "            'ClinicianGroup': 'Non-AI',\n",
    "            'Substantially worsens': 1 if k == 1 else 0,\n",
    "            'Somewhat worsens': 1 if k == 2 else 0,\n",
    "            'No change': 1 if k == 3 else 0,\n",
    "            'Somewhat improves': 1 if k == 4 else 0,\n",
    "            'Substantially improves': 1 if k == 5 else 0,\n",
    "        })\n",
    "\n",
    "for k, v in active_quality_of_care.items():\n",
    "    for _ in range(v):\n",
    "        quality_of_care_long_format.append({\n",
    "            'ClinicianGroup': 'AI',\n",
    "            'Substantially worsens': 1 if k == 1 else 0,\n",
    "            'Somewhat worsens': 1 if k == 2 else 0,\n",
    "            'No change': 1 if k == 3 else 0,\n",
    "            'Somewhat improves': 1 if k == 4 else 0,\n",
    "            'Substantially improves': 1 if k == 5 else 0,\n",
    "        })\n",
    "\n",
    "quality_of_care_long_format_df = pd.DataFrame(quality_of_care_long_format)\n",
    "\n",
    "quality_of_care_lbp_data = get_likert_bar_plot_data(\n",
    "    quality_of_care_long_format_df,\n",
    "    allocation_col = 'ClinicianGroup',\n",
    "    allocation_group_one_values = ['Non-AI'],\n",
    "    allocation_group_two_values = ['AI'],\n",
    "    outcome_cols = ['Substantially worsens', 'Somewhat worsens', 'No change', 'Somewhat improves', 'Substantially improves'],\n",
    "    outcome_true_values = [1],\n",
    "    outcome_false_values = [0],\n",
    "    unique_column = None\n",
    ")\n",
    "fig, fig_df = clustered_bar_plot(\n",
    "    {f'{k}\\n\\n': v for k, v in quality_of_care_lbp_data.items()},\n",
    "    group_one_name = 'Non-AI',\n",
    "    group_two_name = 'AI',\n",
    "    fig_title = 'Clinician-reported impact of the EMR (including AI consult, if present) on quality of care',\n",
    "    y_axis_title = '% of responses',\n",
    "    show_significance = False,\n",
    "    figsize = (10, 5),\n",
    ")\n",
    "save_fig('quality_of_care_emr_impact', fig)\n",
    "save_csv('quality_of_care_emr_impact', fig_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dict_to_percentages_and_errors(d, order):\n",
    "    \"\"\"\n",
    "    Convert a {score: count} dictionary into two lists:\n",
    "    (1) percentages (as floats, e.g. 0.23)\n",
    "    (2) error bars as a (2, n) array: [[lower_errs], [upper_errs]]\n",
    "    \"\"\"\n",
    "    total = sum(d.values())\n",
    "    pct = []\n",
    "    lower_errs = []\n",
    "    upper_errs = []\n",
    "    for k in order:\n",
    "        y = d.get(k, 0)\n",
    "        n = total\n",
    "        bt = binomtest(y, n)\n",
    "        ci = bt.proportion_ci(confidence_level=0.95, method='wilson')\n",
    "        p = bt.statistic\n",
    "        pct.append(p)\n",
    "        lower_errs.append(p - ci.low)\n",
    "        upper_errs.append(ci.high - p)\n",
    "    # yerr must be shape (2, n)\n",
    "    yerr = [lower_errs, upper_errs]\n",
    "    return pct, yerr\n",
    "\n",
    "score_order = [1, 2, 3, 4, 5]\n",
    "\n",
    "active_pct, active_err = _dict_to_percentages_and_errors(\n",
    "    active_quality_of_care, score_order\n",
    ")\n",
    "silent_pct, silent_err = _dict_to_percentages_and_errors(\n",
    "    silent_quality_of_care, score_order\n",
    ")\n",
    "quality_of_care_pct, quality_of_care_err = _dict_to_percentages_and_errors(\n",
    "    active_ai_specific_quality_of_care, score_order\n",
    ")\n",
    "satisfaction_pct, satisfaction_err = _dict_to_percentages_and_errors(\n",
    "    active_ai_specific_satisfaction, score_order\n",
    ")\n",
    "\n",
    "nps_order = list(range(11))\n",
    "nps_pct, nps_err = _dict_to_percentages_and_errors(\n",
    "    active_ai_promoter_score, nps_order\n",
    ")\n",
    "\n",
    "def plot_pct_bar(pct, err, labels, title, color=None, ylim=(0, 0.9), ylabel=\"% of responses\"):\n",
    "    if color is None:\n",
    "        # Use matplotlib's default color cycle, c1 is the second color\n",
    "        color = plt.rcParams['axes.prop_cycle'].by_key()['color'][1]\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    x = np.arange(len(pct))\n",
    "    ax.bar(x, pct, color=color, yerr=err, capsize=4)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=0, ha=\"center\")\n",
    "    ax.set_ylim(*ylim)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Build wrapped labels for each plot\n",
    "wrap = lambda s: textwrap.fill(s, width=13)\n",
    "\n",
    "labels_qoc = list(map(wrap, [\n",
    "    \"Substantially worsens quality\",\n",
    "    \"Somewhat worsens quality\",\n",
    "    \"Does not change quality\",\n",
    "    \"Somewhat improves quality\",\n",
    "    \"Substantially improves quality\",\n",
    "]))\n",
    "\n",
    "labels_sat = list(map(wrap, [\n",
    "    \"Very dissatisfied\",\n",
    "    \"Somewhat dissatisfied\",\n",
    "    \"Neither satisfied nor dissatisfied\",\n",
    "    \"Somewhat satisfied\",\n",
    "\n",
    "    \"Very satisfied\",\n",
    "]))\n",
    "\n",
    "labels_nps = list(map(str, nps_order))\n",
    "\n",
    "# Plot and save all figures\n",
    "fig_qoc = plot_pct_bar(\n",
    "    quality_of_care_pct, quality_of_care_err, labels_qoc,\n",
    "    \"AI group – impact of AI consult on quality of care\"\n",
    ")\n",
    "plt.show()\n",
    "save_fig('quality_of_care_ai_impact', fig_qoc)\n",
    "\n",
    "fig_sat = plot_pct_bar(\n",
    "    satisfaction_pct, satisfaction_err, labels_sat,\n",
    "    \"AI group – satisfaction with AI consult\"\n",
    ")\n",
    "plt.show()\n",
    "save_fig('satisfaction_ai_impact', fig_sat)\n",
    "\n",
    "fig_nps = plot_pct_bar(\n",
    "    nps_pct, nps_err, labels_nps,\n",
    "    \"AI group – Net Promoter Score (NPS) for AI consult\", ylim=(0, 0.7), ylabel=\"% of responses\"\n",
    ")\n",
    "plt.show()\n",
    "save_fig('nps_ai_impact', fig_nps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for quality of care\n",
    "df_quality_of_care = pd.DataFrame({\n",
    "    \"cluster\": [\n",
    "        \"Substantially worsens quality\",\n",
    "        \"Somewhat worsens quality\",\n",
    "        \"Does not change quality\",\n",
    "        \"Somewhat improves quality\",\n",
    "        \"Substantially improves quality\",\n",
    "    ],\n",
    "    \"value\": quality_of_care_pct,\n",
    "    \"lower CI\": [k - v for k, v in zip(quality_of_care_pct, quality_of_care_err[0])],\n",
    "    \"upper CI\": [k + v for k, v in zip(quality_of_care_pct, quality_of_care_err[1])],\n",
    "})\n",
    "\n",
    "save_csv('quality_of_care_ai_impact', df_quality_of_care)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red reduction rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_reds_over_time_df(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    first_final_red: Literal[\"first\", \"final\", \"all\"] = \"final\",\n",
    "    filter_to_rules: list[types.ClinicalDecisionRule] | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Return a tidy DataFrame with (per-week, per-group) red-flag percentages and\n",
    "    Wilson confidence intervals.\n",
    "\n",
    "    This function *only* prepares data – it performs **no** plotting or other\n",
    "    side-effects.\n",
    "    \"\"\"\n",
    "\n",
    "    calls_df = df.dropna(subset=[\"AICalls\"]).copy()\n",
    "\n",
    "    if filter_to_rules is not None:\n",
    "        calls_df[\"AICalls\"] = calls_df[\"AICalls\"].apply(\n",
    "            lambda x: x.for_rules(filter_to_rules)\n",
    "        )\n",
    "\n",
    "    if first_final_red == \"first\":\n",
    "        calls_df[\"red\"] = calls_df.apply(\n",
    "            lambda row: row[\"AICalls\"].any_first_red, axis=1\n",
    "        )\n",
    "    elif first_final_red == \"final\":\n",
    "        calls_df[\"red\"] = calls_df.apply(\n",
    "            lambda row: row[\"AICalls\"].any_final_red, axis=1\n",
    "        )\n",
    "    else:  # \"all\"\n",
    "        calls_df[\"calls\"] = calls_df[\"AICalls\"].apply(lambda x: x.calls)\n",
    "        calls_df = calls_df.explode(\"calls\", ignore_index=True)\n",
    "        calls_df[\"red\"] = calls_df[\"calls\"].apply(lambda c: c.color == types.Color.Red)\n",
    "\n",
    "    calls_df = calls_df.dropna(subset=[\"red\"]).copy()\n",
    "\n",
    "    calls_df[\"VisitDate\"] = pd.to_datetime(calls_df[\"VisitDate\"])\n",
    "    calls_df[\"week\"] = calls_df[\"VisitDate\"].dt.to_period(\"W\").apply(\n",
    "        lambda r: r.start_time\n",
    "    )\n",
    "\n",
    "    summary = (\n",
    "        calls_df.groupby([\"week\", \"ClinicianGroup\"])[\"red\"]\n",
    "        .agg([\"sum\", \"count\"])\n",
    "        .reset_index()\n",
    "        .rename(columns={\"sum\": \"n_red\", \"count\": \"n\"})\n",
    "    )\n",
    "\n",
    "    pct_red, err_lower, err_upper = [], [], []\n",
    "    for _, row in summary.iterrows():\n",
    "        bt = binomtest(int(row[\"n_red\"]), int(row[\"n\"]))\n",
    "        ci = bt.proportion_ci(method=\"wilson\")\n",
    "        p = bt.statistic\n",
    "        pct_red.append(p)\n",
    "        err_lower.append(p - ci.low)\n",
    "        err_upper.append(ci.high - p)\n",
    "\n",
    "    summary[\"pct_final_red\"] = pct_red\n",
    "    summary[\"err_lower\"] = err_lower\n",
    "    summary[\"err_upper\"] = err_upper\n",
    "\n",
    "    return summary\n",
    "\n",
    "def plot_no_ai_ai_over_time(\n",
    "    red_by_week: pd.DataFrame,\n",
    "    x_var: str = 'week',\n",
    "    y_var: str = 'pct_final_red',\n",
    "    x_lim: tuple[float, float] | None = None,\n",
    "    y_lim: tuple[float, float] | None = None,\n",
    "    x_label: str = '',\n",
    "    y_label: str = '',\n",
    "    title: str = '',\n",
    "    grouping_var: str = 'ClinicianGroup',\n",
    "    grouping_var_label: str = 'Group',\n",
    "    group_order: list[str] = [\"Non-AI\", \"AI\"],\n",
    "    y_axis_percent: bool = True,\n",
    "    time_x_axis: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given the output of `compute_final_reds_over_time_df`, render the plot and\n",
    "    return the Matplotlib `Figure`.\n",
    "    \"\"\"\n",
    "    plt.close(\"all\")\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    for group in group_order:\n",
    "        gdf = red_by_week[red_by_week[grouping_var] == group]\n",
    "        yerr = [gdf[\"err_lower\"], gdf[\"err_upper\"]]\n",
    "        ax.errorbar(\n",
    "            gdf[x_var],\n",
    "            gdf[y_var],\n",
    "            yerr=yerr,\n",
    "            marker=\"o\",\n",
    "            capsize=4,\n",
    "            linestyle=\"-\",\n",
    "            label=group,\n",
    "        )\n",
    "\n",
    "    if x_lim is not None:\n",
    "        ax.set_xlim(x_lim)\n",
    "\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    if y_lim:\n",
    "        ax.set_ylim(y_lim)\n",
    "    else:\n",
    "        ax.set_ylim(0, ymax * 1.05)\n",
    "\n",
    "    if time_x_axis:\n",
    "        cut_point_num: float = float(mdates.date2num(cut_point))\n",
    "        ax.axvline(cut_point_num, color=\"gray\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "        ax.xaxis.set_major_locator(mdates.WeekdayLocator())\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%b %d\\n%Y\"))\n",
    "\n",
    "        ax.annotate(\n",
    "            \"Induction period\",\n",
    "            xy=(cut_point_num, 0),\n",
    "            xytext=(cut_point_num - 3, 0),\n",
    "            ha=\"right\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=10,\n",
    "            backgroundcolor=\"white\",\n",
    "            bbox=dict(facecolor=\"white\", edgecolor=\"none\", alpha=0.7),\n",
    "            arrowprops=dict(arrowstyle=\"-\", color=\"gray\", lw=0),\n",
    "        )\n",
    "        ax.annotate(\n",
    "            \"Active deployment\",\n",
    "            xy=(cut_point_num, 0),\n",
    "            xytext=(cut_point_num + 3, 0),\n",
    "            ha=\"left\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=10,\n",
    "            backgroundcolor=\"white\",\n",
    "            bbox=dict(facecolor=\"white\", edgecolor=\"none\", alpha=0.7),\n",
    "            arrowprops=dict(arrowstyle=\"-\", color=\"gray\", lw=0),\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(title)\n",
    "    ax.legend(title=grouping_var_label)\n",
    "    if y_axis_percent:\n",
    "        ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# 3. Convenience wrapper (preserves original API)                             #\n",
    "# --------------------------------------------------------------------------- #\n",
    "def plot_final_reds_over_time(\n",
    "    df: pd.DataFrame,\n",
    "    first_final_red: Literal[\"first\", \"final\", \"all\"] = \"final\",\n",
    "    filter_to_rules: list[types.ClinicalDecisionRule] | None = None,\n",
    "    y_lim: tuple[float, float] | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Back-compatibility wrapper that:\n",
    "    1. Generates the summary data frame.\n",
    "    2. Creates the plot.\n",
    "    3. Returns both, keeping the original return signature intact.\n",
    "    \"\"\"\n",
    "    red_by_week = compute_final_reds_over_time_df(\n",
    "        df,\n",
    "        first_final_red=first_final_red,\n",
    "        filter_to_rules=filter_to_rules,\n",
    "    )\n",
    "    rule_suffix = (\n",
    "        \"\" if filter_to_rules is None else \"\\n\" + \", \".join(r.value for r in filter_to_rules)\n",
    "    )\n",
    "    fig = plot_no_ai_ai_over_time(\n",
    "        red_by_week,\n",
    "        y_label=f\"% of visits with {first_final_red.title()} red\" + rule_suffix,\n",
    "        title=f\"% of {first_final_red.title()} Reds Over Time by Clinician Group\" + rule_suffix,\n",
    "        y_lim=y_lim,\n",
    "    )\n",
    "    return red_by_week, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_by_week, fig = plot_final_reds_over_time(df, first_final_red = 'final', y_lim = (0, 0.6))\n",
    "save_fig(\"final_reds_over_time\", fig)\n",
    "save_csv(\"final_reds_over_time\", red_by_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_by_week, fig = plot_final_reds_over_time(df, first_final_red = 'first', y_lim = (0, 0.6))\n",
    "save_fig(\"first_reds_over_time\", fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_by_week, fig = plot_final_reds_over_time(df, first_final_red = 'first', filter_to_rules = [types.ClinicalDecisionRule.VitalsChiefComplaintEvaluation, types.ClinicalDecisionRule.ClinicalNotes])\n",
    "save_fig(\"first_reds_over_time_vitals_history\", fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_by_week, fig = plot_final_reds_over_time(df, first_final_red = 'final', filter_to_rules = [types.ClinicalDecisionRule.TreatmentRecommendation])\n",
    "save_fig(\"final_reds_over_time_treatment\", fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_by_week, fig = plot_final_reds_over_time(df, first_final_red = 'first', filter_to_rules = [types.ClinicalDecisionRule.TreatmentRecommendation])\n",
    "save_fig(\"first_reds_over_time_treatment\", fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_provider_level_final_red_heterogeneity_df(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    first_final_red: Literal[\"first\", \"final\", \"all\"] = \"final\",\n",
    "    filter_to_rules: list[types.ClinicalDecisionRule] | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Return a tidy DataFrame with (per-week, per-group) red-flag percentages and\n",
    "    Wilson confidence intervals.\n",
    "\n",
    "    This function *only* prepares data – it performs **no** plotting or other\n",
    "    side-effects.\n",
    "    \"\"\"\n",
    "\n",
    "    calls_df = df.dropna(subset=[\"AICalls\"]).copy()\n",
    "\n",
    "    if filter_to_rules is not None:\n",
    "        calls_df[\"AICalls\"] = calls_df[\"AICalls\"].apply(\n",
    "            lambda x: x.for_rules(filter_to_rules)\n",
    "        )\n",
    "\n",
    "    if first_final_red == \"first\":\n",
    "        calls_df[\"red\"] = calls_df.apply(\n",
    "            lambda row: row[\"AICalls\"].any_first_red, axis=1\n",
    "        )\n",
    "    elif first_final_red == \"final\":\n",
    "        calls_df[\"red\"] = calls_df.apply(\n",
    "            lambda row: row[\"AICalls\"].any_final_red, axis=1\n",
    "        )\n",
    "    else:  # \"all\"\n",
    "        calls_df[\"calls\"] = calls_df[\"AICalls\"].apply(lambda x: x.calls)\n",
    "        calls_df = calls_df.explode(\"calls\", ignore_index=True)\n",
    "        calls_df[\"red\"] = calls_df[\"calls\"].apply(lambda c: c.color == types.Color.Red)\n",
    "\n",
    "    calls_df = calls_df.dropna(subset=[\"red\"]).copy()\n",
    "\n",
    "    calls_df['UserID'] = calls_df['UserIDs'].apply(lambda x: x[0] if len(x) == 1 else pd.NA)\n",
    "    calls_df = calls_df.drop(columns=[\"UserIDs\"])\n",
    "\n",
    "    calls_df = calls_df[calls_df.ClinicianGroup == \"AI\"]\n",
    "\n",
    "    # First, get pct_red for each provider per week\n",
    "    provider_summary = (\n",
    "        calls_df.groupby([\"week\", \"UserID\"])[\"red\"]\n",
    "        .agg(\n",
    "            n_red=(\"sum\"),\n",
    "            n=(\"count\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    provider_summary[\"pct_final_red\"] = provider_summary[\"n_red\"] / provider_summary[\"n\"]\n",
    "\n",
    "    # Now, for each week, get the p10/p50/p90 of pct_red across providers\n",
    "    summary = (\n",
    "        provider_summary.groupby(\"week\")[\"pct_final_red\"]\n",
    "        .agg(\n",
    "            min=lambda x: x.min(),\n",
    "            p10=lambda x: x.quantile(0.10),\n",
    "            p25=lambda x: x.quantile(0.25),\n",
    "            p50=\"median\",\n",
    "            p75=lambda x: x.quantile(0.75),\n",
    "            p90=lambda x: x.quantile(0.90),\n",
    "            max=lambda x: x.max(),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    summary = summary.melt(id_vars=\"week\", var_name=\"quantile\", value_name=\"pct_final_red\")\n",
    "\n",
    "    pct_final_red, err_lower, err_upper = [], [], []\n",
    "    for _, row in summary.iterrows():\n",
    "        val = row[\"pct_final_red\"]\n",
    "        pct_final_red.append(val)\n",
    "        err_lower.append(0)\n",
    "        err_upper.append(0)\n",
    "\n",
    "    summary[\"pct_final_red\"] = pct_final_red\n",
    "    summary[\"err_lower\"] = err_lower\n",
    "    summary[\"err_upper\"] = err_upper\n",
    "\n",
    "    group_name_map = {\"p10\": \"p10\", \"p50\": \"p50\", \"p90\": \"p90\"}\n",
    "    summary[\"quantile\"] = (\n",
    "        summary[\"quantile\"].map(group_name_map).fillna(summary[\"quantile\"])\n",
    "    )\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_df = df[df.ClinicianGroup == \"AI\"]\n",
    "plot_df = compute_provider_level_final_red_heterogeneity_df(ai_df, first_final_red=\"final\")\n",
    "fig = plot_no_ai_ai_over_time(\n",
    "    plot_df,\n",
    "    x_var = \"week\",\n",
    "    y_var = \"pct_final_red\",\n",
    "    y_label = \"Percentage of final red flags\",\n",
    "    title = \"Percentage of final red flags by week\",\n",
    "    y_axis_percent = True,\n",
    "    time_x_axis = True,\n",
    "    grouping_var = \"quantile\",\n",
    "    grouping_var_label = \"Quantile\",\n",
    "    group_order = [ \"p10\", \"p25\", \"p50\", \"p75\", \"p90\"],\n",
    ")\n",
    "save_fig(\"final_red_quantiles\", fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot boxplot\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(\n",
    "    data=df,\n",
    "    x='ClinicianGroup',\n",
    "    y='duration_minutes',\n",
    "    showfliers=True,  # show outliers\n",
    "    palette='muted'\n",
    ")\n",
    "plt.title('Clinician attending time by clinician group')\n",
    "plt.ylabel('Clinician attending time (minutes)')\n",
    "plt.tight_layout()\n",
    "plt.ylim(0, 60)\n",
    "\n",
    "ax = plt.gca()\n",
    "xtick_labels = [tick.get_text() for tick in ax.get_xticklabels()]\n",
    "label_map = {\n",
    "    'silent_group': 'Non-AI',\n",
    "    'active_group': 'AI',\n",
    "}\n",
    "new_labels = [label_map.get(lbl, lbl) for lbl in xtick_labels]\n",
    "ax.set_xticklabels(new_labels)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Get the two groups\n",
    "grouped = df.groupby('ClinicianGroup')['duration_minutes']\n",
    "groups = list(grouped.groups.keys())\n",
    "if len(groups) == 2:\n",
    "    group1, group2 = groups\n",
    "    durations1 = grouped.get_group(group1)\n",
    "    durations2 = grouped.get_group(group2)\n",
    "\n",
    "    # Print medians\n",
    "    median1 = durations1.median()\n",
    "    median2 = durations2.median()\n",
    "    print(f\"Median duration for {group1}: {median1:.2f} minutes\")\n",
    "    print(f\"Median duration for {group2}: {median2:.2f} minutes\")\n",
    "\n",
    "    latexvars['nonAIMedianDuration'] = f\"{median1:.2f}\"\n",
    "    latexvars['AIMedianDuration'] = f\"{median2:.2f}\"\n",
    "\n",
    "    # U-test\n",
    "    result = mannwhitneyu(durations1, durations2, alternative='two-sided')\n",
    "    print(f\"U-test between {group1} and {group2}: t={result.statistic:.3f}, p={result.pvalue:.3f}\")\n",
    "    latexvars['MedianDurationP'] = f\"{result.pvalue:.3f}\"\n",
    "else:\n",
    "    print(\"Expected exactly two clinician groups for t-test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_duration_by_var(\n",
    "    df: pd.DataFrame,\n",
    "    var: str,\n",
    "    stratify_by: str = \"ClinicianGroup\",\n",
    "    n_boot: int = 1_000,\n",
    ") -> pd.DataFrame:\n",
    "    records: list[dict] = []\n",
    "    for (var_val, group_val), durations in (\n",
    "        df.groupby([var, stratify_by])[\"duration_minutes\"]\n",
    "    ):\n",
    "        durations = durations.dropna().to_numpy()\n",
    "\n",
    "        # Point estimate\n",
    "        median = np.median(durations)\n",
    "\n",
    "        # Bootstrap CI of the median\n",
    "        boot_samples = np.random.choice(\n",
    "            durations, size=(n_boot, len(durations)), replace=True\n",
    "        )\n",
    "        boot_medians = np.median(boot_samples, axis=1)\n",
    "        ci_low, ci_high = np.percentile(boot_medians, [2.5, 97.5])\n",
    "\n",
    "        records.append(\n",
    "            {\n",
    "                var: var_val,\n",
    "                stratify_by: group_val,\n",
    "                \"median_duration\": median,\n",
    "                \"err_lower\": median - ci_low,\n",
    "                \"err_upper\": ci_high - median,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    summary = pd.DataFrame.from_records(records)\n",
    "    summary = summary[[var, stratify_by, \"median_duration\", \"err_lower\", \"err_upper\"]]\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = compute_duration_by_var(df, var=\"n_aicalls\", stratify_by=\"ClinicianGroup\")\n",
    "data_df = data_df[data_df['n_aicalls'] <= 12]\n",
    "fig = plot_no_ai_ai_over_time(\n",
    "    data_df,\n",
    "    y_var = \"median_duration\",\n",
    "    x_var = \"n_aicalls\",\n",
    "    y_label=\"Median attending time (minutes)\",\n",
    "    title=\"Clinician attending time by number of AI consult triggers\",\n",
    "    y_axis_percent=False,\n",
    "    time_x_axis=False,\n",
    "    x_label = \"Number of times AI Consult was or would have been triggered\",\n",
    "    y_lim=(0, 40),\n",
    ")\n",
    "save_fig(\"duration_by_n_ai_calls\", fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to n_aicalls <= 12\n",
    "filtered_df = results_ai_df_gpt41[results_ai_df_gpt41[\"n_aicalls\"] <= 12]\n",
    "\n",
    "# Compute mean and bootstrap CIs for treatment_likert by n_aicalls and ClinicianGroup\n",
    "def wilson_ci(data, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Compute the mean (proportion) and Wilson 95% confidence interval for binary data.\n",
    "    Returns (mean, lower, upper).\n",
    "    \"\"\"\n",
    "    data = np.array(data.dropna())\n",
    "    if len(data) == 0:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    # If data is not binary, raise an error\n",
    "    unique_vals = np.unique(data)\n",
    "    if not np.all(np.isin(unique_vals, [0, 1])):\n",
    "        raise ValueError(\"wilson_ci expects binary (0/1) data\")\n",
    "    count = np.sum(data)\n",
    "    nobs = len(data)\n",
    "    mean = count / nobs\n",
    "    lower, upper = proportion_confint(count, nobs, alpha=alpha, method='wilson')\n",
    "    return mean, lower, upper\n",
    "\n",
    "summary_records = []\n",
    "for (n_aicalls, group), subdf in filtered_df.groupby([\"n_aicalls\", \"ClinicianGroup\"]):\n",
    "    mean, lower, upper = wilson_ci(subdf[\"treatment_likert_is_low\"])\n",
    "    summary_records.append({\n",
    "        \"n_aicalls\": n_aicalls,\n",
    "        \"ClinicianGroup\": group,\n",
    "        \"mean_treatment_likert\": mean,\n",
    "        \"ci_lower\": lower,\n",
    "        \"ci_upper\": upper,\n",
    "        \"err_lower\": mean - lower,\n",
    "        \"err_upper\": upper - mean,\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_records)\n",
    "\n",
    "# Use the same plotting style as in the previous cell (see @file_context_0)\n",
    "fig = plot_no_ai_ai_over_time(\n",
    "    summary_df,\n",
    "    y_var=\"mean_treatment_likert\",\n",
    "    x_var=\"n_aicalls\",\n",
    "    y_label=\"Rate of treatment errors\",\n",
    "    title=\"Rate of treatment errors, assigned by GPT-4.1, vs number of AI calls\",\n",
    "    y_axis_percent=True,\n",
    "    time_x_axis=False,\n",
    "    x_label = \"Number of times AI Consult was or would have been triggered\",\n",
    "    y_lim=None,\n",
    ")\n",
    "save_fig(\"treatment_errors_vs_n_aicalls\", fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to visits with duration <= 30 minutes\n",
    "filtered_df = results_ai_df_gpt41[results_ai_df_gpt41[\"duration_minutes\"] <= 30].copy()\n",
    "\n",
    "# Bin visit durations\n",
    "bin_width = 5\n",
    "filtered_df[\"VisitDurationBin\"] = (filtered_df[\"duration_minutes\"] // bin_width * bin_width).astype(int)\n",
    "\n",
    "# Compute treatment error rate (proportion where treatment_likert_is_low == 1) and Wilson CI for each bin/group\n",
    "summary_records = []\n",
    "for (duration_bin, group), subdf in filtered_df.groupby([\"VisitDurationBin\", \"ClinicianGroup\"]):\n",
    "    mean, lower, upper = wilson_ci(subdf[\"treatment_likert_is_low\"])\n",
    "    summary_records.append({\n",
    "        \"VisitDurationBin\": duration_bin,\n",
    "        \"ClinicianGroup\": group,\n",
    "        \"treatment_error_rate\": mean,\n",
    "        \"ci_lower\": lower,\n",
    "        \"ci_upper\": upper,\n",
    "        \"err_lower\": mean - lower,\n",
    "        \"err_upper\": upper - mean,\n",
    "        \"n\": len(subdf),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_records)\n",
    "summary_df = summary_df.sort_values(\"VisitDurationBin\")\n",
    "\n",
    "# Plot using the same style as plot_no_ai_ai_over_time\n",
    "fig = plot_no_ai_ai_over_time(\n",
    "    summary_df,\n",
    "    y_var=\"treatment_error_rate\",\n",
    "    x_var=\"VisitDurationBin\",\n",
    "    y_label=\"Rate of treatment errors\",\n",
    "    title=\"Rate of treatment errors, assigned by GPT-4.1, vs clinician attending time\",\n",
    "    y_axis_percent=True,\n",
    "    time_x_axis=False,\n",
    "    x_label=\"Clinician attending time (minutes, binned)\",\n",
    "    y_lim=None,\n",
    ")\n",
    "save_fig(\"treatment_errors_vs_visit_duration\", fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_note_length(row):\n",
    "    doc = row['ClinicalDocumentation']\n",
    "    if doc is None:\n",
    "        return None\n",
    "    note = getattr(doc, 'clinical_notes_clean', None)\n",
    "    if note is None:\n",
    "        return None\n",
    "    return len(note)\n",
    "\n",
    "date_col = 'VisitDate'\n",
    "\n",
    "# Compute note length and week\n",
    "df = results_ai_df_gpt41.copy()\n",
    "df['note_length'] = df.apply(get_note_length, axis=1)\n",
    "df['week'] = pd.to_datetime(df[date_col]).dt.to_period('W').dt.start_time\n",
    "\n",
    "# Group by week and ClinicianGroup, compute median and CI\n",
    "def bootstrap_median_ci_note(data, n_boot=1000, ci=95):\n",
    "    data = np.array(data.dropna())\n",
    "    if len(data) == 0:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    medians = np.array([\n",
    "        np.median(np.random.choice(data, size=len(data), replace=True))\n",
    "        for _ in range(n_boot)\n",
    "    ])\n",
    "    median = np.median(data)\n",
    "    lower = np.percentile(medians, (100 - ci) / 2)\n",
    "    upper = np.percentile(medians, 100 - (100 - ci) / 2)\n",
    "    return median, lower, upper\n",
    "\n",
    "summary_records = []\n",
    "for (week, group), subdf in df.groupby(['week', 'ClinicianGroup']):\n",
    "    median, lower, upper = bootstrap_median_ci_note(subdf['note_length'])\n",
    "    summary_records.append({\n",
    "        'week': week,\n",
    "        'ClinicianGroup': group,\n",
    "        'median_note_length': median,\n",
    "        'ci_lower': lower,\n",
    "        'ci_upper': upper,\n",
    "        'err_lower': median - lower,\n",
    "        'err_upper': upper - median,\n",
    "    })\n",
    "\n",
    "summary_notes_df = pd.DataFrame(summary_records)\n",
    "summary_notes_df = summary_notes_df.sort_values('week')\n",
    "\n",
    "# Plot\n",
    "fig = plot_no_ai_ai_over_time(\n",
    "    summary_notes_df,\n",
    "    y_var=\"median_note_length\",\n",
    "    x_var=\"week\",\n",
    "    y_label=\"Median clinical note length (chars)\",\n",
    "    title=\"Median clinical note length over time (by week), AI vs non-AI\",\n",
    "    y_axis_percent=False,\n",
    "    time_x_axis=True,\n",
    "    y_lim=None,\n",
    ")\n",
    "save_fig(\"median_clinical_note_length_vs_week\", fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_value_counts = ai_interaction_scrubbed[ai_interaction_scrubbed.Silent == 'Active'].AiLike.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latexvars['nAICalls'] = f\"{ai_value_counts.sum():.0f}\"\n",
    "\n",
    "latexvars['nAICallsThumbsAny'] = f\"{ai_value_counts['Up'] + ai_value_counts['Down']:.0f}\"\n",
    "latexvars['pctAICallsThumbsAny'] = f\"{(ai_value_counts['Up'] + ai_value_counts['Down']) / ai_value_counts.sum():.1%}\"\n",
    "\n",
    "latexvars['nAICallsThumbsUp'] = f\"{ai_value_counts['Up']:.0f}\"\n",
    "latexvars['nAICallsThumbsDown'] = f\"{ai_value_counts['Down']:.0f}\"\n",
    "\n",
    "latexvars['pctAICallsThumbsUp'] = f\"{ai_value_counts['Up'] / (ai_value_counts['Up'] + ai_value_counts['Down']):.1%}\"\n",
    "latexvars['pctAICallsThumbsDown'] = f\"{ai_value_counts['Down'] / (ai_value_counts['Up'] + ai_value_counts['Down']):.1%}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportion_ci(successes, n, ci=95):\n",
    "    if n == 0:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    if float(successes).is_integer():\n",
    "        successes = int(successes)\n",
    "    else:\n",
    "        raise ValueError(\"successes must be an integer\")\n",
    "\n",
    "    if float(n).is_integer():\n",
    "        n = int(n)\n",
    "    else:\n",
    "        raise ValueError(\"n must be an integer\")\n",
    "\n",
    "    result = binomtest(successes, n)\n",
    "    alpha = 1 - ci / 100\n",
    "    ci_low, ci_upp = result.proportion_ci(confidence_level=1 - alpha, method=\"wilson\")\n",
    "    return ci_low, ci_upp\n",
    "\n",
    "active_ai = ai_interaction_scrubbed[ai_interaction_scrubbed.Silent == 'Active'].copy()\n",
    "active_ai['CreatedOn'] = pd.to_datetime(active_ai['CreatedOn'], errors='coerce')\n",
    "active_ai['week'] = active_ai['CreatedOn'].dt.to_period('W').dt.start_time\n",
    "\n",
    "active_ai = active_ai[active_ai['AiLike'].notna()]\n",
    "\n",
    "valid_likes = active_ai[active_ai['AiLike'].isin(['Up', 'Down'])].copy()\n",
    "\n",
    "ai_like_counts = (\n",
    "    valid_likes.groupby(['week', 'AiLike'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .sort_index()\n",
    ")\n",
    "ai_like_counts['ClinicianGroup'] = 'AI'\n",
    "\n",
    "ai_like_counts['n'] = ai_like_counts.get('Up', 0) + ai_like_counts.get('Down', 0)\n",
    "ai_like_counts['percent_up'] = ai_like_counts.get('Up', 0) / ai_like_counts['n']\n",
    "\n",
    "ci_bounds = ai_like_counts.apply(\n",
    "    lambda row: proportion_ci(row.get('Up', 0), row['n']) if row['n'] > 0 else (np.nan, np.nan),\n",
    "    axis=1, result_type='expand'\n",
    ")\n",
    "ai_like_counts['ci_lower'] = ci_bounds[0]\n",
    "ai_like_counts['ci_upper'] = ci_bounds[1]\n",
    "\n",
    "ai_like_counts['err_lower'] = ai_like_counts['percent_up'] - ai_like_counts['ci_lower']\n",
    "ai_like_counts['err_upper'] = ai_like_counts['ci_upper'] - ai_like_counts['percent_up']\n",
    "\n",
    "\n",
    "# Use the shared plotting function for consistency\n",
    "ai_like_counts = ai_like_counts.reset_index()\n",
    "ai_like_counts = ai_like_counts.sort_values('week')\n",
    "\n",
    "fig = plot_no_ai_ai_over_time(\n",
    "    red_by_week=ai_like_counts,\n",
    "    y_var=\"percent_up\",\n",
    "    x_var=\"week\",\n",
    "    y_label=\"Percent Up (%)\",\n",
    "    title=\"Percent of AI Consult interactions with a thumbs up rating among all interactions with ratings\",\n",
    "    grouping_var='ClinicianGroup',\n",
    "    grouping_var_label=\"\",\n",
    "    group_order=['', 'AI'],\n",
    "    y_axis_percent=True,\n",
    "    time_x_axis=True,\n",
    "    y_lim=None,\n",
    ")\n",
    "save_fig(\"thumbs_up_over_time\", fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latexvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process latexvars dict into LaTeX \\newcommand strings\n",
    "def process_value(v):\n",
    "    if pd.isna(v):\n",
    "        return \"N/A\"\n",
    "    elif isinstance(v, float) and np.isinf(v):\n",
    "        return \"Inf\"\n",
    "    else:\n",
    "        return v\n",
    "\n",
    "latex_newcommands = \"\\n\".join(\n",
    "    [f\"\\\\newcommand\\\\{k}{{{process_value(v)}\\\\xspace}}\" for k, v in latexvars.items()]\n",
    ")\n",
    "def escape_percent(s):\n",
    "    # Replace any % that is not already escaped (\\%) with \\%\n",
    "    if isinstance(s, str):\n",
    "        s = s.replace(\"%\", r\"\\%\")\n",
    "        s = s.replace(\"\\\\\\\\\", \"\\\\\")\n",
    "    return s\n",
    "\n",
    "latex_newcommands = \"\\n\".join(\n",
    "    [f\"\\\\newcommand\\\\{k}{{{escape_percent(v)}\\\\xspace}}\" for k, v in latexvars.items()]\n",
    ")\n",
    "\n",
    "bf.write_text(plots_folder + \"latex_commands.tex\", latex_newcommands)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
